{
    "AWS APIs Called More Often Than Usual Per Account - Demo": {
        "actions_UBASeverity": 2, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 10, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "| `Load_Sample_Log_Data(AWS CloudTrail)` | bucket _time span=1d | stats values(eval(\"1\")) as count by _time ", 
        "description": [
            "First we bring in our basic demo dataset. In this case, anonymized AWS CloudTrail logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Next we use stats to summarize the number of events per user per day."
        ], 
        "label": "AWS APIs Called More Often Than Usual Per Account - Demo", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "aws-cloudtrail-data-anon.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "scaleFactor": 2, 
        "value": "| `Load_Sample_Log_Data(AWS CloudTrail)` \n| bucket _time span=1d \n| stats count by  user _time", 
        "windowSize": 0
    }, 
    "AWS APIs Called More Often Than Usual Per Account - Live": {
        "actions_UBASeverity": 2, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 10, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "index=* sourcetype=aws:cloudtrail eventName=ConsoleLogin OR eventName=CreateImage OR eventName=AssociateAddress OR eventName=AttachInternetGateway OR eventName=AttachVolume OR eventName=StartInstances OR eventName=StopInstances OR eventName=UpdateService OR eventName=UpdateLoginProfile | bucket _time span=1d | stats dc(user) as count by _time", 
        "description": [
            "First we bring in our basic dataset, AWS CloudTrail logs that are filtered for interesting APIs.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Next we use stats to summarize the number of events per user per day."
        ], 
        "label": "AWS APIs Called More Often Than Usual Per Account - Live", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have AWS CloudTrail data", 
                "resolution": "In order to run this search, you must have AWS CloudTrail data onboard. Visit the <a href=\"/app/Splunk_Security_Essentials/data_source?technology=AWS%20CloudTrail\">data onboarding guide for AWS CloudTrail in this app</a>, or browse to <a href=\"https://splunkbase.splunk.com/app/1876/\">apps.splunk.com</a> for more information.", 
                "test": "| tstats count where earliest=-2h latest=now index=* sourcetype=aws:cloudtrail"
            }
        ], 
        "scaleFactor": 2, 
        "value": "index=* sourcetype=aws:cloudtrail eventName=ConsoleLogin OR eventName=CreateImage OR eventName=AssociateAddress OR eventName=AttachInternetGateway OR eventName=AttachVolume OR eventName=StartInstances OR eventName=StopInstances OR eventName=UpdateService OR eventName=UpdateLoginProfile \n| bucket _time span=1d \n| stats count by user _time", 
        "windowSize": 0
    }, 
    "AWS Unusual Amount of Modifications to ACLs - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 0, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "| `Load_Sample_Log_Data(AWS CloudTrail)`  | search eventName=AuthorizeSecurityGroup*  | bucket _time span=1d | stats values(eval(\"1\")) as count by _time ", 
        "description": [
            "First we bring in our basic demo dataset. In this case, anonymized AWS CloudTrail logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "With our dataset onboard, we then filter down to just the events indicating a modification of ACLs", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Next we use stats to summarize the number of events per user per day."
        ], 
        "label": "AWS Unusual Amount of Modifications to ACLs - Demo", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "aws-cloudtrail-data-anon.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "scaleFactor": 2, 
        "value": "| `Load_Sample_Log_Data(AWS CloudTrail)` \n| search eventName=AuthorizeSecurityGroup* \n| bucket _time span=1d \n| stats count by user _time", 
        "windowSize": 0
    }, 
    "AWS Unusual Amount of Modifications to ACLs - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 0, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "index=* sourcetype=aws:cloudtrail eventName=AuthorizeSecurityGroup* | bucket _time span=1d | stats dc(user) as count by _time", 
        "description": [
            "First we bring in our basic dataset, AWS CloudTrail logs that are filtered for ACL modification events.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Next we use stats to summarize the number of events per user per day."
        ], 
        "label": "AWS Unusual Amount of Modifications to ACLs - Live", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have AWS CloudTrail data", 
                "resolution": "In order to run this search, you must have AWS CloudTrail data onboard. Visit the <a href=\"/app/Splunk_Security_Essentials/data_source?technology=AWS%20CloudTrail\">data onboarding guide for AWS CloudTrail in this app</a>, or browse to <a href=\"https://splunkbase.splunk.com/app/1876/\">apps.splunk.com</a> for more information.", 
                "test": "| tstats count where earliest=-2h latest=now index=* sourcetype=aws:cloudtrail"
            }
        ], 
        "scaleFactor": 2, 
        "value": "index=* sourcetype=aws:cloudtrail eventName=AuthorizeSecurityGroup* \n| bucket _time span=1d \n| stats count by user _time", 
        "windowSize": 0
    }, 
    "Detect Spike in SMB Traffic - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 80, 
        "actions_riskObjectType": "system", 
        "cardinalityTest": "| inputlookup UC_smb_spike_detection | search (dest_port=139 OR dest_port=445) | bucket _time span=1d | stats dc(src_ip) as count by _time ", 
        "description": [
            "First we pull in our demo dataset of Firewall logs", 
            "Next we filter for just SMB connections.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the last day.", 
            "Now we are looking at the number of unique destinations per source IP, per day."
        ], 
        "label": "Detect Spike in SMB Traffic - Demo", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "src_ip", 
        "prereqs": [
            {
                "field": "UC_smb_spike_detection.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "scaleFactor": 2, 
        "value": "| inputlookup UC_smb_spike_detection \n| search (dest_port=139 OR dest_port=445) \n| bucket _time span=1d \n| stats dc(dest_ip) as count by src_ip, _time", 
        "windowSize": 0
    }, 
    "Detect Spike in SMB Traffic - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 80, 
        "actions_riskObjectType": "system", 
        "cardinalityTest": "index=* ((tag=network tag=communicate) OR (sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco:asa OR sourcetype=stream* )) (dest_port=139 OR dest_port=445) | bucket _time span=1d | stats dc(src_ip) as count by _time ", 
        "description": [
            "First we pull in our basic dataset, which comes from Firewall Logs for SMB connections.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the last day.", 
            "Now we are looking at the number of unique destinations per source IP, per day."
        ], 
        "label": "Detect Spike in SMB Traffic - Live", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "src_ip", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have network traffic.", 
                "resolution": "Ingest network traffic logs, consider using Splunk Stream.", 
                "test": "index=* ((tag=network tag=communicate) OR (sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco:asa OR sourcetype=stream*)) earliest=-1h latest=now | stats count"
            }
        ], 
        "value": "index=* ((tag=network tag=communicate) OR (sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco:asa OR sourcetype=stream* )) (dest_port=139 OR dest_port=445) \n| bucket _time span=1d \n| stats dc(dest_ip) as count by src_ip, _time "
    }, 
    "Distinct Hosts Communicated With Per Day - Accelerated": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "All_Traffic.src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "cardinalityTest": "| tstats summariesonly=t allow_old_summaries=t dc(All_Traffic.src_ip) as count from datamodel=Network_Traffic by _time span=1d ", 
        "description": [
            "Here, tstats is pulling in one command a super-fast count per src_ip, per day."
        ], 
        "label": "Distinct Hosts Communicated With Per Day - Accelerated", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "All_Traffic.src_ip", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have data in Network Traffic data model", 
                "resolution": "This search requires Firewall or Netflow data to run. We are searching here for the common information model network traffic data model.", 
                "test": "| tstats count from datamodel=Network_Traffic where earliest=-1h "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an accelerated Network Traffic data model", 
                "resolution": "In addition to searching for the common information model network traffic data model, we are telling Splunk to only visit accelerated data models.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Network_Traffic where earliest=-1h "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Network Traffic data model must have src_ip and dest_ip fields", 
                "resolution": "In addition to searching for the accelerated common information model network traffic data model, we are telling Splunk to verify that there is a src_ip and dest_ip in this data set.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t dc(All_Traffic.dest_ip) as dest dc(All_Traffic.src_ip) as src from datamodel=Network_Traffic where earliest=-1h | eval count = dest * src"
            }
        ], 
        "scaleFactor": 2, 
        "value": "| tstats summariesonly=t allow_old_summaries=t dc(All_Traffic.dest_ip) as count from datamodel=Network_Traffic by All_Traffic.src_ip _time span=1d ", 
        "windowSize": 0
    }, 
    "Distinct Hosts Communicated With Per Day - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "cardinalityTest": "|  `Load_Sample_Log_Data(\"Sample Firewall Data\")`| bucket _time span=1d | stats dc(src_ip) as count by _time ", 
        "description": [
            "First we pull in our demo dataset.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per src_ip, per day."
        ], 
        "label": "Distinct Hosts Communicated With Per Day - Demo", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "src_ip", 
        "prereqs": [
            {
                "field": "od_splunklive_fw_data.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "scaleFactor": 2, 
        "value": "|  `Load_Sample_Log_Data(\"Sample Firewall Data\")`\n| bucket _time span=1d \n| stats dc(dest_ip) as count by src_ip, _time", 
        "windowSize": 0
    }, 
    "Distinct Hosts Communicated With Per Day - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "cardinalityTest": "(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)  | bucket _time span=1d | stats dc(src_ip) as count by _time ", 
        "description": [
            "First we pull in our Firewall dataset (we should specify a single index and sourcetype in production environments).", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per src_ip, per day."
        ], 
        "label": "Distinct Hosts Communicated With Per Day - Live", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "src_ip", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Firewall data", 
                "resolution": "This search requires Firewall or Netflow data to run. By default, we're checking for Common Information Model compliant data, and then also manually specifying the standard sourcetypes for Check Point, Palo Alto Networks, and Cisco ASAs. You should specify your particular index and sourcetype in the actual search to improve performance (or better yet, accelerate with the common information model!)", 
                "test": "(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)| head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a src_ip and dest_ip field", 
                "resolution": "This search is also looking for firewall logs, but with the added filter of making sure that a src_ip and dest_ip is defined.", 
                "test": "((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)) src_ip=* dest_ip=* | head 100 | stats count "
            }
        ], 
        "scaleFactor": 2, 
        "value": "(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)  \n| bucket _time span=1d \n| stats dc(dest_ip) as count by src_ip, _time", 
        "windowSize": 0
    }, 
    "Git File Views or Downloads Per Day - Accelerated": {
        "actions_UBASeverity": 2, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 10, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "| tstats summariesonly=t allow_old_summaries=t dc(user) as count from datamodel=Git where nodename=Git_View by _time span=1d", 
        "description": [
            "Here, tstats is pulling in one command a super-fast count per user, per day.", 
            "(self-explanatory)"
        ], 
        "label": "Git File Views or Downloads Per Day - Accelerated", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an accelerated Git data model (non-default)", 
                "resolution": "This search accelerated Git data. There is no formal CIM data model for Source Code checkins or checkouts, so we are presuming a custom data model called Git.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Git where earliest=-24h latest=now nodename=Git_View "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a user field in accelerated Git data model", 
                "resolution": "The Git data model must have a user field defined.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t dc(user) as count from datamodel=Git where earliest=-24h latest=now nodename=Git_View"
            }
        ], 
        "scaleFactor": 2, 
        "value": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Git where nodename=Git_View groupby user, _time span=1d \n| eval comment=\"<--- We don't have a standard data model that includes git repos, so you will need to build one to leverage data model acceleration\"   ", 
        "windowSize": 0
    }, 
    "Git File Views or Downloads Per Day - Demo": {
        "actions_UBASeverity": 2, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 10, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "|  `Load_Sample_Log_Data(\"Source Code Access Logs\")`| bucket _time span=1d | stats dc(user) as count by _time ", 
        "description": [
            "First we pull in our demo dataset.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Git File Views or Downloads Per Day - Demo", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "anonymized_git_history.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "scaleFactor": 2, 
        "value": "|  `Load_Sample_Log_Data(\"Source Code Access Logs\")`\n| bucket _time span=1d \n| stats count by user _time ", 
        "windowSize": 0
    }, 
    "Git File Views or Downloads Per Day - Live": {
        "actions_UBASeverity": 2, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 10, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "index=* source=\"*/atlassian-bitbucket-access.log\" | bucket _time span=1d | stats dc(user) as count by _time ", 
        "description": [
            "First we pull in our Atlassian Git dataset.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Git File Views or Downloads Per Day - Live", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have BitBucket / Git data", 
                "resolution": "In tests so far, Atlassian BitBucket git logs are stored in a file called atlassian-bitbucket-access.log. We're looking for that here.", 
                "test": "| metasearch earliest=-24h latest=now index=* source=\"*/atlassian-bitbucket-access.log\" | head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a user defined in your data", 
                "resolution": "You should have a field called \"user\" defined in your bitbucket logs. If that's not currently extracted, build an extraction for it (or do an inline rex in the SPL below to work around this).", 
                "test": "earliest=-2h latest=now index=* source=\"*/atlassian-bitbucket-access.log\" | head 100 | stats dc(user) as count "
            }
        ], 
        "scaleFactor": 2, 
        "value": "index=* source=\"*/atlassian-bitbucket-access.log\" \n| bucket _time span=1d \n| stats count by user _time ", 
        "windowSize": 0
    }, 
    "Increase in Interactive Logons - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 0, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "|  `Load_Sample_Log_Data(\"Interactive Logins\")`| bucket _time span=1d | stats dc(user) as count by _time  ", 
        "description": [
            "First we pull in our demo dataset.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Increase in Interactive Logons - Demo", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "scaleFactor": 2, 
        "value": "|  `Load_Sample_Log_Data(\"Interactive Logins\")`\n| bucket _time span=1d \n| stats dc(dest) as count by _time user ", 
        "windowSize": 0
    }, 
    "Increase in Interactive Logons - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 0, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "index=* source=\"winEventLog:Security\" Logon_Type=2 OR Logon_Type=10 OR Logon_Type=11 Logon Type TaskCategory=Logon Audit Success | bucket _time span=1d | stats dc(user) as count by _time", 
        "description": [
            "First we pull in our dataset of Windows Authentication specifying Interactive logon types.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Increase in Interactive Logons - Live", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security data", 
                "resolution": "This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.", 
                "test": "| metasearch earliest=-2h latest=now index=* source=\"winEventLog:Security\" | head | stats count "
            }
        ], 
        "scaleFactor": 2, 
        "value": "index=* source=\"winEventLog:Security\" Logon_Type=2 OR Logon_Type=10 OR Logon_Type=11 Logon Type TaskCategory=Logon Audit Success \n| bucket _time span=1d \n| stats dc(dest) as count by _time user", 
        "windowSize": 0
    }, 
    "Increase in Interactively Logged In Users - Demo": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "dest", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "system", 
        "cardinalityTest": "|  `Load_Sample_Log_Data(\"Interactive Logins\")`| bucket _time span=1d | stats dc(dest) as count by _time  ", 
        "description": [
            "First we pull in our demo dataset.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Increase in Interactively Logged In Users - Demo", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "dest", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "scaleFactor": 2, 
        "value": "|  `Load_Sample_Log_Data(\"Interactive Logins\")`\n| bucket _time span=1d \n| stats dc(user) as count by _time dest ", 
        "windowSize": 0
    }, 
    "Increase in Interactively Logged In Users - Live": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "dest", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "system", 
        "cardinalityTest": "index=* source=\"winEventLog:Security\" Logon_Type=2 OR Logon_Type=10 OR Logon_Type=11 Logon Type TaskCategory=Logon Audit Success | bucket _time span=1d | stats dc(dest) as count by _time", 
        "description": [
            "First we pull in our dataset of Windows Authentication events specifying interactive logon types.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per system, per day."
        ], 
        "label": "Increase in Interactively Logged In Users - Live", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "dest", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security data", 
                "resolution": "This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.", 
                "test": "| metasearch earliest=-2h latest=now index=* source=\"winEventLog:Security\" | head | stats count "
            }
        ], 
        "scaleFactor": 2, 
        "value": "index=* source=\"winEventLog:Security\" Logon_Type=2 OR Logon_Type=10 OR Logon_Type=11 Logon Type TaskCategory=Logon Audit Success \n| bucket _time span=1d \n| stats dc(user) as count by _time dest", 
        "windowSize": 0
    }, 
    "Increase in Windows Privilege Escalation - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 0, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Unprivileged_Account_Name", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "|  `Load_Sample_Log_Data(\"Windows Run As Logs (Event ID 4648)\")`| makemv Account_Name delim=\",\" | bucket _time span=1d | stats dc(Unprivileged_Account_Name) as count by _time  ", 
        "description": [
            "First we pull in our demo dataset.", 
            "This line won't exist outside of demo data.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Increase in Windows Privilege Escalation - Demo", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "Unprivileged_Account_Name", 
        "prereqs": [
            {
                "field": "event_id_4648_runas.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "scaleFactor": 2, 
        "value": "|  `Load_Sample_Log_Data(\"Windows Run As Logs (Event ID 4648)\")`\n| makemv Account_Name delim=\",\" \n| bucket _time span=1d \n| stats count by _time Unprivileged_Account_Name", 
        "windowSize": 0
    }, 
    "Increase in Windows Privilege Escalation - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 0, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "index=* source=\"winEventLog:Security\" 4648 EventCode=4648 | search NOT Account_Name=*$ | eval Unprivileged_Account_Name=mvindex(Account_Name, 1) | bucket _time span=1d | stats dc(Unprivileged_Account_Name) as count by _time ", 
        "description": [
            "First we pull in our dataset, consisting of Windows Security logs with Event ID 4648, showing \"Run As\" events.", 
            "Next we filter out the Windows System usernames, where this can occur frequently", 
            "Windows Security logs often include two usernames -- the acting username, and the target username. We want the latter (note that this hasn't been proven to work uniformly across all log sources, but it seems to work well for this scenario).", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Increase in Windows Privilege Escalation - Live", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security data", 
                "resolution": "This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.", 
                "test": "| metasearch earliest=-2h latest=now index=* source=\"winEventLog:Security\" | head | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Privileged Escalation Events (EventCode=4648)", 
                "resolution": "Windows Security Event ID 4648 tracks the explicit use of credentials, as in a runas event or batch login from a scheduled task. You can enable this from your Windows Logon Event policy configuration.", 
                "test": "| metasearch earliest=-30d source=\"winEventLog:Security\" index=* TERM(eventcode=4648)  | head | stats count"
            }
        ], 
        "scaleFactor": 2, 
        "value": "index=* source=\"winEventLog:Security\" 4648 EventCode=4648 \n| search NOT Account_Name=*$ \n| eval Unprivileged_Account_Name=mvindex(Account_Name, 1) \n| bucket _time span=1d \n| stats count by _time Unprivileged_Account_Name", 
        "windowSize": 0
    }, 
    "Many DLP Alerts for User - Accelerated": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "| tstats summariesonly=t allow_old_summaries=t dc(DLP_Incidents.user) as count from datamodel=DLP where nodename=DLP_Incidents by  _time span=1d ", 
        "description": [
            "Here, tstats is pulling in one command a super-fast count of DLP Alerts per user per day."
        ], 
        "label": "Many DLP Alerts for User - Accelerated", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must data in the DLP data model", 
                "resolution": "This search requires DLP data. This is dependent on the <a href=\"https://splunkbase.splunk.com/app/1621/\">Common Information Model</a> being present, and also having your data mapped to CIM via appropriate TAs. Find the TA for your apps on <a href=\"http://splunkbase.com/\">SplunkBase</a>, and they should already have the proper fields and tags defined.", 
                "test": "| tstats count from datamodel=DLP where nodename=DLP_Incidents earliest=-8h"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an accelerated DLP data model", 
                "resolution": "This search requires an accelerated DLP data model. In order to run a fast accelerated search, you should accelerate your data model. (<a href=\"https://docs.splunk.com/Documentation/Splunk/latest/HadoopAnalytics/Configuredatamodelacceleration#Accelerate_the_data_model\">docs</a>)", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=DLP where earliest=-8h nodename=DLP_Incidents"
            }
        ], 
        "scaleFactor": 5, 
        "value": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=DLP where nodename=DLP_Incidents by DLP_Incidents.user _time span=1d | rename DLP_Incidents.* as *", 
        "windowSize": 0
    }, 
    "Many DLP Alerts for User - Demo": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "| `Load_Sample_Log_Data(Fabricated DLP Alerts)` | bucket _time span=1d | stats dc(user) as count by _time", 
        "description": [
            "First we pull in our demo dataset.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Many DLP Alerts for User - Demo", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "UC_dlp_alerts.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "scaleFactor": 2, 
        "value": "| `Load_Sample_Log_Data(Fabricated DLP Alerts)` \n| bucket _time span=1d \n| stats count by user _time", 
        "windowSize": 0
    }, 
    "Many DLP Alerts for User - Live": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "index=* tag=dlp tag=incident \n| bucket _time span=1d \n| stats dc(user) as count by  _time", 
        "description": [
            "First we pull in our DLP events, tagged via the TAs that are complaint with Splunk's Common Information Model. You can adjust this to the index and sourcetype of your DLP logs as well.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Many DLP Alerts for User - Live", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have DLP data", 
                "resolution": "This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.", 
                "test": "earliest=-24h latest=now index=* tag=dlp tag=incident | head | stats count "
            }
        ], 
        "scaleFactor": 5, 
        "value": "index=* tag=dlp tag=incident \n| bucket _time span=1d \n| stats count by user", 
        "windowSize": 0
    }, 
    "Many USB File Copies for User - Demo": {
        "actions_UBASeverity": 2, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 10, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "| `Load_Sample_Log_Data(Fabricated Files Copied to USB)` | bucket _time span=1d | stats dc(user) as count by _time", 
        "description": [
            "First we pull in our demo dataset.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Many USB File Copies for User - Demo", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "UC_file_copied_to_usb.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "scaleFactor": 2, 
        "value": "| `Load_Sample_Log_Data(Fabricated Files Copied to USB)` \n| bucket _time span=1d \n| stats count by user, _time", 
        "windowSize": 0
    }, 
    "Many USB File Copies for User - Live": {
        "actions_UBASeverity": 2, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 10, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "index=* source=win*security 4663 EventCode=4663 Object_Name=* (Accesses=\"WriteData *\" OR Accesses=\"AppendData *\") \n| regex Object_Name!=\"^.Device.HarddiskVolume\\d*.\\s*$\" \n| bucket _time span=1d \n| stats dc(user) as count by  _time", 
        "description": [
            "First we pull in our Windows security log, filtering to removable storage events that were introduced in Windows 10, and specifically to file writes.", 
            "In our lab testing datasets, we found it prudent to filter to just removable hard disk, particularly excluding CD drives. We do this via the regex search command, which allows us to flexibly match the Object_Name.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Many USB File Copies for User - Live", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security data", 
                "resolution": "This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.", 
                "test": "| metasearch earliest=-2h latest=now index=* source=\"winEventLog:Security\" | head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Removable Storage Event logs", 
                "resolution": "This is a new feature introduced in Windows 10 -- read about it <a href=\"https://docs.microsoft.com/en-us/windows/security/threat-protection/auditing/monitor-the-use-of-removable-storage-devices\">here</a>.", 
                "test": "source=\"winEventLog:Security\" index=* 4663 EventCode=4663 earliest=-4h removable storage | head 100 | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have file writes via Removable Storage Event logs", 
                "resolution": "Must have occurrences of file writes.", 
                "test": "source=\"winEventLog:Security\" earliest=-4h 4663 EventCode=4663 index=* removable storage (Accesses=\"WriteData *\" OR Accesses=\"AppendData *\")  | head 100 | stats count"
            }
        ], 
        "scaleFactor": 5, 
        "value": "index=* source=win*security EventCode=4663 Object_Name=* (Accesses=\"WriteData *\" OR Accesses=\"AppendData *\") \n| regex Object_Name!=\"^.Device.HarddiskVolume\\d*.\\s*$\" \n| bucket _time span=1d \n| stats count by user _time", 
        "windowSize": 0
    }, 
    "Number of Unique Patient Records Viewed Per Day - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "EmployeeName", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "|  `Load_Sample_Log_Data(\"Aggregated Cerner EMR Logs\")`| stats dc(EmployeeName) as count by _time ", 
        "description": [
            "First we pull in our demo dataset.", 
            "We would normally need to aggregate now per user per day, but in this case the demo dataset is already aggregated (pulling from a summary index, as is often done in this scenario)."
        ], 
        "label": "Number of Unique Patient Records Viewed Per Day - Demo", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "NumOpens", 
        "outlierVariableSubject": "EmployeeName", 
        "prereqs": [
            {
                "field": "healthcare_cerner_patient_records.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "scaleFactor": 6, 
        "value": "|  `Load_Sample_Log_Data(\"Aggregated Cerner EMR Logs\")`\n| table _time EmployeeName NumOpens Role YearsAtCompany City Username", 
        "windowSize": 0
    }, 
    "Number of Unique Patient Records Viewed Per Day - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "prsnl_name", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "index=* sourcetype=Cerner | bucket_time span=1d | stats dc(prsnl_name) as count by  _time", 
        "description": [
            "First we pull in our Cerner audit log dataset.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Number of Unique Patient Records Viewed Per Day - Live", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "prsnl_name", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Cerner data (or similar EMR data)", 
                "resolution": "While this use case applies to any similar data, our sample search is looking for a sourcetype of Cerner somewhere. ", 
                "test": "| metasearch earliest=-24h latest=now index=* sourcetype=Cerner | head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a prsnl_id defined in your data", 
                "resolution": "You should have a field called \"prsnl_id\" defined in your Cerner logs. If that's not currently extracted, build an extraction for it (or do an inline rex in the SPL below to work around this).", 
                "test": "earliest=-2h latest=now index=* sourcetype=Cerner| head 100 | stats dc(prsnl_id) as count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a \"event_list.participants.person_id\" defined in your data", 
                "resolution": "You should have a field called \"event_list.participants.person_id\" defined in your Cerner logs. If that's not currently extracted, build an extraction for it (or do an inline rex in the SPL below to work around this).", 
                "test": "earliest=-2h latest=now index=* sourcetype=Cerner| head 100 | stats dc(\"event_list.participants.person_id\") as count "
            }
        ], 
        "scaleFactor": 3, 
        "value": "index=* sourcetype=Cerner \n| bucket_time span=1d \n| stats dc(\"event_list.participants.person_id\") as count by prsnl_name  _time", 
        "windowSize": 0
    }, 
    "Pages Printed Per User Per Day - Accelerated": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "| tstats summariesonly=t allow_old_summaries=t dc(user) as count from datamodel=printer where nodename=Print_Jobs by _time span=1d", 
        "description": [
            "Here, tstats is pulling in one command a super-fast count per user, per day.", 
            "(self-explanatory)"
        ], 
        "label": "Pages Printed Per User Per Day - Accelerated", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "Pages", 
        "outlierVariableSubject": "User", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a Printer data model (not default)", 
                "resolution": "This search requires Printer data. There is no default printer data model in Splunk, so you will need to define one in order to use an accelerated search. May we suggest building a data model with the fields user and Page_Count and then accelerate it?", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Printer where nodename=Print_Jobs earliest=-6h"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Printer data model must have a field called Page_Count defined", 
                "resolution": "You should have a field called \"Page_Count\" defined in your printer data model (referenced in tstats as Printer.Page_Count). If that's not currently present and accelerated, do so. If it's a different field name, provide that below.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t dc(Printer.Page_Count) as count from datamodel=Printer where nodename=Print_Jobs earliest=-6h"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Printer data model must have the user field defined", 
                "resolution": "You should have a field called \"User\" defined in your printer data model (referenced in tstats as Printer.User). If that's not currently present and accelerated, do so. If it's a different field name, provide that below.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t dc(Printer.Page_Count) as count from datamodel=Printer where nodename=Print_Jobs earliest=-6h"
            }
        ], 
        "scaleFactor": 1, 
        "value": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Printer where nodename=Print_Jobs groupby user, _time span=1d \n| eval comment=\"<--- We don't have a standard data model that includes pages printed, so you will need to build one to leverage data model acceleration... that said this is usually low volume enough to not be a big deal\"  ", 
        "windowSize": 0
    }, 
    "Pages Printed Per User Per Day - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_createUBA": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "| `Load_Sample_Log_Data(\"Printer Logs\")` | bucket _time span=1d | stats dc(User) as count by _time ", 
        "description": [
            "First we pull in our demo dataset.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Pages Printed Per User Per Day - Demo", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "Pages", 
        "outlierVariableSubject": "User", 
        "prereqs": [
            {
                "field": "uniflow_printer_log_sample.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "scaleFactor": 1, 
        "value": "| `Load_Sample_Log_Data(\"Printer Logs\")`  \n| bucket _time span=1d \n| stats sum(Page_Count) as Pages by User _time", 
        "windowSize": 0
    }, 
    "Pages Printed Per User Per Day - Live": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "index=* sourcetype=uniflow OR (source=win*system EventCode=307) | bucket _time span=1d stats dc(User) by _time ", 
        "description": [
            "First we pull in our printer dataset.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Pages Printed Per User Per Day - Live", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "Pages", 
        "outlierVariableSubject": "User", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Printer data", 
                "resolution": "This search requires Printer data. By default we are looking for either Uniflow logs (used from our demo data sample) or Windows Print Server logs. If you don't have this data right now, consider <a href=\"https://blogs.technet.microsoft.com/askperf/2008/08/12/two-minute-drill-enabling-print-queue-logging/\">ingesting it</a>! If you have other printer logs, go ahead and substitute the sourcetype below.", 
                "test": "earliest=-6h latest=now sourcetype=uniflow OR (source=win*system EventCode=307) index=* | head 100 | stats count dc(Page_Count) as pages dc(User) as users"
            }, 
            {
                "field": "pages", 
                "greaterorequalto": 1, 
                "name": "Must have a field called Page_Count defined", 
                "resolution": "You should have a field called \"Page_Count\" defined in your printer logs. If that's not currently extracted, build an extraction for it (or do an inline rex in the SPL below to work around this). Or just choose a different field below.", 
                "test": "earliest=-6h latest=now sourcetype=uniflow OR (source=win*system EventCode=307) index=* | head 100 | stats count dc(Page_Count) as pages dc(User) as users"
            }, 
            {
                "field": "users", 
                "greaterorequalto": 1, 
                "name": "Must have the user field defined", 
                "resolution": "You should have a field called \"User\" defined in your printer logs. If that's not currently extracted, build an extraction for it (or do an inline rex in the SPL below to work around this). Or just choose a different field below.", 
                "test": "earliest=-6h latest=now sourcetype=uniflow OR (source=win*system EventCode=307) index=* | head 100 | stats count dc(Page_Count) as pages dc(User) as users"
            }
        ], 
        "scaleFactor": 1, 
        "value": "index=*  sourcetype=uniflow OR (source=win*system EventCode=307) \n| bucket _time span=1d \n| stats sum(Page_Count) as Pages by User _time", 
        "windowSize": 0
    }, 
    "Spike in Email from Address - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 0, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Sender", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "| `Load_Sample_Log_Data(\"Email Logs\")`| bucket _time span=1d | stats dc(Sender) as count by _time ", 
        "description": [
            "First we pull in our demo dataset.", 
            "Then we filter for where the sending email address is @mycompany.com", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Spike in Email from Address - Demo", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "Sender", 
        "prereqs": [
            {
                "field": "Anonymized_Email_Logs.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "scaleFactor": 2, 
        "value": "| `Load_Sample_Log_Data(\"Email Logs\")`\n| search Sender=*@mycompany.com \n| bucket _time span=1d \n| stats count by Sender, _time", 
        "windowSize": 0
    }, 
    "Spike in Email from Address - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 0, 
        "actions_createRisk": 1, 
        "actions_riskObject": "sender", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "index=*  sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email src_user=* | bucket _time span=1d | stats dc(src_user) as count by _time ", 
        "description": [
            "First we pull in our email dataset.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per src_ip, per day."
        ], 
        "label": "Spike in Email from Address - Live", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "sender", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Email Data", 
                "resolution": "This search requires Email data. The out of the box field extractions support the Common Information Model, including Cisco ESA/Ironport and Microsoft Exchange. If you don't have this data today, we highly recommend ingesting it with the <a href=\"https://splunkbase.splunk.com/app/1761/\">Cisco ESA TA</a> or the <a href=\"https://splunkbase.splunk.com/app/3225/\">Splunk Add-on for Microsoft Exchange</a>. For best performance, accelerate the email data model from the <a href=\"https://splunkbase.splunk.com/app/1621/\">Common Information Model</a>!", 
                "test": "| tstats count where index=* sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email earliest=-4h"
            }
        ], 
        "scaleFactor": 2, 
        "value": "index=* sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email src_user=*\n| bucket _time span=1d \n| stats count by src_user, _time", 
        "windowSize": 0
    }, 
    "Spike in Password Reset Emails - Accelerated": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 0, 
        "actions_createRisk": 0, 
        "cardinalityTest": "| tstats summaries_only=t allow_old_summaries=t count from datamodel=Email by _time span=1d | eval count=1", 
        "description": [
            "Here, tstats is pulling in one command a super-fast count of emails where the subject contains \"Password Reset\" per src_ip, per day.", 
            "We're adding a Password Reset tag to this. You could also expand this out for multiple items, including new phishing campaigns."
        ], 
        "label": "Spike in Password Reset Emails - Accelerated", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "Detect_Type", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an Email data model", 
                "resolution": "This search requires an Email data. This is dependent on the <a href=\"https://splunkbase.splunk.com/app/1621/\">Common Information Model</a> being present, and also having your data mapped to CIM via appropriate TAs, usually with the out of the box field extractions from the <a href=\"https://splunkbase.splunk.com/app/1761/\">Cisco ESA TA</a>, the <a href=\"https://splunkbase.splunk.com/app/3225/\">Splunk Add-on for Microsoft Exchange</a>, etc.", 
                "test": "| tstats summaries_only=f allow_old_summaries=t count from datamodel=Email where earliest=-1h"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an Accelerated Email data model", 
                "resolution": "This search requires an accelerated Email data. In order to run a fast accelerated search, you should accelerate your data model. (<a href=\"https://docs.splunk.com/Documentation/Splunk/latest/HadoopAnalytics/Configuredatamodelacceleration#Accelerate_the_data_model\">docs</a>)", 
                "test": "| tstats summaries_only=t allow_old_summaries=t count from datamodel=Email where earliest=-1h"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Subjects (All_Email.subject) in your Accelerated Email data model", 
                "resolution": "This search assumes that you have actual source email addresses -- check your field extractions for src_user and then rebuild your data models if not.", 
                "test": "| tstats summaries_only=t allow_old_summaries=t dc(All_Email.subject) as count from datamodel=Email where earliest=-1h"
            }
        ], 
        "scaleFactor": 2, 
        "value": "| tstats summaries_only=t allow_old_summaries=t count from datamodel=Email where All_Email.subject=\"*Password Reset*\" by _time span=1d \n| eval Detect_Type=\"Password Reset\"", 
        "windowSize": 0
    }, 
    "Spike in Password Reset Emails - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 0, 
        "actions_createRisk": 0, 
        "cardinalityTest": "| `Load_Sample_Log_Data(\"Email Logs\")`| bucket _time span=1d | stats values(eval(\"1\")) by _time ", 
        "description": [
            "First we pull in our demo dataset.", 
            "Based on the message subject, tag it with a value for Detect_Type", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per detection type tag, per day."
        ], 
        "label": "Spike in Password Reset Emails - Demo", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "Detect_Type", 
        "prereqs": [
            {
                "field": "Anonymized_Email_Logs.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "scaleFactor": 2, 
        "value": "| `Load_Sample_Log_Data(\"Email Logs\")`\n| eval Detect_Type=\"\", Detect_Type=case(LIKE(Subject, \"%Password Reset%\"), \"Password Reset\", LIKE(Subject, \"%Validate Credentials%\"), \"Validate Credentials\",1=1, null) \n| bucket _time span=1d \n| stats count by _time Detect_Type ", 
        "windowSize": 0
    }, 
    "Spike in Password Reset Emails - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 0, 
        "actions_createRisk": 0, 
        "cardinalityTest": "index=*  sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email src_user=* | bucket _time span=1d | stats dc(src_user) as count by _time ", 
        "description": [
            "First we pull in our email dataset, with filters for Password Reset somewhere in the message.", 
            "Based on the message subject, tag it with a value for Detect_Type", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per detection type tag, per day."
        ], 
        "label": "Spike in Password Reset Emails - Live", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "Detect_Type", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Email Data", 
                "resolution": "This search requires Email data. The out of the box field extractions support the Common Information Model, including Cisco ESA/Ironport and Microsoft Exchange. If you don't have this data today, we highly recommend ingesting it with the <a href=\"https://splunkbase.splunk.com/app/1761/\">Cisco ESA TA</a> or the <a href=\"https://splunkbase.splunk.com/app/3225/\">Splunk Add-on for Microsoft Exchange</a>. For best performance, accelerate the email data model from the <a href=\"https://splunkbase.splunk.com/app/1621/\">Common Information Model</a>!", 
                "test": "| tstats count where index=* sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email earliest=-4h"
            }
        ], 
        "scaleFactor": 2, 
        "value": "index=* sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email Password Reset \n| eval Detect_Type=case(LIKE(Subject, \"%Password Reset%\"), \"Password Reset\", LIKE(Subject, \"%Validate Credentials%\"), \"Validate Credentials\",1=1, null) \n| bucket _time span=1d \n| stats count by _time Detect_Type ", 
        "windowSize": 0
    }, 
    "Spike in SFDC Documents Downloaded - Demo": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "USER_NAME", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "| `Load_Sample_Log_Data(\"SFDC Data\")` | search EVENT_TYPE=DocumentAttachmentDownloads | bucket _time span=1d | stats values(eval(\"1\")) as count by _time ", 
        "description": [
            "First we pull in our demo SFDC dataset.", 
            "Then we filter for what we're looking for in this use case, specifically the DocumentAttachmentDownloads EVENT_TYPE.", 
            "Then we enrich to convert the SFDC USER_ID into a friendly username via a lookup.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Spike in SFDC Documents Downloaded - Demo", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "USER_NAME", 
        "prereqs": [
            {
                "field": "SFDC_Sample_Data_Anon.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "scaleFactor": 2, 
        "value": "| `Load_Sample_Log_Data(\"SFDC Data\")` \n| search EVENT_TYPE=DocumentAttachmentDownloads \n| lookup SFDC_User_Lookup USER_ID \n| bucket _time span=1d \n| stats count by USER_NAME _time", 
        "windowSize": 0
    }, 
    "Spike in SFDC Documents Downloaded - Live": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "USER_NAME", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "index=sfdc EVENT_TYPE=DocumentAttachmentDownloads | bucket _time span=1d | stats dc(USER_ID) as count by _time ", 
        "description": [
            "First we pull in our SFDC dataset and filter for what we're looking for in this use case, specifically the DocumentAttachmentDownloads EVENT_TYPE.", 
            "Then we enrich to convert the SFDC USER_ID into a friendly username via a lookup.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Spike in SFDC Documents Downloaded - Live", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "USER_NAME", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Salesforce Data (assumes index=SFDC)", 
                "resolution": "This search requires data from the Salesforce Event Log File API. This is an additional fee from Salesforce, and can be effectively ingested and analyzed with the <a href=\"https://splunkbase.splunk.com/app/1931\">Splunk App for Salesforce</a>.", 
                "test": "| metasearch index=sfdc  earliest=-24h | head 100 | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Download Data (assumes index=SFDC)", 
                "resolution": "This search requires data from the Salesforce Event Log File API, and specifically DocumentAttachmentDownloads EVENT_TYPEs. It is assumed that will always be present if you are ingesting data from the Salesforce Event Log File, this check just validates that.", 
                "test": "| metasearch index=sfdc DocumentAttachmentDownloads earliest=-24h | head 100| stats count"
            }
        ], 
        "scaleFactor": 2, 
        "value": "index=sfdc EVENT_TYPE=DocumentAttachmentDownloads \n| lookup SFDC_User_Lookup USER_ID \n| bucket _time span=1d \n| stats count by USER_NAME _time", 
        "windowSize": 0
    }, 
    "Spike in SFDC Records Exported - Demo": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "USER_NAME", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "| `Load_Sample_Log_Data(\"SFDC Data\")` | search ROWS_PROCESSED>0 EVENT_TYPE=API OR EVENT_TYPE=BulkAPI OR EVENT_TYPE=RestAPI| bucket _time span=1d | stats values(eval(\"1\")) as count by _time ", 
        "description": [
            "First we pull in our demo SFDC dataset.", 
            "Then we filter for what we're looking for in this use case, specifically export EVENT_TYPEs with at least one ROWS_PROCESSED.", 
            "Then we enrich to convert the SFDC USER_ID into a friendly username via a lookup.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Spike in SFDC Records Exported - Demo", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "rows", 
        "outlierVariableSubject": "USER_NAME", 
        "prereqs": [
            {
                "field": "SFDC_Sample_Data_Anon.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "scaleFactor": 2, 
        "value": "| `Load_Sample_Log_Data(\"SFDC Data\")`  \n| search ROWS_PROCESSED>0 EVENT_TYPE=API OR EVENT_TYPE=BulkAPI OR EVENT_TYPE=RestAPI\n| lookup SFDC_User_Lookup USER_ID\n| bucket _time span=1d \n| stats sum(ROWS_PROCESSED) as rows by _time USER_NAME", 
        "windowSize": 0
    }, 
    "Spike in SFDC Records Exported - Live": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "USER_NAME", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "index=sfdc ROWS_PROCESSED>0 EVENT_TYPE=API OR EVENT_TYPE=BulkAPI OR EVENT_TYPE=RestAPI| bucket _time span=1d | stats dc(USER_ID) as count by _time ", 
        "description": [
            "First we pull in our SFDC dataset and filter for what we're looking for in this use case, specifically export EVENT_TYPEs with at least one ROWS_PROCESSED.", 
            "Then we enrich to convert the SFDC USER_ID into a friendly username via a lookup.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Spike in SFDC Records Exported - Live", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "rows", 
        "outlierVariableSubject": "USER_NAME", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Salesforce Data (assumes index=SFDC)", 
                "resolution": "This search requires data from the Salesforce Event Log File API. This is an additional fee from Salesforce, and can be effectively ingested and analyzed with the <a href=\"https://splunkbase.splunk.com/app/1931\">Splunk App for Salesforce</a>.", 
                "test": "| metasearch index=sfdc  earliest=-24h | head 100 | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Export Data (assumes index=SFDC)", 
                "resolution": "This search requires data from the Salesforce Event Log File API, and specifically the ROWS_PROCESSED by the API / BulkAPI / RestAPI EVENT_TYPEs. It is assumed that will always be present if you are ingesting data from the Salesforce Event Log File, this check just validates that.", 
                "test": "| metasearch index=sfdc API OR BulkAPI OR RestAPI earliest=-24h | head 100| stats count"
            }
        ], 
        "scaleFactor": 2, 
        "value": "index=sfdc ROWS_PROCESSED>0 EVENT_TYPE=API OR EVENT_TYPE=BulkAPI OR EVENT_TYPE=RestAPI\n| lookup SFDC_User_Lookup USER_ID\n| bucket _time span=1d \n| stats sum(ROWS_PROCESSED) as rows by _time USER_NAME", 
        "windowSize": 0
    }, 
    "Unique Hosts Logged Into Per Day - Accelerated": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "| tstats summariesonly=t allow_old_summaries=t dc(All_Traffic.src_ip) as count from datamodel=Network_Traffic by _time span=1d", 
        "description": [
            "Here, tstats is pulling in one command a super-fast count per user, per day.", 
            "I usually like to rename data model fields after my tstats, as it becomes much easier to use."
        ], 
        "label": "Unique Hosts Logged Into Per Day - Accelerated", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an accelerated Authentication data model", 
                "resolution": "This search requires an accelerated authentication data model to run. If it is not present, consider ingesting Windows Security data via the Splunk Universal Forwarder, and then accelerating it with the Common Information App from <a href=\"http://apps.splunk.com/\">apps.splunk.com</a>.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count  from datamodel=Authentication where earliest=-2h "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Authentication data model must have a field called dest defined", 
                "resolution": "You should have a field called \"dest\" defined in your accelerated Authentication data model (referenced in tstats as Authentication.dest). If it is not present, consider ingesting Windows Security data via the Splunk Universal Forwarder, and then accelerating it with the Common Information App from <a href=\"http://apps.splunk.com/\">apps.splunk.com</a>.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t dc(Authentication.dest) as count  from datamodel=Authentication where earliest=-2h"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Authentication data model must have the user field defined", 
                "resolution": "You should have a field called \"user\" defined in your accelerated Authentication data model (referenced in tstats as Authentication.user). If it is not present, consider ingesting Windows Security data via the Splunk Universal Forwarder, and then accelerating it with the Common Information App from <a href=\"http://apps.splunk.com/\">apps.splunk.com</a>.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t dc(Authentication.user) as count from datamodel=Authentication where earliest=-2h"
            }
        ], 
        "scaleFactor": 2, 
        "value": "| tstats summariesonly=t allow_old_summaries=t dc(Authentication.dest) as count  from datamodel=Authentication  groupby _time span=1d, Authentication.user \n| rename \"Authentication.user\" as user", 
        "windowSize": 0
    }, 
    "Unique Hosts Logged Into Per Day - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "|  `Load_Sample_Log_Data(\"Windows Logon Activity\")`| bucket _time span=1d | stats dc(user) as count by _time ", 
        "description": [
            "First we pull in our demo dataset.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Unique Hosts Logged Into Per Day - Demo", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "Sampled_AnonymizedLogonActivity.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "scaleFactor": 2, 
        "value": "|  `Load_Sample_Log_Data(\"Windows Logon Activity\")`\n| bucket _time span=1d \n| stats dc(anonymized_ComputerName) as count by user _time", 
        "windowSize": 0
    }, 
    "Unique Hosts Logged Into Per Day - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "cardinalityTest": "index=* source=\"winEventLog:Security\" (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) | bucket _time span=1d | stats dc(user) as count by  _time", 
        "description": [
            "First we pull in our Windows Security log dataset, filtering to logon Event IDs.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Finally, we can count and aggregate per user, per day."
        ], 
        "label": "Unique Hosts Logged Into Per Day - Live", 
        "outlierSearchType": "Avg", 
        "outlierVariable": "count", 
        "outlierVariableSubject": "user", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security data", 
                "resolution": "This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.", 
                "test": "| metasearch earliest=-2h latest=now index=* source=\"winEventLog:Security\" | head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Logon Success Data", 
                "resolution": "You should log logon events. There are many event IDs that we look for in the underlying logs, but they should all fall into the Audit Successful (or Failed) Logon events in your Windows Audit Policy. (<a href=\"https://technet.microsoft.com/en-us/library/cc431373.aspx\">docs</a>)", 
                "test": "source=\"winEventLog:Security\" index=* (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) | head 100 | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have the user field defined", 
                "resolution": "You should have a field called \"user\" defined in your Windows Security logs. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!", 
                "test": "source=\"winEventLog:Security\" earliest=-2h index=* (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) | head 100 | stats dc(user) as count"
            }
        ], 
        "scaleFactor": 2, 
        "value": "index=* source=\"winEventLog:Security\" (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) \n| bucket _time span=1d \n| stats dc(host) as count by user _time", 
        "windowSize": 0
    }
}