{
    "AWS Public Bucket - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "sourcetype", 
        "actions_riskObjectScore": 80, 
        "actions_riskObjectType": "other", 
        "description": [
            "There is.. lamentably no way to really show the search here. Use the Live Search to get the extreme detail of managing AWS logs!"
        ], 
        "label": "AWS Public Bucket - Demo", 
        "prereqs": [
            {
                "field": "UC_aws_public_buckets.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(AWS CloudTrail Public Bucket)` "
    }, 
    "AWS Public Bucket - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "sourcetype", 
        "actions_riskObjectScore": 80, 
        "actions_riskObjectType": "other", 
        "description": [
            "First we bring in AWS Cloudtrail logs, filtering for the PutBucketAcl events that occur when bucket permissions are changed, and filtering for any that include AllUsers.", 
            "Next, we extract the User who made the change, via the spath search command that will traverse the JSON easily.", 
            "Similarly, we extract the bucket name.", 
            "Here is where things get tricky -- AWS uses JSON arrays to show multiple permissions in one message. What we're doing here is extracting that block of ACLs -- spath will return a multi-value field to us. Then we can expand that into multiple events (so if before there were 2 events with 3 ACLs defined in each, we would end up with six events -- three copies of each original event, but the grantee field would be different). Finally, we can use spath to extract the values from each grantee field.", 
            "Next, we can search for just those individual permissions apply to all users (unauthenticated users). While we're missing some context here about who else has permissions, we can follow-on to investigate.", 
            "Last, we format the data a bit to meet our needs.", 
            "Now we calculate the distance using an approximation for the curvature of the earth. Easy, right? I do not understand it, I copy-pasted from https://answers.splunk.com/answers/317935/calculating-distances-between-points-with-geoip-us.html#answer-568451", 
            "Here we pull the date of the event, to make this easier to run over longer time windows.", 
            "Finally we use stats to collect all of the values into one line, per user, per day, and per set of locations. We're using some specific AWS data fields here -- if you're using a log source like VPN, then you might choose other fields."
        ], 
        "label": "AWS Public Bucket - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have AWS CloudTrail data (though could be applied to other data sources)", 
                "resolution": "In order to run this search, you must have AWS CloudTrail data onboard. Visit the <a href=\"/app/Splunk_Security_Essentials/data_source?technology=AWS%20CloudTrail\">data onboarding guide for AWS CloudTrail in this app</a>, or browse to <a href=\"https://splunkbase.splunk.com/app/1876/\">apps.splunk.com</a> for more information.", 
                "test": "| tstats count where earliest=-2h latest=now index=* sourcetype=aws:cloudtrail"
            }
        ], 
        "value": "index=* sourcetype=aws:cloudtrail AllUsers eventName=PutBucketAcl \n| spath output=userIdentityArn path=userIdentity.arn \n| spath output=bucketName path=\"requestParameters.bucketName\" \n| spath output=aclControlList path=\"requestParameters.AccessControlPolicy.AccessControlList\" | spath input=aclControlList output=grantee path=Grant{} | mvexpand grantee | spath input=grantee \n| search \"Grantee.URI\"=*AllUsers \n| table _time, Permission, Grantee.URI, bucketName, userIdentityArn | sort - _time"
    }, 
    "Access to Inscope Resources GDPR - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic demo dataset. In this case, anonymized Palo Alto Networks logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we filter for access to PII data (Workday though you can apply to any other sources).", 
            "Finally, we format just the data that users want to see."
        ], 
        "label": "Access to Inscope Resources GDPR - Demo", 
        "prereqs": [
            {
                "field": "od_splunklive_fw_data.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Sample Firewall Data)` \n| search app=workday* \n| table _time user app bytes* src_ip dest_ip"
    }, 
    "Access to Inscope Resources GDPR - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic dataset, Palo Alto Networks logs, filtered for access to PII data (Workday, though you can apply to any other data sources).", 
            "Finally, we format just the data that users want to see."
        ], 
        "label": "Access to Inscope Resources GDPR - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Firewall data", 
                "resolution": "This search requires Firewall or Netflow data to run. By default, we're checking for Common Information Model compliant data, and then also manually specifying the standard sourcetypes for Check Point, Palo Alto Networks, and Cisco ASAs. You should specify your particular index and sourcetype in the actual search to improve performance (or better yet, accelerate with the common information model!)", 
                "test": "(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)| head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an app field", 
                "resolution": "This search is also looking for firewall logs, but with the added filter of making sure that an app is defined.", 
                "test": "((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)) app=* | head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have workday in this data source (otherwise adjust the search)", 
                "resolution": "This search is also looking for firewall logs, but with the added filter of making sure that the workday app exists in the data", 
                "test": "((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)) app=workday* | head 100 | stats count "
            }
        ], 
        "value": "((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa) app=workday* \n| table _time user app bytes* src_ip dest_ip"
    }, 
    "Access to Inscope Resources Unencrypted GDPR - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic demo dataset. In this case, anonymized Palo Alto Networks logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we filter for access to PII data (Workday), over any non-HTTPS port. You can also detect this with any mechanism that allows you to analyze whether content is encrypted.", 
            "Finally, we format just the data that users want to see."
        ], 
        "label": "Access to Inscope Resources Unencrypted GDPR - Demo", 
        "prereqs": [
            {
                "field": "od_splunklive_fw_data.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Sample Firewall Data)` \n| search app=workday* dest_port!=443 \n| table _time user app bytes* src_ip dest_ip dest_port"
    }, 
    "Access to Inscope Resources Unencrypted GDPR - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic dataset, Palo Alto Networks logs, filtered for access to PII data (Workday), over any non-HTTPS port. You can also detect this with any mechanism that allows you to analyze whether content is encrypted.", 
            "Finally, we format just the data that users want to see."
        ], 
        "label": "Access to Inscope Resources Unencrypted GDPR - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Firewall data", 
                "resolution": "This search requires Firewall or Netflow data to run. By default, we're checking for Common Information Model compliant data, and then also manually specifying the standard sourcetypes for Check Point, Palo Alto Networks, and Cisco ASAs. You should specify your particular index and sourcetype in the actual search to improve performance (or better yet, accelerate with the common information model!)", 
                "test": "(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)| head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an app field", 
                "resolution": "This search is also looking for firewall logs, but with the added filter of making sure that an app is defined.", 
                "test": "((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)) app=* | head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have workday in this data source (otherwise adjust the search)", 
                "resolution": "This search is also looking for firewall logs, but with the added filter of making sure that the workday app exists in the data", 
                "test": "((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)) app=workday* | head 100 | stats count "
            }
        ], 
        "value": "((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa) app=workday* dest_port!=443 \n| table _time user app bytes* src_ip dest_ip dest_port"
    }, 
    "Activity Expired Identity GDPR - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic demo dataset. In this case, showing Interactive Logins. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we check a lookup that shows the user status (this would typically be pulled from SA-ldapsearch or ADMon).", 
            "Now we can filter for users where the expiration is at least a day ago (timezones are hard), or that are disabled.", 
            "Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR."
        ], 
        "label": "Activity Expired Identity GDPR - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Interactive Logins)` \n| lookup user_account_status user \n| where _time > relative_time(terminationDate, \"+1d\") \n| lookup gdpr_system_category host as dest OUTPUT category | search category=*"
    }, 
    "Activity Expired Identity GDPR - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic dataset. In this case, showing successful logins.", 
            "Next we check a lookup that shows the user status (this would typically be pulled from SA-ldapsearch or ADMon).", 
            "Now we can filter for users where the expiration is at least a day ago (timezones are hard), or that are disabled.", 
            "Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR."
        ], 
        "label": "Activity Expired Identity GDPR - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security data", 
                "resolution": "This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.", 
                "test": "| metasearch earliest=-2h latest=now sourcetype=\"*WinEventLog:Security\" index=* | head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Logon Success Data", 
                "resolution": "You should log logon events. There are many event IDs that we look for in the underlying logs, but they should all fall into the Audit Successful (or Failed) Logon events in your Windows Audit Policy. (<a href=\"https://technet.microsoft.com/en-us/library/cc431373.aspx\">docs</a>)", 
                "test": "sourcetype=\"*WinEventLog:Security\" (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) index=* | head 100 | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have the user field defined", 
                "resolution": "You should have a field called \"user\" defined in your Windows Security logs. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!", 
                "test": "sourcetype=\"*WinEventLog:Security\" earliest=-2h index=* (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) | head 100 | stats dc(user) as count"
            }
        ], 
        "value": "index=* (source=win*security OR sourcetype=linux_secure OR tag=authentication) user=* user!=\"\" action=success \n| lookup user_account_status.csv user \n| where _time > relative_time(terminationDate, \"+1d\") \n| lookup gdpr_system_category.csv host as dest OUTPUT category | search category=*"
    }, 
    "Aggregated Risk by Domains - Demo": {
        "actions_createNotable": 1, 
        "actions_createRisk": 0, 
        "description": [
            "First we bring in our basic demo dataset. In this case, a list of events from the risk index of a demo Splunk Enterprise Security environment. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Many people have asked for the ability to alert on events crossing different security domains. In OOTB ES correlation searches the standard format for a correlation search is \"Security Domain - Name of the Rule - Rule\", so this regex will extract the security domain.", 
            "In order to do this analysis, we are going to bucket by _time so we can look over many days. (This is more relevant for demo data.)", 
            "The heart of the search uses the stats command to calculate the # of security domains and the # of different correlation searches that fire per risk_object. We also include the values for those fields for contextual convenience.", 
            "Finally we can filter for risk_objects that have 3 or more different security domains or 5 or more searches firing. (Thresholds are somewhat arbitrary, but probably reasonable for many organizations.)"
        ], 
        "label": "Aggregated Risk by Domains - Demo", 
        "prereqs": [
            {
                "field": "UC_generic_risk_events.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup with Risk Events", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Risk Events)`\n| rex field=search_name \"^(?<security_domain>[\\w ]*) -\"  \n| bucket _time span=1d \n| stats values(security_domain) as security_domain dc(security_domain) as num_security_domain values(search_name) as search_name dc(search_name) as num_search_name by risk_object _time \n| where num_security_domain >= 3 OR num_search_name >= 5"
    }, 
    "Aggregated Risk by Domains - Live": {
        "actions_createNotable": 1, 
        "actions_createRisk": 0, 
        "description": [
            "First we bring in our dataset of events from the risk index of a Splunk Enterprise Security environment.", 
            "Many people have asked for the ability to alert on events crossing different security domains. In OOTB ES correlation searches the standard format for a correlation search is \"Security Domain - Name of the Rule - Rule\", so this regex will extract the security domain.", 
            "The heart of the search uses the stats command to calculate the # of security domains and the # of different correlation searches that fire per risk_object. We also include the values for those fields for contextual convenience.", 
            "Finally we can filter for risk_objects that have 3 or more different security domains or 5 or more searches firing. (Thresholds are somewhat arbitrary, but probably reasonable for many organizations.)"
        ], 
        "label": "Aggregated Risk by Domains - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a risk index", 
                "resolution": "This search presumes the presence of Splunk Enterprise Security to provide the Risk Framework. Reach out to your Splunk team to find out more about Splunk ES, or adapt this search for your own list of risk events.", 
                "test": "| metasearch index=risk | head 100"
            }
        ], 
        "value": "index=risk earliest=-7d\n| rex field=search_name \"^(?<security_domain>[\\w ]*) -\" \n| stats values(security_domain) as security_domain dc(security_domain) as num_security_domain values(search_name) as search_name dc(search_name) as num_search_name by risk_object _time \n| where num_security_domain >= 3 OR num_search_name >= 5"
    }, 
    "Aggregated Risk by Score - Demo": {
        "actions_createNotable": 1, 
        "actions_createRisk": 0, 
        "description": [
            "First we bring in our basic demo dataset. In this case, a list of events from the risk index of a demo Splunk Enterprise Security environment. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "This line is only needed for demo data (which, like us, gets older and more out of date with each passing moment) and so we use eventstats to calculate the most recent timestamp instead of just using eval's now() function.", 
            "Next we add up the total risk per risk_object. We do this over the entire lifetime of the dataset (30 days) in the field thirty_day_risk, and then we run just over the most recent day in the field one_day_risk. The latter is possible through the power of stats+eval which allows us to encode eval functions in our stats command.", 
            "While we could build a behavioral threshold, it's usually easier and more manageable to just use a set threshold to detect what amount of risk we're willing to tolerate. These can be adjusted from company-to-company, but the basic idea is to set a different threshold for recent bursty risk activities versus long term slow-and-low activities.", 
            "Here we filter for where either the one day risk level or the thirty day risk level are over their respective thresholds.", 
            "One final step. To make sure that we are being clear to the SOC analysts, we should indicate why we chose to surface this alert, so we use the same threshold to add a comment to the search."
        ], 
        "label": "Aggregated Risk by Score - Demo", 
        "prereqs": [
            {
                "field": "UC_generic_risk_events.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup with Risk Events", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Risk Events)` \n| eventstats max(_time) as maxtime \n| stats values(search_name) as search_names sum(risk_score) as thirty_day_risk sum(eval(if(_time > relative_time(maxtime, \"-1d@d\"),risk_score,0))) as one_day_risk by risk_object  \n| eval threshold_1day = 500, threshold_30day = 1200 \n| where one_day_risk>threshold_1day OR thirty_day_risk>threshold_30day \n| eval risk_score_reason = case(one_day_risk>threshold_1day, \"One Day Risk Score above \" . threshold_1day, thirty_day_risk>threshold_30day . \" on \" . strftime(now(), \"%m-%d-%Y\"), \"Thirty Day Risk Score above \" . threshold_30day)"
    }, 
    "Aggregated Risk by Score - Live": {
        "actions_createNotable": 1, 
        "actions_createRisk": 0, 
        "description": [
            "First we bring in our dataset of events from the risk index of a Splunk Enterprise Security environment.", 
            "Next we add up the total risk per risk_object. We do this over the entire lifetime of the dataset (30 days) in the field thirty_day_risk, and then we run just over the most recent day in the field one_day_risk. The latter is possible through the power of stats+eval which allows us to encode eval functions in our stats command.", 
            "While we could build a behavioral threshold, it's usually easier and more manageable to just use a set threshold to detect what amount of risk we're willing to tolerate. These can be adjusted from company-to-company, but the basic idea is to set a different threshold for recent bursty risk activities versus long term slow-and-low activities.", 
            "Here we filter for where either the one day risk level or the thirty day risk level are over their respective thresholds.", 
            "One final step. To make sure that we are being clear to the SOC analysts, we should indicate why we chose to surface this alert, so we use the same threshold to add a comment to the search."
        ], 
        "label": "Aggregated Risk by Score - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a risk index", 
                "resolution": "This search presumes the presence of Splunk Enterprise Security to provide the Risk Framework. Reach out to your Splunk team to find out more about Splunk ES, or adapt this search for your own list of risk events.", 
                "test": "| metasearch index=risk | head 100"
            }
        ], 
        "value": "index=risk earliest=-30d \n| stats values(search_name) as search_names sum(risk_score) as thirty_day_risk sum(eval(if(_time > relative_time(now(), \"-1d@d\"),risk_score,0))) as one_day_risk by risk_object  \n| eval threshold_1day = 500, threshold_30day = 1200 \n| where one_day_risk>threshold_1day OR thirty_day_risk>threshold_30day \n| eval risk_score_reason = case(one_day_risk>threshold_1day, \"One Day Risk Score above \" . threshold_1day, thirty_day_risk>threshold_30day . \" on \" . strftime(now(), \"%m-%d-%Y\"), \"Thirty Day Risk Score above \" . threshold_30day)"
    }, 
    "Basic Brute Force - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic demo dataset. In this case, Windows logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same hour.", 
            "Next we use the magic of stats+eval to count how many events there are where the action is success, or the action is failure", 
            "Finally we filter for where there is at least one success, and more than 100 failures."
        ], 
        "label": "Basic Brute Force - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Windows Brute Force)` \n| bucket _time span=1h \n| stats count(eval(action=\"success\")) as successes count(eval(action=\"failure\")) as failures by src _time \n| where successes>0 AND failures>100"
    }, 
    "Basic Brute Force - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic dataset. In this case, Windows logs.", 
            "Next we use the magic of stats+eval to count how many events there are where the action is success, or the action is failure", 
            "Finally we filter for where there is at least one success, and more than 100 failures."
        ], 
        "label": "Basic Brute Force - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security data", 
                "resolution": "This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.", 
                "test": "| metasearch earliest=-2h latest=now sourcetype=\"*WinEventLog:Security\" index=* | head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have the user field defined", 
                "resolution": "You should have a field called \"user\" defined in your Windows Security logs. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!", 
                "test": "sourcetype=\"*WinEventLog:Security\" earliest=-2h index=* | head 100 | stats dc(user) as count"
            }
        ], 
        "value": "index=* (source=win*security OR sourcetype=linux_secure OR tag=authentication) user=* user!=\"\"  \n| stats count(eval(action=\"success\")) as successes count(eval(action=\"failure\")) as failures by src dest \n| where successes>0 AND failures>100"
    }, 
    "Basic Expired Account - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic demo dataset. In this case, showing Interactive Logins. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we check a lookup that shows the user status (this would typically be pulled from SA-ldapsearch or ADMon).", 
            "Now we can filter for users where the expiration is at least a day ago (timezones are hard), or that are disabled."
        ], 
        "label": "Basic Expired Account - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Interactive Logins)` \n| lookup user_account_status user \n| where _time > relative_time(terminationDate, \"+1d\") "
    }, 
    "Basic Expired Account - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic dataset. In this case, successful logins.", 
            "Next we check a lookup that shows the user status (this would typically be pulled from SA-ldapsearch or ADMon).", 
            "Now we can filter for users where the expiration is at least a day ago (timezones are hard), or that are disabled."
        ], 
        "label": "Basic Expired Account - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security data", 
                "resolution": "This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.", 
                "test": "| metasearch earliest=-2h latest=now sourcetype=\"*WinEventLog:Security\" index=* | head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Logon Success Data", 
                "resolution": "You should log logon events. There are many event IDs that we look for in the underlying logs, but they should all fall into the Audit Successful (or Failed) Logon events in your Windows Audit Policy. (<a href=\"https://technet.microsoft.com/en-us/library/cc431373.aspx\">docs</a>)", 
                "test": "sourcetype=\"*WinEventLog:Security\" (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) index=* | head 100 | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have the user field defined", 
                "resolution": "You should have a field called \"user\" defined in your Windows Security logs. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!", 
                "test": "sourcetype=\"*WinEventLog:Security\" earliest=-2h index=* (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) | head 100 | stats dc(user) as count"
            }
        ], 
        "value": "index=* (source=win*security OR sourcetype=linux_secure OR tag=authentication) user=* user!=\"\" action=success \n| lookup user_account_status.csv user \n| where _time > relative_time(terminationDate, \"+1d\")"
    }, 
    "Basic Malware Outbreak - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Computer_Name", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic demo dataset, Symantec Endpoint Protection Risks. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "While there are several approaches to grouping events, and stats is the fastest, we're using transaction because it's the easiest. This will let us group all the events based on the Risk_Name.", 
            "Finally, we can look to see if there are more than three different computers that have been affected."
        ], 
        "label": "Basic Malware Outbreak - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Symantec Endpoint Protection Risks)` \n| transaction maxpause=1d Risk_Name \n| where mvcount(Computer_Name)>3"
    }, 
    "Basic Malware Outbreak - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Computer_Name", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic dataset, Symantec Endpoint Protection Risks, over the last 24 hours.", 
            "While there are several approaches to grouping events, and stats is the fastest, we're using transaction because it's the easiest. This will let us group all the events based on the Risk_Name.", 
            "Finally, we can look to see if there are more than three different computers that have been affected."
        ], 
        "label": "Basic Malware Outbreak - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Symantec AV data", 
                "resolution": "For simplicity, this search is written specifically for Symantec AV data, but it can be easily modified for other sources. ", 
                "test": "| metasearch earliest=-24h latest=now index=* sourcetype=symantec:ep:* | head 100 | stats count "
            }
        ], 
        "value": "index=* sourcetype=symantec:*  earliest=-24h\n| transaction maxpause=1d Risk_Name \n| where mvcount(Computer_Name)>3"
    }, 
    "Basic Scanning - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic demo dataset, Firewall Logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same hour.", 
            "Next, stats gives us the distinct count (aka unique count) of ips and ports that were used per source IP, per hour.", 
            "Finally we can filter for more than 1000 src_ips or dest_ports."
        ], 
        "label": "Basic Scanning - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Sample Firewall Data)` \n| bucket _time span=1h \n| stats dc(dest_port) as num_dest_port dc(dest_ip) as num_dest_ip by src_ip, _time \n| where num_dest_port > 1000 OR num_dest_ip > 1000"
    }, 
    "Basic Scanning - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic dataset, Firewall Logs, from the last hour.", 
            "Next, stats gives us the distinct count (aka unique count) of ips and ports that were used per source IP.", 
            "Finally we can filter for more than 1000 src_ips or dest_ports."
        ], 
        "label": "Basic Scanning - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Firewall data", 
                "resolution": "This search requires Firewall or Netflow data to run. By default, we're checking for Common Information Model compliant data, and then also manually specifying the standard sourcetypes for Check Point, Palo Alto Networks, and Cisco ASAs. You should specify your particular index and sourcetype in the actual search to improve performance (or better yet, accelerate with the common information model!)", 
                "test": "(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)| head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a dest_ip and dest_port field", 
                "resolution": "This search is also looking for firewall logs, but with the added filter of making sure that a dest_ip and dest_port defined.", 
                "test": "((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)) dest_ip=* dest_port=*| head 100 | stats count "
            }
        ], 
        "value": "(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa) earliest=-1h \n| stats dc(dest_port) as num_dest_port dc(dest_ip) as num_dest_ip by src_ip \n| where num_dest_port > 1000 OR num_dest_ip > 1000"
    }, 
    "Brute Force GDPR - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic demo dataset. In this case, Windows logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same hour.", 
            "Next we use the magic of stats+eval to count how many events there are where the action is success, or the action is failure", 
            "Finally we filter for where there is at least one success, and more than 100 failures."
        ], 
        "label": "Brute Force GDPR - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\" \n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Windows Brute Force)` \n| lookup gdpr_system_category host as dest | search category=* \n| bucket _time span=1h \n| stats count(eval(action=\"success\")) as successes count(eval(action=\"failure\")) as failures by src _time \n| where successes>0 AND failures>100"
    }, 
    "Brute Force GDPR - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic dataset. In this case, Windows logs.", 
            "Next we use the magic of stats+eval to count how many events there are where the action is success, or the action is failure", 
            "Next we filter for where there is at least one success, and more than 100 failures.", 
            "Finally we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR."
        ], 
        "label": "Brute Force GDPR - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security data", 
                "resolution": "This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.", 
                "test": "| metasearch earliest=-2h latest=now sourcetype=\"*WinEventLog:Security\" index=* | head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have the user field defined", 
                "resolution": "You should have a field called \"user\" defined in your Windows Security logs. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!", 
                "test": "sourcetype=\"*WinEventLog:Security\" earliest=-2h index=* | head 100 | stats dc(user) as count"
            }
        ], 
        "value": "index=* (source=win*security OR sourcetype=linux_secure OR tag=authentication) user=* user!=\"\"  \n| stats count(eval(action=\"success\")) as successes count(eval(action=\"failure\")) as failures by src dest \n| where successes>0 AND failures>100  \n| lookup gdpr_system_category host as dest| search category=* "
    }, 
    "Brute Force Slow and Low GDPR - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic demo dataset. In this case, Windows logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same hour.", 
            "Next we use the magic of stats+eval to count how many events there are where the action is success, or the action is failure", 
            "Finally we filter for where there is at least one success, and more than 100 failures."
        ], 
        "label": "Brute Force Slow and Low GDPR - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Windows Brute Force)` \n| lookup gdpr_system_category host as dest | search category=* \n| bucket _time span=1d \n| stats count(eval(action=\"success\")) as successes count(eval(action=\"failure\")) as failures by src _time \n| where successes>0 AND failures>100"
    }, 
    "Brute Force Slow and Low GDPR - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic dataset. In this case, Windows logs.", 
            "Next we use the magic of stats+eval to count how many events there are where the action is success, or the action is failure", 
            "Finally we filter for where there is at least one success, and more than 100 failures.", 
            "Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR."
        ], 
        "label": "Brute Force Slow and Low GDPR - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security data", 
                "resolution": "This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.", 
                "test": "| metasearch earliest=-2h latest=now sourcetype=\"*WinEventLog:Security\" index=* | head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Logon Success Data", 
                "resolution": "You should log logon events. There are many event IDs that we look for in the underlying logs, but they should all fall into the Audit Successful (or Failed) Logon events in your Windows Audit Policy. (<a href=\"https://technet.microsoft.com/en-us/library/cc431373.aspx\">docs</a>)", 
                "test": "sourcetype=\"*WinEventLog:Security\" (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) index=* | head 100 | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have the user field defined", 
                "resolution": "You should have a field called \"user\" defined in your Windows Security logs. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!", 
                "test": "sourcetype=\"*WinEventLog:Security\" earliest=-2h index=* (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) | head 100 | stats dc(user) as count"
            }
        ], 
        "value": "index=* (source=win*security OR sourcetype=linux_secure OR tag=authentication) user=* user!=\"\"  \n| stats count(eval(action=\"success\")) as successes count(eval(action=\"failure\")) as failures by src dest  \n| where successes>0 AND failures>100  \n| lookup gdpr_system_category host as dest | search category=* "
    }, 
    "Build Departmental Peer Group - Demo": {
        "actions_createNotable": 0, 
        "actions_createRisk": 0, 
        "actions_riskObjectScore": 0, 
        "description": [
            "First we bring in our basic demo dataset. In this case, anonymized LDAPSearch Output. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we filter for just users that have a department defined.", 
            "Renaming sAMAccountName makes things easier to type, and also conforms to the Splunk Common Information Model", 
            "Now we focus on just our most relevant fields.", 
            "Eventstats allows us to distribute values across multiple events, without changing any existing values. In this case, we're taking all of the values for username per department, and adding them together. Every row will have three fields, the department name, the user name, and a multi-value field with each member of that department.", 
            "Multi-value fields get tricky when we persist them to disk. I always control the process, by making it a single value, comma-separated list.", 
            "Finally, we output this to disk."
        ], 
        "label": "Build Departmental Peer Group - Demo", 
        "prereqs": [
            {
                "field": "UC_raw_data_for_privilege_calculations.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(LDAP Data for Privilege Analysis)` \n| search department=* \n| rename sAMAccountName as user \n| table department user \n| eventstats values(user) as peergroup by department \n| eval peergroup=mvjoin(peergroup, \",\")\n| outputlookup myPeerGroup.csv"
    }, 
    "Build Departmental Peer Group - Live": {
        "actions_createNotable": 0, 
        "actions_createRisk": 0, 
        "actions_riskObjectScore": 0, 
        "description": [
            "Okay, this is pretty tricky. Because ldapsearch (at least in my testing) doesn't allow us to look for users who are members of particular groups (which would be so much simpler, much like the VIPs from LDAP example), we have to first pull the list of privileged groups and then look for the information of every user in those groups. We will start by pulling the list of privileged groups. As noted in the How To Implement, there are many different approaches to ingesting this data -- ldapsearch is generally the simplest to get started with.", 
            "Renaming sAMAccountName makes things easier to type, and also conforms to the Splunk Common Information Model", 
            "Now we focus on just our most relevant fields. ldapsearch will also give us several other fields that we don't care about (such as _raw), and | table will get rid of them while putting the output in the proper display mode.", 
            "Eventstats allows us to distribute values across multiple events, without changing any existing values. In this case, we're taking all of the values for username per department, and adding them together. Every row will have three fields, the department name, the user name, and a multi-value field with each member of that department.", 
            "Multi-value fields get tricky when we persist them to disk. I always control the process, by making it a single value, comma-separated list.", 
            "Finally, we output this to disk."
        ], 
        "label": "Build Departmental Peer Group - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Splunk Supporting Add-on for Active Directory Installed", 
                "resolution": "The Splunk Supporting Add-on for Active Directory app allows us to query AD environments via LDAP to get everything we need. Check it out -- <a href=\"https://splunkbase.splunk.com/app/1151/\" target=\"_blank\">app link</a>, <a href=\"http://docs.splunk.com/Documentation/SA-LdapSearch/2.1.6/User/AbouttheSplunkSupportingAdd-onforActiveDirectory\" target=\"_blank\">docs link</a>.", 
                "test": "| rest /services/apps/local | search disabled=0 title=\"SA-ldapsearch\" | stats count"
            }
        ], 
        "value": "| ldapsearch domain=splunk.local search=\"(&(objectclass=user)(!(objectClass=computer))(department=*))\" attrs=\"sAMAccountName,department\"\n| rename sAMAccountName as user \n| table department user \n| eventstats values(user) as peergroup by department \n| eval peergroup=mvjoin(peergroup, \",\")\n| outputlookup myPeerGroup.csv"
    }, 
    "Codeword Threshold Departments - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic demo dataset. In this case, anonymized Confluence logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we filter for just search history in Confluence.", 
            "While you wouldn't have to do this with live data, for our sample data we're going to extract out the queryString explicitly.", 
            "Next we use eval's urldecode function to convert plus signs to spaces, and any other url encoding that might exist.", 
            "Now that we have everything looking clearly, we're going to use a regex to extract project code names from the search string. Normally with rex, you would include the regular expression here as a quoted string (much less scary). We're going to make this more complicated by using a subsearch, but it has the benefit of requiring that you don't have to enter the data twice. We'll explain the subsearch in the next line.", 
            "The goal of this line is to return a single string with the list of all the project code names in a field extraction, like \"(?<codeword_matched>Project Eagle|Project Leprechaun)\". This is fairly advanced SPL, so don't worry if it doesn't make sense to you, but the meat is that we have a lookup called sse_project_codenames that has a column for codeword, and a column for department. When we use a subsearch that has a field named \"search\" it will be literally interpreted by most parts of SPL, so it will insert the regex happily. If this doesn't make sense to you, read up on subsearches on docs.splunk.com, and then you can always just copy-paste the demo SPL and try it out!", 
            "Now we use the lookup command so that we can understand what department every project codename belongs to, pulled from the CSV file.", 
            "For simplicity, we want to group events together based on the day (you might look at this based on the hour, you might use the transaction command to give you a rolling window -- there are lots of approaches).", 
            "Now we use stats to look at the distinct count of department (the number of unique departments) whose codewords were searched by user, by day.", 
            "Finally we filter for where people have looked at five or more different departments."
        ], 
        "label": "Codeword Threshold Departments - Demo", 
        "prereqs": [
            {
                "field": "UC_anonymized_confluence_logs.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(\"Confluence Logs\")` \n| search url=\"*dosearchsite*\" \n| rex field=url \"queryString=(?<queryString>.*)\" \n| eval queryString=urldecode(queryString) \n| rex field=queryString max_match=10 \n[| inputlookup sse_project_codenames | stats values(codeword) as search | eval search=mvjoin(search, \"|\") | eval search=\"\\\"(?<codeword_matched>\" . search . \")\\\"\"]  \n| lookup sse_project_codenames codeword as codeword_matched \n| bucket _time span=1d \n| stats dc(department) as count by user, _time \n| where count >= 5"
    }, 
    "Codeword Threshold Departments - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our dataset, filtered for just the search history in Confluence.", 
            "Next we use eval's urldecode function to convert plus signs to spaces, and any other url encoding that might exist.", 
            "Now that we have everything looking clearly, we're going to use a regex to extract project code names from the search string. Normally with rex, you would include the regular expression here as a quoted string (much less scary). We're going to make this more complicated by using a subsearch, but it has the benefit of requiring that you don't have to enter the data twice. We'll explain the subsearch in the next line.", 
            "The goal of this line is to return a single string with the list of all the project code names in a field extraction, like \"(?<codeword_matched>Project Eagle|Project Leprochaun)\". This is fairly advanced SPL, so don't worry if it doesn't make sense to you, but the meat is that we have a lookup called sse_project_codenames that has a column for codeword, and a column for department. When we use a subsearch that has a field named \"search\" it will be literally interpreted by most parts of SPL, so it will insert the regex happily. If this doesn't make sense to you, read up on subsearches on docs.splunk.com, and then you can always just copy-paste the demo SPL and try it out!", 
            "Now we use the lookup command so that we can understand what department every project codename belongs to, pulled from the CSV file.", 
            "For simplicity, we want to group events together based on the day (you might look at this based on the hour, you might use the transaction command to give you a rolling window -- there are lots of approaches).", 
            "Now we use stats to look at the distinct count of department (the number of unique departments) whose codewords were searched by user, by day.", 
            "Finally we filter for where people have looked at five or more different departments."
        ], 
        "label": "Codeword Threshold Departments - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Confluence logs", 
                "resolution": "Ingest the logs from your Confluence server via the Universal Forwarder.", 
                "test": "index=* source=\"/opt/confluence/*/access_log*\" hoursago=4 | head 100 | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Confluence search logs", 
                "resolution": "dosearchsite is in the URL for searches on Confluence. If there is another manner for searching internal content, adjust this search string accordingly.", 
                "test": "index=* source=\"/opt/confluence/*/access_log*\" hoursago=8 dosearchsite | head 100 | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "queryString must be extracted", 
                "resolution": "Extract the queryString field via something like | rex \"queryString=(?<queryString>[^ ?\\\"]*)\" ", 
                "test": "index=* source=\"/opt/confluence/*/access_log*\" dosearchsite hoursago=4 queryString=* | head 100 | stats count"
            }
        ], 
        "value": "index=* source=\"/opt/confluence/*/access_log*\" dosearchsite \n| eval queryString=urldecode(queryString) \n| rex field=queryString max_match=10 \n[| inputlookup sse_project_codenames | stats values(codeword) as search | eval search=mvjoin(search, \"|\") | eval search=\"\\\"(?<codeword_matched>\" . search . \")\\\"\"]  \n| lookup sse_project_codenames codeword as codeword_matched \n| bucket _time span=1d \n| stats dc(department) as count by user, _time \n| where count >= 5"
    }, 
    "Command line length statistical analysis - Demo": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our demo dataset. This could be any EDR data source that provides the full CLI string.", 
            "Because we just care about process launches, we filter for EventCode 1, which is how Sysmon denotes a process launch.", 
            "Next we use eval to calculate how long the command line (file path + command line args) is.", 
            "Eventstats is like stats, but just adds the results to your existing dataset. So this will give us the avg and stdev per each host.", 
            "Finally we use stats to roll up multiple process launches, giving us the length of that cli string alongside the avg and stdev of the host, per host, per cli string.", 
            "With that setup, we can find processes that have substantially longer cli strings (10 * the standard deviation) than the average on this system."
        ], 
        "label": "Command line length statistical analysis - Demo", 
        "prereqs": [
            {
                "field": "UC_malicious_cmdline.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| inputlookup UC_malicious_cmdline \n| search EventCode=1 \n| eval cmdlen=len(CommandLine) \n| eventstats stdev(cmdlen) as stdev,avg(cmdlen) as avg by host \n| stats max(cmdlen) as maxlen, values(stdev) as stdevperhost, values(avg) as avgperhost by host,CommandLine \n| where maxlen> ((10*stdevperhost) + avgperhost)"
    }, 
    "Command line length statistical analysis - Live": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our basic dataset, which consists of XML format Sysmon logs from the endpoints (ingested via the Sysmon TA). This could be any EDR data source that provides the full CLI string. Because we're looking for process launches, we then filter for EventCode=1 (the Sysmon Process Launch code).", 
            "Next we use eval to calculate how long the command line (file path + command line args) is.", 
            "Eventstats is like stats, but just adds the results to your existing dataset. So this will give us the avg and stdev of the command line length per host.", 
            "Finally we use stats to roll up multiple process launches, giving us the length of that cli string alongside the avg and stdev of the host, per host, per cli string.", 
            "With that setup, we can find processes that have substantially longer cli strings (10 * the standard deviation) than the average on this system."
        ], 
        "label": "Command line length statistical analysis - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Microsoft Sysmon logs", 
                "resolution": "Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=\"https://splunkbase.splunk.com/app/1914/\">Splunk App</a>. Check out the <a href=\"http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf\">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!", 
                "test": "| metasearch index=* sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\"  earliest=-1h latest=now | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have process start events (EventCode=1)", 
                "resolution": "Check your Sysmon configuration file to ensure you are not filtering out EventCode 1 events.", 
                "test": "sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\" EventCode=1 index=* | head 100 | stats count"
            }
        ], 
        "value": "index=* sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\" EventCode=1 \n| eval cmdlen=len(CommandLine) \n| eventstats stdev(cmdlen) as stdev,avg(cmdlen) as avg by host \n| stats max(cmdlen) as maxlen, values(stdev) as stdevperhost, values(avg) as avgperhost by host,CommandLine \n| where maxlen> ((10*stdevperhost) + avgperhost)"
    }, 
    "Deleting USN Journal Log - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our basic demo data", 
            "Next we look for any instances of fsutil being launched (EventCode 1 indicates a process launch), and filter to make sure our suspicious fields re in the CommandLine string.", 
            "Then we put the data into a table because that's the easiest thing to use."
        ], 
        "label": "Deleting USN Journal Log - Demo", 
        "prereqs": [
            {
                "field": "UC_fsutil.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| inputlookup UC_fsutil \n| search EventCode=1 Image=*fsutil* CommandLine=*usn* CommandLine=*deletejournal* \n| table _time host Image CommandLine"
    }, 
    "Deleting USN Journal Log - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our Sysmon data (though any EDR / process launch data containing the command line string would suffice). We are looking for any instances of fsutil being launched (EventCode 1 indicates a process launch), and filtering to make sure our suspicious fields re in the CommandLine string.", 
            "Then we put the data into a table because that's the easiest thing to use."
        ], 
        "label": "Deleting USN Journal Log - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Microsoft Sysmon logs", 
                "resolution": "Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=\"https://splunkbase.splunk.com/app/1914/\">Splunk App</a>. Check out the <a href=\"http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf\">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!", 
                "test": "| metasearch index=* sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\"  earliest=-1h latest=now | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have process start events (EventCode=1)", 
                "resolution": "Check your Sysmon configuration file to ensure you are not filtering out EventCode 1 events.", 
                "test": "sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\" EventCode=1 index=* | head 100 | stats count"
            }
        ], 
        "value": "index=* sourcetype=XmlWinEventLog:Microsoft-Windows-Sysmon/Operational EventCode=1 Image=*fsutil* CommandLine=*usn* CommandLine=*deletejournal* \n| table _time host Image CommandLine"
    }, 
    "Detect SMB Traffic Allowed - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our demo dataset of Firewall logs", 
            "Next we filter for just SMB connections.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the last day.", 
            "Now we are looking at the number of connections per source / dest IP pair per day, over our time range"
        ], 
        "label": "Detect SMB Traffic Allowed - Demo", 
        "prereqs": [
            {
                "field": "UC_smb_traffic_allowed.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| inputlookup UC_smb_traffic_allowed \n| search action=allowed (app=smb OR dest_port=139 OR dest_port=445) \n| bucket _time span=1d \n| stats count by _time src_ip dest_ip dest_port"
    }, 
    "Detect SMB Traffic Allowed - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our basic dataset, which comes from Firewall Logs for SMB connections.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the last day.", 
            "Now we are looking at the number of connections per source / dest IP pair per day, over our time range"
        ], 
        "label": "Detect SMB Traffic Allowed - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have network traffic.", 
                "resolution": "Ingest network traffic logs, consider using Splunk Stream.", 
                "test": "index=* ((tag=network tag=communicate) OR (sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco:asa OR sourcetype=stream*)) earliest=-1h latest=now | stats count"
            }
        ], 
        "value": "index=* ((tag=network tag=communicate) OR (sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco:asa OR sourcetype=stream* )) action=allowed (app=smb OR dest_port=139 OR dest_port=445) \n| bucket _time span=1d \n| stats count by _time src_ip dest_ip dest_port"
    }, 
    "Detecting WMI Remote Process Creation - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our basic demo data", 
            "Next we look for any instances of WMIC (Windows Management Instrumentation Command-line) being launched (EventCode 1 indicates a process launch), and filter to make sure our suspicious fields are in the CommandLine string.", 
            "Then we put the data into a table because that's the easiest thing to use."
        ], 
        "label": "Detecting WMI Remote Process Creation - Demo", 
        "prereqs": [
            {
                "field": "UC_wmi.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| inputlookup UC_wmi \n| search EventCode=1 Image=*wmic* CommandLine=*node* CommandLine=\"*process call create*\" \n| table _time host Image CommandLine"
    }, 
    "Detecting WMI Remote Process Creation - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our Sysmon EDR (though any other process launch logs with the full command line would suffice) data. We look for any instances of WMIC (Windows Management Instrumentation Command-line) being launched (EventCode 1 indicates a process launch), and filter to make sure our suspicious fields are in the CommandLine string.", 
            "Then we put the data into a table because that's the easiest thing to use."
        ], 
        "label": "Detecting WMI Remote Process Creation - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Microsoft Sysmon logs", 
                "resolution": "Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=\"https://splunkbase.splunk.com/app/1914/\">Splunk App</a>. Check out the <a href=\"http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf\">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!", 
                "test": "| metasearch index=* sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\"  earliest=-1h latest=now | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have process start events (EventCode=1)", 
                "resolution": "Check your Sysmon configuration file to ensure you are not filtering out EventCode 1 events.", 
                "test": "sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\" EventCode=1 index=* | head 100 | stats count"
            }
        ], 
        "value": "index=* sourcetype=XmlWinEventLog:Microsoft-Windows-Sysmon/Operational EventCode=1 Image=*wmic* CommandLine=*node* CommandLine=\"*process call create*\" \n| table _time host Image CommandLine"
    }, 
    "DynDNS - Accelerated": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "This uses tstats to quickly search an accelerated Web Proxy data model.", 
            "To make the search easier to use, we rename the Data Model fields into friendly versions by removing the Web. from the beginning.", 
            "Because we are looking for dynamic dns providers, we're going to need to separate out subdomains from the registered domain. URL Toolbox is just the tool for this job!", 
            "Next we can use our lookup of ddns domains (see How to Implement). This will add a field called inlist with the value \"true\" for any matches.", 
            "And finally we can look for those records that are matches.", 
            "With our dataset complete, we just need to arrange the fields to be useful."
        ], 
        "label": "DynDNS - Accelerated", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Proxy data in the Web data model", 
                "resolution": "Proxy data can come in many forms, including from Palo Alto Networks and other NGFWs, dedicated proxies like BlueCoat, or network monitoring tools like Splunk Stream or bro. You must also have the Common Information Model app, and have the appropriate TAs installed so that your data is mapped. Follow our in-app data onboarding guides for examples of how to do this (or leverage the non-accelerated version).", 
                "test": "| tstats count from datamodel=Web where earliest=-1h"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an accelerated Web data model", 
                "resolution": "The Web data model must have the acceleration check-box hit, and must have made decent progress. ", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Web where earliest=-1h"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a bytes_out field", 
                "resolution": "Your proxy must record how much data was sent outbound", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Web where earliest=-1h Web.bytes_out>1"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a user field", 
                "resolution": "Your proxy must record how much data was sent outbound", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Web where earliest=-1h Web.user=*"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have URL Toolbox Installed", 
                "resolution": "The URL Toolbox app, written by Cedric Le Roux, provides effective URL Parsing. Download <a href=\"https://splunkbase.splunk.com/app/2734/\">here</a>.", 
                "test": "| rest /services/apps/local | search disabled=0 label=\"URL Toolbox\" | stats count"
            }
        ], 
        "value": "| tstats summariesonly=t allow_old_summaries=t count values(Web.user) as user values(Web.app) as app values(Web.src) as src values(Web.dest) as dest from datamodel=Web where nodename=Web.proxy by Web.url _time span=1s \n| rename Web.* as * \n| eval list=\"mozilla\" | `ut_parse_extended(url,list)`\n| lookup dynamic_dns_lookup domain as ut_domain OUTPUT inlist\n| search inlist=true\n| table _time ut_domain inlist bytes* uri dest src user app"
    }, 
    "DynDNS - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic demo dataset, proxy logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Because we are looking for dynamic dns providers, we're going to need to separate out subdomains from the registered domain. URL Toolbox is just the tool for this job!", 
            "Next we can use our lookup of ddns domains (see How to Implement). This will add a field called inlist with the value \"true\" for any matches.", 
            "And finally we can look for those records that are matches.", 
            "With our dataset complete, we just need to arrange the fields to be useful."
        ], 
        "label": "DynDNS - Demo", 
        "prereqs": [
            {
                "field": "bots-webproxy-data.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have URL Toolbox Installed", 
                "resolution": "The URL Toolbox app, written by Cedric Le Roux, provides effective URL Parsing. Download <a href=\"https://splunkbase.splunk.com/app/2734/\">here</a>.", 
                "test": "| rest /services/apps/local | search disabled=0 label=\"URL Toolbox\" | stats count"
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Web Proxy Logs)`\n| eval list=\"mozilla\" | `ut_parse_extended(uri,list)`\n| lookup dynamic_dns_lookup domain as ut_domain OUTPUT inlist\n| search inlist=true \n| table _time ut_domain inlist bytes* uri"
    }, 
    "DynDNS - Live": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our dataset of proxy logs.", 
            "Because we are looking for dynamic dns providers, we're going to need to separate out subdomains from the registered domain. URL Toolbox is just the tool for this job!", 
            "Next we can use our lookup of ddns domains (see How to Implement). This will add a field called inlist with the value \"true\" for any matches.", 
            "And finally we can look for those records that are matches.", 
            "With our dataset complete, we just need to arrange the fields to be useful."
        ], 
        "label": "DynDNS - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Proxy data", 
                "resolution": "Proxy data can come in many forms, including from Palo Alto Networks and other NGFWs, dedicated proxies like BlueCoat, or network monitoring tools like Splunk Stream or bro.", 
                "test": "| metasearch earliest=-2h latest=now index=* sourcetype=pan:threat OR (sourcetype=opsec URL Filtering) OR sourcetype=bluecoat:proxysg* OR sourcetype=websense* | head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have URL Toolbox Installed", 
                "resolution": "The URL Toolbox app, written by Cedric Le Roux, provides effective URL Parsing. Download <a href=\"https://splunkbase.splunk.com/app/2734/\">here</a>.", 
                "test": "| rest /services/apps/local | search disabled=0 label=\"URL Toolbox\" | stats count"
            }
        ], 
        "value": "index=* sourcetype=pan:threat OR (tag=web tag=proxy) earliest=-20m@m earliest=-5m@m \n| eval list=\"mozilla\" | `ut_parse_extended(url,list)`\n| lookup dynamic_dns_lookup domain as ut_domain OUTPUT inlist\n| search inlist=true \n| table _time ut_domain inlist bytes* uri"
    }, 
    "Emails With Lookalike Domains - Accelerated": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 0, 
        "description": [
            "First we ask tstats to give us a list of source addresses for emails (with a count), and we rename it so that it's easier to work with.", 
            "Next we are going to extract the domain.", 
            "Now we aggregate per actual domain we will analyze, for performance reasons", 
            "Let's filter out any domains that our organization owns and expects to receive email from. You can have several domains here (I recommend no more than 10-20 -- eventually urltoolbox will get tired and stop doing adding Levenshtein fields, so you can look for null ut_levenshtein later if you are pushing this boundary).", 
            "Now we use the free URL Toolbox app to parse out subdomains from the top level domains. We want to analyze each one, so that an attacker can't send mycompany.yourithelpdesk.com and get through, or mail.mycampany.com.", 
            "The field we are going to pass to the Levenshtein algorithm is domain_detected, so let's add each subdomain to the multi-value field domain_detected.", 
            "This step is not required, but I like to filter down the list of fields mid-search just to make it easier for me to read and track it. URL Toolbox adds a *lot* of fields, but these four are the only fields I care about from now on.", 
            "Last piece of prep -- let's simplify everything exactly the two fields that URL Toolbox's Levenshtein algorithm is expecting.", 
            "Now the real magic: URL Toolbox is given two multi-value fields, and it does the cross checking to calculate the Levenshtein score for each combination. We pull out the lowest score from this group.", 
            "Now we filter for a Levenshtein score less than three (so two or fewer changes required to go from the domain to one of our standard domains). Those who have used Levenshtein are likely thinking: \"Wait, what about the > 0 that we always use?\" -- we accomplished that by filtering out standard domains way back at the start.", 
            "Finally we do some | fields and | rename so that everything looks nice and friendly for analysts to understand what we're looking at."
        ], 
        "label": "Emails With Lookalike Domains - Accelerated", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have URL Toolbox Installed (provides Levenshtein lookalike detection)", 
                "resolution": "The URL Toolbox app, written by Cedric Le Roux, not only provides effective URL Parsing but also Levenshtein similarity checking (e.g., typo detection) and Shannon entropy detection (random characters). Download <a href=\"https://splunkbase.splunk.com/app/2734/\">here</a>.", 
                "test": "| rest /services/apps/local | search disabled=0 label=\"URL Toolbox\" | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an Email data model", 
                "resolution": "This search requires an Email data. This is dependent on the <a href=\"https://splunkbase.splunk.com/app/1621/\">Common Information Model</a> being present, and also having your data mapped to CIM via appropriate TAs, usually with the out of the box field extractions from the <a href=\"https://splunkbase.splunk.com/app/1761/\">Cisco ESA TA</a>, the <a href=\"https://splunkbase.splunk.com/app/3225/\">Splunk Add-on for Microsoft Exchange</a>, etc.", 
                "test": "| tstats summaries_only=f allow_old_summaries=t count from datamodel=Email where earliest=-1h"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an accelerated Email data model", 
                "resolution": "This search requires an accelerated Email data. In order to run a fast accelerated search, you should accelerate your data model. (<a href=\"https://docs.splunk.com/Documentation/Splunk/latest/HadoopAnalytics/Configuredatamodelacceleration#Accelerate_the_data_model\">docs</a>)", 
                "test": "| tstats summaries_only=t allow_old_summaries=t count from datamodel=Email where earliest=-1h"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Sender Email Addresses (src_user) in your accelerated Email data model", 
                "resolution": "This search assumes that you have actual source email addresses -- check your field extractions for src_user and then rebuild your data models if not.", 
                "test": "| tstats summaries_only=t allow_old_summaries=t dc(All_Email.src_user) as count from datamodel=Email where earliest=-1h"
            }
        ], 
        "value": "| tstats summaries_only=t allow_old_summaries=t count from datamodel=Email by All_Email.src_user | rename All_Email.src_user as src_user \n| rex field=src_user \"\\@(?<domain_detected>.*)\" \n| stats sum(count) as count by domain_detected \n| eval domain_detected=mvfilter(domain_detected!=\"mycompany.com\" AND domain_detected!=\"company.com\" AND domain_detected!=\"mycompanylovestheenvironment.com\") \n| eval list=\"mozilla\" | `ut_parse_extended(domain_detected, list)` \n| foreach ut_subdomain_level* [eval orig_domain=domain_detected, domain_detected=mvappend(domain_detected, '<<FIELD>>' . \".\" . ut_tld)] \n| fields orig_domain domain_detected ut_domain count   \n| eval word1=mvappend(domain_detected, ut_domain), word2 = mvappend(\"mycompany.com\", \"company.com\", \"mycompanylovestheenvironment.com\") \n| lookup ut_levenshtein_lookup word1 word2 | eval ut_levenshtein= min(ut_levenshtein) \n| where ut_levenshtein < 3 \n| fields - domain_detected ut_domain | rename orig_domain as top_level_domain_in_incoming_email word1 as domain_names_analyzed word2 as company_domains_used count as num_occurrences ut_levenshtein as Levenshtein_Similarity_Score"
    }, 
    "Emails With Lookalike Domains - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 0, 
        "description": [
            "First we start by pulling our demo email logs, where we have a source address (this could also work for proxy logs!)", 
            "This is an intensive exercise, so let's start by aggregating per source address, so we don't end up running over the same email many times", 
            "Next we are going to extract the domain -- probably this should actually occur before the last stats, but the performance is similar and this way it matches the accelerated search where this step is required.", 
            "Now we aggregate per actual domain we will analyze, for performance reasons", 
            "Let's filter out any domains that our organization owns and expects to receive email from. You can have several domains here (I recommend no more than 10-20 -- eventually urltoolbox will get tired and stop doing adding Levenshtein fields, so you can look for null ut_levenshtein later if you are pushing this boundary).", 
            "Now we use the free URL Toolbox app to parse out subdomains from the top level domains. We want to analyze each one, so that an attacker can't send mycompany.yourithelpdesk.com and get through, or mail.mycampany.com.", 
            "The field we are going to pass to the Levenshtein algorithm is domain_detected, so let's add each subdomain to the multi-value field domain_detected.", 
            "This step is not required, but I like to filter down the list of fields mid-search just to make it easier for me to read and track it. URL Toolbox adds a *lot* of fields, but these four are the only fields I care about from now on.", 
            "Last piece of prep -- let's simplify everything exactly the two fields that URL Toolbox's Levenshtein algorithm is expecting.", 
            "Now the real magic: URL Toolbox is given two multi-value fields, and it does the cross checking to calculate the Levenshtein score for each combination. We pull out the lowest score from this group.", 
            "Now we filter for a Levenshtein score less than three (so two or fewer changes required to go from the domain to one of our standard domains). Those who have used Levenshtein are likely thinking: \"Wait, what about the > 0 that we always use?\" -- we accomplished that by filtering out standard domains way back at the start.", 
            "Finally we do some | fields and | rename so that everything looks nice and friendly for analysts to understand what we're looking at."
        ], 
        "label": "Emails With Lookalike Domains - Demo", 
        "prereqs": [
            {
                "field": "Anonymized_Email_Logs.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have URL Toolbox Installed (provides Levenshtein lookalike detection and domain parsing)", 
                "resolution": "The URL Toolbox app, written by Cedric Le Roux, not only provides effective URL Parsing but also Levenshtein similarity checking (e.g., typo detection) and Shannon entropy detection (random characters). Download <a href=\"https://splunkbase.splunk.com/app/2734/\">here</a>.", 
                "test": "| rest /services/apps/local | search disabled=0 label=\"URL Toolbox\" | stats count"
            }
        ], 
        "value": "| `Load_Sample_Log_Data(\"Email Logs\")`\n| stats count by Sender \n| rex field=Sender \"\\@(?<domain_detected>.*)\" \n| stats sum(count) as count by domain_detected \n| eval domain_detected=mvfilter(domain_detected!=\"mycompany.com\" AND domain_detected!=\"company.com\" AND domain_detected!=\"mycompanylovestheenvironment.com\") \n| eval list=\"mozilla\" | `ut_parse_extended(domain_detected, list)` \n| foreach ut_subdomain_level* [eval orig_domain=domain_detected, domain_detected=mvappend(domain_detected, '<<FIELD>>' . \".\" . ut_tld)] \n| fields orig_domain domain_detected ut_domain count   \n| eval word1=mvappend(domain_detected, ut_domain), word2 = mvappend(\"mycompany.com\", \"company.com\", \"mycompanylovestheenvironment.com\") \n| lookup ut_levenshtein_lookup word1 word2 | eval ut_levenshtein= min(ut_levenshtein) \n| where ut_levenshtein < 3 \n| fields - domain_detected ut_domain | rename orig_domain as top_level_domain_in_incoming_email word1 as domain_names_analyzed word2 as company_domains_used count as num_occurrences ut_levenshtein as Levenshtein_Similarity_Score"
    }, 
    "Emails With Lookalike Domains - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 0, 
        "description": [
            "First we start by pulling our email logs, where we have a source address (this could also work for proxy logs!)", 
            "This is an intensive exercise, so let's start by aggregating per source address, so we don't end up running over the same email many times", 
            "Next we are going to extract the domain -- probably this should actually occur before the last stats, but the performance is similar and this way it matches the accelerated search where this step is required.", 
            "Now we aggregate per actual domain we will analyze, for performance reasons", 
            "Let's filter out any domains that our organization owns and expects to receive email from. You can have several domains here (I recommend no more than 10-20 -- eventually urltoolbox will get tired and stop doing adding Levenshtein fields, so you can look for null ut_levenshtein later if you are pushing this boundary).", 
            "Now we use the free URL Toolbox app to parse out subdomains from the top level domains. We want to analyze each one, so that an attacker can't send mycompany.yourithelpdesk.com and get through, or mail.mycampany.com.", 
            "The field we are going to pass to the Levenshtein algorithm is domain_detected, so let's add each subdomain to the multi-value field domain_detected.", 
            "This step is not required, but I like to filter down the list of fields mid-search just to make it easier for me to read and track it. URL Toolbox adds a *lot* of fields, but these four are the only fields I care about from now on.", 
            "Last piece of prep -- let's simplify everything exactly the two fields that URL Toolbox's Levenshtein algorithm is expecting.", 
            "Now the real magic: URL Toolbox is given two multi-value fields, and it does the cross checking to calculate the Levenshtein score for each combination. We pull out the lowest score from this group.", 
            "Now we filter for a Levenshtein score less than three (so two or fewer changes required to go from the domain to one of our standard domains). Those who have used Levenshtein are likely thinking: \"Wait, what about the > 0 that we always use?\" -- we accomplished that by filtering out standard domains way back at the start.", 
            "Finally we do some | fields and | rename so that everything looks nice and friendly for analysts to understand what we're looking at."
        ], 
        "label": "Emails With Lookalike Domains - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Email Data", 
                "resolution": "This search requires Email data. The out of the box field extractions support the Common Information Model, including Cisco ESA/Ironport and Microsoft Exchange. If you don't have this data today, we highly recommend ingesting it with the <a href=\"https://splunkbase.splunk.com/app/1761/\">Cisco ESA TA</a> or the <a href=\"https://splunkbase.splunk.com/app/3225/\">Splunk Add-on for Microsoft Exchange</a>. For best performance, accelerate the email data model from the <a href=\"https://splunkbase.splunk.com/app/1621/\">Common Information Model</a>!", 
                "test": "| tstats count where index=* sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email earliest=-4h"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have URL Toolbox Installed (provides Levenshtein lookalike detection)", 
                "resolution": "The URL Toolbox app, written by Cedric Le Roux, not only provides effective URL Parsing but also Levenshtein similarity checking (e.g., typo detection) and Shannon entropy detection (random characters). Download <a href=\"https://splunkbase.splunk.com/app/2734/\">here</a>.", 
                "test": "| rest /services/apps/local | search disabled=0 label=\"URL Toolbox\" | stats count"
            }
        ], 
        "value": "index=* sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email src_user=*\n| stats count by src_user \n| rex field=src_user \"\\@(?<domain_detected>.*)\" \n| stats sum(count) as count by domain_detected \n| eval domain_detected=mvfilter(domain_detected!=\"mycompany.com\" AND domain_detected!=\"company.com\" AND domain_detected!=\"mycompanylovestheenvironment.com\") \n| eval list=\"mozilla\" | `ut_parse_extended(domain_detected, list)` \n| foreach ut_subdomain_level* [eval orig_domain=domain_detected, domain_detected=mvappend(domain_detected, '<<FIELD>>' . \".\" . ut_tld)] \n| fields orig_domain domain_detected ut_domain count   \n| eval word1=mvappend(domain_detected, ut_domain), word2 = mvappend(\"mycompany.com\", \"company.com\", \"mycompanylovestheenvironment.com\") \n| lookup ut_levenshtein_lookup word1 word2 | eval ut_levenshtein= min(ut_levenshtein) \n| where ut_levenshtein < 3 \n| fields - domain_detected ut_domain | rename orig_domain as top_level_domain_in_incoming_email word1 as domain_names_analyzed word2 as company_domains_used count as num_occurrences ut_levenshtein as Levenshtein_Similarity_Score"
    }, 
    "Endpoint Uncleaned Malware - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Computer_Name", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic demo dataset. In this case, we are using risk events from Symantec Endpoint Protection. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "To detect uncleaned Malware, we look for where the action taken is not the primary or secondary action expected.", 
            "Finally we put everything in a nice and usable table!"
        ], 
        "label": "Endpoint Uncleaned Malware - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Symantec Endpoint Protection Risks)` \n| where Actual_Action!=Requested_Action AND Actual_Action!=Secondary_Action  \n| table _time Action_Action Requested_Action Secondary_Action Risk_Name File_Path Computer_Name"
    }, 
    "Endpoint Uncleaned Malware - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Computer_Name", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic dataset. In this case, we are using risk events from Symantec Endpoint Protection.", 
            "To detect uncleaned Malware, we look for where the action taken is not the primary or secondary action expected.", 
            "Finally we put everything in a nice and usable table!"
        ], 
        "label": "Endpoint Uncleaned Malware - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Symantec AV data", 
                "resolution": "For simplicity, this search is written specifically for Symantec AV data, but it can be easily modified for other sources. ", 
                "test": "| metasearch earliest=-24h latest=now index=* sourcetype=symantec:ep:* | head 100 | stats count "
            }
        ], 
        "value": "index=* sourcetype=symantec:ep:*:file  \n| where Actual_Action!=Requested_Action AND Actual_Action!=Secondary_Action  \n| table _time Action_Action Requested_Action Secondary_Action Risk_Name File_Path Computer_Name"
    }, 
    "Expected Host GDPR - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic demo dataset, which is just a count of events per host (see the live search for detail). We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "We're filtering for hosts that have always sent us data, and are not currently sending us data.", 
            "Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR."
        ], 
        "label": "Expected Host GDPR - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Raw Event Counts by Host)` \n| where current=0 AND isnotnull(historical) AND historical>1000 \n| lookup gdpr_system_category host | search category=*"
    }, 
    "Expected Host GDPR - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our historical dataset, going back about 7 days.", 
            "Then we pull in the number of events seen per host from the last couple of hours.", 
            "Now we can use stats to pull those two tstats (with prestats=t) into the light, giving us the recent number, and the historical number per host.", 
            "We're filtering for hosts that have always sent us data, and are not currently sending us data.", 
            "Finally we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR."
        ], 
        "label": "Expected Host GDPR - Live", 
        "value": "| tstats prestats=t count(host) where latest=-2h earliest=-7d index=* groupby host \n| tstats prestats=t append=t count where earliest=-2h index=* groupby host  \n| stats count(host) as historical count as current by host \n| where current=0 AND isnotnull(historical) AND historical>1000 \n| lookup gdpr_system_category host | search category=*"
    }, 
    "FW Default Rules - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic demo dataset. In this case, DNS logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we filter for connections using the default rule.", 
            "Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR."
        ], 
        "label": "FW Default Rules - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(DNS Logs)` \n| search rule=\"*-default\" \n| lookup gdpr_system_category.csv host as src_ip | search category=*"
    }, 
    "FW Default Rules - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic dataset. In this case, firewall logs. We filter for connections using the default rule.", 
            "Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR."
        ], 
        "label": "FW Default Rules - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Palo Alto Networks Firewall data", 
                "resolution": "This search requires Firewall data. For simplicity, we've hard-coded the pattern of the PAN default rule names, but you could apply the same logic to other firewalls by adding your internal default rule naming scheme.", 
                "test": "index=pan_logs sourcetype=pan*traffic earliest=-2h| head 100 | stats count "
            }
        ], 
        "value": "index=pan_logs sourcetype=pan*traffic rule=\"*-default\" action=allowed \n| lookup gdpr_system_category.csv host as src_ip | search category=*"
    }, 
    "Fake Windows Processes - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our basic demo data", 
            "Next we are going to look for any process launches (Sysmon EventCode 1) that are being launched from the standard Windows x86 or x64 system directories.", 
            "In order for us to see if those filenames are typically associated with Windows processes, we need to get the filename alone. Here, the rex command allows us to do a relatively simple regex to extract that filename (though Splunk has other mechanisms).", 
            "Now that we have our filenames, we want to look them up. Splunk has a capability of looking up data in a CSV file through the lookup command. This will take the filename we just extracted, \"look it up\" in the csv file, and then add any new fields.", 
            "The field from the lookup is \"systemFile\" so we can now search for any true systemFile field.", 
            "And finally we can pull out all the filenames and put them into a usable format via the table command."
        ], 
        "label": "Fake Windows Processes - Demo", 
        "prereqs": [
            {
                "field": "UC_fake_win_process.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": " | inputlookup UC_fake_win_process \n| search EventCode=1 Image!=*Windows\\\\System32* Image!=*Windows\\\\SysWOW64* \n| rex field=Image .*\\\\\\(?<filename>\\S+)\\s?$ \n| lookup isWindowsSystemFile_lookup filename \n| search systemFile=true \n| table _time dest host user Image"
    }, 
    "Fake Windows Processes - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our Windows Process Launch logs (Event ID 4688), or any Sysmon process launch logs (EventCode 1). We are going to look for any process launches that are being launched from the standard Windows x86 or x64 system directories.", 
            "Because we have two different potential data with their own filenames, we are using eval to put them together into one. (Splunk's Common Information Model makes this much easier and more automatic.)", 
            "In order for us to see if those filenames are typically associated with Windows processes, we need to get the filename alone. Here, the rex command allows us to do a relatively simple regex to extract that filename (though Splunk has other mechanisms).", 
            "Now that we have our filenames, we want to look them up. Splunk has a capability of looking up data in a CSV file through the lookup command. This will take the filename we just extracted, \"look it up\" in the csv file, and then add any new fields.", 
            "The field from the lookup is \"systemFile\" so we can now search for any true systemFile field.", 
            "And finally we can pull out all the filenames and put them into a usable format via the table command."
        ], 
        "label": "Fake Windows Processes - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security Logs or Sysmon Installed", 
                "resolution": "Begin ingesting Windows Security Logs or install Sysmon", 
                "test": "| metasearch earliest=-2h latest=now (source=\"*win*security\" OR sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\") index=* | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Process Launch Logs (Event ID 4688) Or Sysmon process start events (EventCode=1)", 
                "resolution": "Begin ingesting Windows Security Logs or install Sysmon", 
                "test": "| metasearch earliest=-2h latest=now (source=\"*win*security\" OR sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\") (EventCode=4688 OR EventCode=1) index=* | stats count"
            }
        ], 
        "value": "index=* (source=*win*security EventCode=4688 New_Process_Name!=*Windows\\\\System32* New_Process_Name!=*Windows\\\\SysWOW64*) OR (sourcetype=XmlWinEventLog:Microsoft-Windows-Sysmon/Operational EventCode=1 Image!=*Windows\\\\System32* Image!=*Windows\\\\SysWOW64*) \n| eval process=coalesce(Image, New_Process_Name)  \n| rex field=process .*\\\\\\(?<filename>\\S+)\\s?$ \n| lookup isWindowsSystemFile_lookup filename \n| search systemFile=true \n| table _time dest host user process"
    }, 
    "Find Processes with Renamed Executables - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "sha1", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "other", 
        "description": [
            "First we pull in our demo dataset. This could be any EDR data source that provides file hash information.", 
            "Earlier versions of Sysmon didn't extract a filename by default, so we are adding that in here. We also make it lowercase so that Windows' lack of case sensitivity doesn't mess with our analysis.", 
            "Next we use stats to calculate how many different filenames that hash ran as, including the filenames and the full cli strings for contextual data.", 
            "Finally, we filter for where the same hash ran as at least two filenames, indicating a rename."
        ], 
        "label": "Find Processes with Renamed Executables - Demo", 
        "prereqs": [
            {
                "field": "generic_sysmon_service_launch_logs.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(\"Generic Sysmon Process Launches\")` \n| rex field=Image \"[\\\\\\/](?<filename>[^\\\\\\/]*)$\" | eval filename=lower(filename) \n| stats dc(filename) as NumFilenames values(filename) as Filenames values(Image) as Images by sha1 \n| where NumFilenames>1"
    }, 
    "Find Processes with Renamed Executables - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "sha1", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "other", 
        "description": [
            "First we pull in our basic dataset, which consists of XML format Sysmon logs from the endpoints (ingested via the Sysmon TA). This could be any EDR data source that provides file hash information. Because we're looking for process launches, we then filter for EventCode=1 (the Sysmon Process Launch code).", 
            "Earlier versions of Sysmon didn't extract a filename by default, so we are adding that in here. We also make it lowercase so that Windows' lack of case sensitivity doesn't mess with our analysis.", 
            "Next we use stats to calculate how many different filenames that hash ran as, including the filenames and the full cli strings for contextual data.", 
            "Finally, we filter for where the same hash ran as at least two filenames, indicating a rename."
        ], 
        "label": "Find Processes with Renamed Executables - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Microsoft Sysmon logs", 
                "resolution": "Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=\"https://splunkbase.splunk.com/app/1914/\">Splunk App</a>. Check out the <a href=\"http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf\">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!", 
                "test": "| metasearch index=* sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\"  earliest=-1h latest=now  | stats count "
            }
        ], 
        "value": "index=* sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\" EventCode=1 \n| rex field=Image \"[\\\\\\/](?<filename>[^\\\\\\/]*)$\" | eval filename=lower(filename)\n| stats dc(filename) as NumFilenames values(filename) as Filenames values(Image) as Images by sha1 \n| where NumFilenames>1"
    }, 
    "Find Unusually Long CLI Commands - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our demo dataset. This could be any EDR data source that provides the full CLI string.", 
            "Next we use eval to calculate how long the command line (file path + command line args) is.", 
            "Eventstats is like stats, but just adds the results to your existing dataset. So this will give us the avg and stdev per each host.", 
            "Finally we use stats to roll up multiple process launches, giving us the length of that cli string alongside the avg and stdev of the host, per host, per cli string.", 
            "With that setup, we can find processes that have substantially longer cli strings (4 * the standard deviation) than the average on this system."
        ], 
        "label": "Find Unusually Long CLI Commands - Demo", 
        "prereqs": [
            {
                "field": "STE_Sysmon_commandline.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(\"Sysmon Data\")` \n|  eval cmdlen=len(CommandLine) \n| eventstats stdev(cmdlen) as stdev,avg(cmdlen) as avg by host\n| stats max(cmdlen) as maxlen, values(stdev) as stdevperhost, values(avg) as avgperhost by host,CommandLine \n| where maxlen>4*stdevperhost+avgperhost"
    }, 
    "Find Unusually Long CLI Commands - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our basic dataset, which consists of XML format Sysmon logs from the endpoints (ingested via the Sysmon TA). This could be any EDR data source that provides the full CLI string. Because we're looking for process launches, we then filter for EventCode=1 (the Sysmon Process Launch code).", 
            "Eventstats is like stats, but just adds the results to your existing dataset. So this will give us the avg and stdev of the command line length per host.", 
            "Finally we use stats to roll up multiple process launches, giving us the length of that cli string alongside the avg and stdev of the host, per host, per cli string.", 
            "With that setup, we can find processes that have substantially longer cli strings (4 * the standard deviation) than the average on this system."
        ], 
        "label": "Find Unusually Long CLI Commands - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Microsoft Sysmon logs", 
                "resolution": "Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=\"https://splunkbase.splunk.com/app/1914/\">Splunk App</a>. Check out the <a href=\"http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf\">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!", 
                "test": "| metasearch index=* sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\"  earliest=-1h latest=now  | stats count "
            }
        ], 
        "value": "index=* sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\" EventCode=1  \n|eval cmdlen=len(CommandLine) | eventstats stdev(cmdlen) as stdev,avg(cmdlen) as avg by host\n| stats max(cmdlen) as maxlen, values(stdev) as stdevperhost, values(avg) as avgperhost by host,CommandLine \n| where maxlen>4*stdevperhost+avgperhost"
    }, 
    "Flight Risk Emails - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Sender", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we start by pulling our demo email logs, and filter for outbound emails.", 
            "In order to understand if a user's name is in the filename, we need to know what a user's name is. Lets look up this data in LDAP output from the ldapsearch app, detailed in the \"Pull List of Privileged Users\" example.", 
            "Now we use the first name and last name to look up different incarnations in the attachment, storing the result (1 or 0) in a field called hasNameInAttachment. (Why not filter directly? We want to be able to make a big search in the next line, and while you could do that with | where all in one.. most people would prefer not to.) We're looking for first name, or last name, or first initial + last name, so we would get resume_jane.pdf and smith_resume.pdf and jsmith.docx.", 
            "Now we actually do the search. We look to see if the filename is in the attachment, we look to see if the recipient looks like a third party careers address, and we look to see if there is a resume attached.", 
            "The bucket command flattens the timestamps so we can easily see how many days we saw this behavior on.", 
            "Almost done. Now we do some stats to see if it's a one-off, or if this is happening over the course of multiple days. We also list the recipient addresses and emails, for convenience.", 
            "Finally, we filter for where the activity occurs over multiple days, or there are many destinations or instances of the attachment."
        ], 
        "label": "Flight Risk Emails - Demo", 
        "prereqs": [
            {
                "field": "Anonymized_Email_Logs.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(\"Email Logs\")` | search Sender=*@mycompany.com Recipient!=*@mycompany.com \n| lookup UC_raw_data_for_privilege_calculations mail as Sender OUTPUT firstname lastname \n| eval hasNameInAttachment=case(like(lower(file_name), \"%\" . lower(firstname) . \"%\"), 1, like(lower(file_name), \"%\" . lower(lastname) . \"%\"), 1,like(lower(file_name), \"%\" . lower(substr(firstname, 1, 1) . lastname) . \"%\"), 1, 1=1, 0) \n| search hasNameInAttachment=1 OR file_name=*resume* OR (Recipient=careers@* OR Recipient=recruiting@* OR Recipient=jobs*)\n| bucket _time span=1d \n| stats list(Recipient) as Recipients list(file_name) as file_names sum(hasNameInAttachment) dc(_time) as num_days by Sender \n| where num_days>1 OR mvcount(Recipients) > 3 OR mvcount(file_names)>3"
    }, 
    "Flight Risk Emails - Live": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we start by pulling our email logs, and filter for outbound emails.", 
            "In order to understand if a user's name is in the filename, we need to know what a user's name is. Lets look up this data in LDAP output from the ldapsearch app, detailed in the \"Pull List of Privileged Users\" example.", 
            "Now we use the first name and last name to look up different incarnations in the attachment, storing the result (1 or 0) in a field called hasNameInAttachment. (Why not filter directly? We want to be able to make a big search in the next line, and while you could do that with | where all in one.. most people would prefer not to.) We're looking for first name, or last name, or first initial + last name, so we would get resume_jane.pdf and smith_resume.pdf and jsmith.docx.", 
            "Now we actually do the search. We look to see if the filename is in the attachment, we look to see if the recipient looks like a third party careers address, and we look to see if there is a resume attached.", 
            "The bucket command flattens the timestamps so we can easily see how many days we saw this behavior on.", 
            "Almost done. Now we do some stats to see if it's a one-off, or if this is happening over the course of multiple days. We also list the recipient addresses and emails, for convenience.", 
            "Finally, we filter for where the activity occurs over multiple days, or there are many destinations or instances of the attachment."
        ], 
        "label": "Flight Risk Emails - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Email Data", 
                "resolution": "This search requires Email data. The out of the box field extractions support the Common Information Model, including Cisco ESA/Ironport and Microsoft Exchange. If you don't have this data today, we highly recommend ingesting it with the <a href=\"https://splunkbase.splunk.com/app/1761/\">Cisco ESA TA</a> or the <a href=\"https://splunkbase.splunk.com/app/3225/\">Splunk Add-on for Microsoft Exchange</a>. For best performance, accelerate the email data model from the <a href=\"https://splunkbase.splunk.com/app/1621/\">Common Information Model</a>!", 
                "test": "index=* sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email earliest=-4h | head 100"
            }, 
            {
                "field": "LDAPSearch.csv", 
                "greaterorequalto": 1, 
                "name": "Must have LDAPSearch lookup", 
                "resolution": "Verify that a you have an LDAPSearch.csv lookup defined, with the output of | ldapsearch (per the Pulling VIP Users example implementation docs).", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "index=* sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email src_user=*@mycompany.com dest_user!=*@mycompany.com \n| lookup LDAPSearch.csv mail as src_user OUTPUT firstname lastname \n| eval hasNameInAttachment=case(like(lower(file_name), \"%\" . lower(firstname) . \"%\"), 1, like(lower(file_name), \"%\" . lower(lastname) . \"%\"), 1,like(lower(file_name), \"%\" . lower(substr(firstname, 1, 1) . lastname) . \"%\"), 1, 1=1, 0) \n| search hasNameInAttachment=1 OR file_name=*resume* OR (dest_user=careers@* OR dest_user=recruiting@* OR dest_user=jobs*)\n| bucket _time span=1d \n| stats list(dest_user) as recipients list(file_name) as file_names sum(hasNameInAttachment) dc(_time) as num_days by src_user \n| where num_days>1 OR mvcount(recipients) > 3 OR mvcount(file_names)>3"
    }, 
    "Flight Risk Printing - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_createUBA": 1, 
        "actions_riskObject": "User", 
        "actions_riskObjectType": "user", 
        "description": [
            "First we pull in our demo dataset.", 
            "We filter for print jobs that look suspicious based on the filename. Here we're looking for the terms resume, interview, or offer letter.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Next, we count how many days we've seen suspicious files printed, and list those files out for convenience", 
            "Finally, we filter for users where we've seen printing on multiple days."
        ], 
        "label": "Flight Risk Printing - Demo", 
        "prereqs": [
            {
                "field": "uniflow_printer_log_sample.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(\"Printer Logs\")`\n| search File_Printed=*resume* OR File_Printed=*interview* OR File_Printed=*offer*letter* \n| bucket _time span=1d \n| stats dc(_time) as num_days list(File_Printed) by User \n| where num_days>1"
    }, 
    "Flight Risk Printing - Live": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "User", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we pull in our printer dataset. We filter for print jobs that look suspicious based on the filename. Here we're looking for the terms resume, interview, or offer letter.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.", 
            "Next, we count how many days we've seen suspicious files printed, and list those files out for convenience", 
            "Finally, we filter for users where we've seen printing on multiple days."
        ], 
        "label": "Flight Risk Printing - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Printer data", 
                "resolution": "This search requires Printer data. By default we are looking for either Uniflow logs (used from our demo data sample) or Windows Print Server logs. If you don't have this data right now, consider <a href=\"https://blogs.technet.microsoft.com/askperf/2008/08/12/two-minute-drill-enabling-print-queue-logging/\">ingesting it</a>! If you have other printer logs, go ahead and substitute the sourcetype below.", 
                "test": "earliest=-6h latest=now sourcetype=uniflow OR (source=win*system EventCode=307) index=* | head 100 | stats count dc(File_Printed) as files dc(User) as users"
            }, 
            {
                "field": "pages", 
                "greaterorequalto": 1, 
                "name": "Must have a field called Page_Count defined", 
                "resolution": "You should have a field called \"Page_Count\" defined in your printer logs. If that's not currently extracted, build an extraction for it (or do an inline rex in the SPL below to work around this). Or just choose a different field below.", 
                "test": "earliest=-6h latest=now sourcetype=uniflow OR (source=win*system EventCode=307) index=* | head 100 | stats count dc(File_Printed) as files dc(User) as users"
            }, 
            {
                "field": "users", 
                "greaterorequalto": 1, 
                "name": "Must have the user field defined", 
                "resolution": "You should have a field called \"User\" defined in your printer logs. If that's not currently extracted, build an extraction for it (or do an inline rex in the SPL below to work around this). Or just choose a different field below.", 
                "test": "earliest=-6h latest=now sourcetype=uniflow OR (source=win*system EventCode=307) index=* | head 100 | stats count dc(File_Printed) as files dc(User) as users"
            }
        ], 
        "value": "index=* sourcetype=uniflow OR (source=win*system EventCode=307) (File_Printed=*resume* OR File_Printed=*interview* OR File_Printed=*offer*letter*) \n| bucket _time span=1d \n| stats dc(_time) as num_days list(File_Printed) by User \n| where num_days>1"
    }, 
    "Flight Risk Web Browsing - Demo": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our demo dataset of anonymized proxy events.", 
            "Next, we filter to look at just job-search data, which is the PAN categorization for used for job search sites.", 
            "Next, we look at the number of days in which job hunting was occurred.", 
            "Finally, we filter for users where we've seen printing on multiple days."
        ], 
        "label": "Flight Risk Web Browsing - Demo", 
        "prereqs": [
            {
                "field": "od_splunklive_fw_data.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Sample Firewall Data)` \n| search category=job-search \n| bucket _time span=1d | stats dc(_time) as num_days count as num_connections by user \n| where num_days>1"
    }, 
    "Flight Risk Web Browsing - Live": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our dataset of proxy events, filtered for just proxy activity.", 
            "Next, we look at the number of days in which job hunting was occurred.", 
            "Finally, we filter for users where we've seen printing on multiple days."
        ], 
        "label": "Flight Risk Web Browsing - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Proxy data", 
                "resolution": "This search requires NGFW or Web Proxy data to run. By default, we're checking for Common Information Model compliant data, and then also manually specifying the standard sourcetypes for Check Point, Palo Alto Networks, and Blue Coat. You should specify your particular index and sourcetype in the actual search to improve performance (or better yet, accelerate with the common information model!)", 
                "test": "(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa) category=*| head 100 | stats count "
            }
        ], 
        "value": "index=pan_logs category=job-search \n| bucket _time span=1d | stats dc(_time) as num_days values(app) values(category) count as num_connections by user \n| where num_days>1"
    }, 
    "Hosts Not in CMDB - Accelerated": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "All_Traffic.src_ip", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we look in the network traffic data model for traffic originating from RFC1918 space.", 
            "Then we put take the src_ip field and look it up in our asset information, to see what hosts are in the asset data. This is a CIDR lookup, and if it ever matches the field \"key\" will be output.", 
            "Finally, we look for fields where there is no key field output."
        ], 
        "label": "Hosts Not in CMDB - Accelerated", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have data in Network Traffic data model", 
                "resolution": "This search requires Firewall or Netflow data to run. We are searching here for the common information model network traffic data model.", 
                "test": "| tstats count from datamodel=Network_Traffic where earliest=-1h "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an accelerated Network Traffic data model", 
                "resolution": "In addition to searching for the common information model network traffic data model, we are telling Splunk to only visit accelerated data models.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Network_Traffic where earliest=-1h "
            }, 
            {
                "field": "assets_by_cidr.csv", 
                "greaterorequalto": 1, 
                "name": "Must have an Asset Lookup", 
                "resolution": "Here we're using the lookup that is built out for Enterprise Security, but you could use any CMDB lookup with a CIRMatch column for the ip.", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Network_Traffic where All_Traffic.src_ip=10.0.0.0/8 OR All_Traffic.src_ip=192.168.0.0/16 OR All_Traffic.src_ip=172.16.0.0./12 by All_Traffic.src_ip \n| lookup asset_lookup_by_str ip as All_Traffic.src_ip OUTPUT key \n| where isnull(key)"
    }, 
    "Hosts Not in CMDB - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our basic demo data, and aggregate per source IP", 
            "Next we filter for source_ips that are coming from RFC1918 IP space.", 
            "Then we put take the src_ip field and look it up in our asset information, to see what hosts are in the asset data. This is a CIDR lookup, and if it ever matches the field \"key\" will be output.", 
            "Finally, we look for fields where there is no key field output."
        ], 
        "label": "Hosts Not in CMDB - Demo", 
        "prereqs": [
            {
                "field": "od_splunklive_fw_data.csv", 
                "greaterorequalto": 1, 
                "name": "Must have an Asset Lookup", 
                "resolution": "Here we're using the lookup that is built out for Enterprise Security, but you could use any CMDB lookup with a CIRMatch column for the ip.", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }, 
            {
                "field": "sse_sample_asset_list.csv", 
                "greaterorequalto": 1, 
                "name": "Must have an Asset Lookup", 
                "resolution": "Here we're using the lookup that is built out for Enterprise Security, but you could use any CMDB lookup with a CIRMatch column for the ip.", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Sample Firewall Data)` | stats count by src_ip \n| search src_ip=10.0.0.0/8 OR src_ip=192.168.0.0/16 OR src_ip=172.16.0.0/12 \n| lookup sse_sample_asset_list ip as src_ip \n| where isnull(key)"
    }, 
    "Hosts That Stop Reporting Sourcetypes - Live": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we start by pulling a count of events overall by host. (Note that when using back-to-back tstats, your field names need to be different, but you can't us the familiar 'count as count1' syntax, so here we use 'count(host)' to distinguish from the 'count' in the next line.", 
            "Now we pull a count of events for our in scope security source(s) by host, here Windows Security logs.", 
            "Next we use stats to combine our two prestats=t tstats commands into one usable stats. Whenever you use tstats prestats=t, you need a stats, chart, or similiar to pull the hidden prestats fields into the light so that you can use them.", 
            "From lines 1-3 we have a count of events per source (all vs security) per host per day, now we can calculate a percentage of logs that were WinSecurity on each of those days (for each of those hosts).", 
            "Technically this line isn't really required because we should be able to use now() in the next line, but I typically use it for uniformity with the demo datasets and in case you have some weird timezone hijinks in your environment.", 
            "Now we start the tricky piece. How do we figure out the business logic for when we want to be alerted to a security log going silent? At this stage, we're collecting a number of data points that we could use. For example, past_instances_of_no_logs will tell us how many days in the past we had no logs from this host -- if this is non-zero, then no logs today is much more likely to be benign. Similarly, we can use avg and stdev to calculate how wide a distribution there typically is -- if sometimes you have tons of security events, sometimes you have none, then this is could be just chance.", 
            "Finally we apply our filtering. In testing, the most reliable metric seemed to be looking for hosts where we do have a baseline (at least ten days, so we know something about this host), we have some historical data (isnotnull(avg)), and we've never seen a day with zero events before."
        ], 
        "label": "Hosts That Stop Reporting Sourcetypes - Live", 
        "value": "| tstats prestats=t count(host) where index=* groupby host _time span=1d \n| tstats prestats=t append=t count where index=* source=\"winEventLog:Security\" by host  _time span=1d \n| stats count(host) as all_logs count as win_logs by host _time \n| eval win_perc=round(100*(win_logs / all_logs), 2) \n| eventstats max(_time) as maxtime \n| stats count as num_data_samples avg(eval(if(_time<relative_time(maxtime, \"-1d@d\"), win_perc, null))) as avg sum(eval(if(_time<relative_time(maxtime, \"-1d@d\") AND win_perc=0, 1, null))) as past_instances_of_no_logs max(eval(if(_time>=relative_time(maxtime, \"-1d@d\"), win_perc, null))) as latest by host \n| where isnotnull(avg) AND num_data_samples>10 AND isnull(past_instances_of_no_logs) AND latest=0"
    }, 
    "Huge Volume of DNS Requests - Accelerated": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our accelerated dataset, which comes from Firewall Logs and targets scenarios where we will have a large number of DNS connections with a small amount of volume each. tstats here is giving us the number of bytes sent per source_ip, filtered for dest_port 53", 
            "Eventstats then allows us to calculate all manner of statistics. This is one of the more complicated stats syntaxes that you will see, but it's actually not that complicated. The big component here is leveraging stats+eval, where we can embed the flexible logic of eval inside of stats. In this case, when we are calculating our average and standard deviation, we really want to exclude the most recent values (that which we're concerned about), so that they don't sway our average (imagine you churn along at 1 kb per hour, then in the last hour it's 150 MB.. you really want your normal baseline to be 1 KB). One other note here -- we are doing two different eventstats, one on a global basis, one on a per host basis. That's so we can try to identify a host that's always at the top of the charts, while overall looking across the org, giving us a good balance across servers with static IPs and DHCP hosts that move around.", 
            "Here's where we really start doing the important work. Our lengthy eventstats gave us all these fields that we can filter on and interpret based on. (When testing this out, feel free to remove this line and those that follow, so you can see the raw fields coming out of eventstats.) Now we need to filter for hosts that are substantially above the norm.", 
            "From our last line, we have focused in to just hosts that are behaving abnormally. Here, we are using eval to add another field to the results -- not one focused on detection logic, but to try to add context and summarize some of the maths for an analyst to see why we are surfacing this host.", 
            "Finally, we clear up some of the nonsense fields we don't care that much about, again to make things clearer for the analyst."
        ], 
        "label": "Huge Volume of DNS Requests - Accelerated", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have data in your Network Traffic data model", 
                "resolution": "This search requires Firewall or Netflow data to run. We are searching here for the common information model network traffic data model.", 
                "test": "| tstats count from datamodel=Network_Traffic where earliest=-1h "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an accelerated Network Traffic data model", 
                "resolution": "In addition to searching for the common information model network traffic data model, we are telling Splunk to only visit accelerated data models.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Network_Traffic where earliest=-1h "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Network Traffic data model must have a src_ip", 
                "resolution": "In addition to searching for the accelerated common information model network traffic data model, we are telling Splunk to verify that there is a src_ip in this data set.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t dc(All_Traffic.src_ip) as count from datamodel=Network_Traffic where earliest=-1h "
            }
        ], 
        "value": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Network_Traffic where All_Traffic.dest_port=53 by All_Traffic.src_ip _time span=1d \n| eventstats max(_time) as maxtime avg(count) as avg_count stdev(count) as stdev_count | eventstats count as num_data_samples avg(eval(if(_time < relative_time(maxtime, \"@h\"),count,null))) as per_source_avg_count stdev(eval(if(_time < relative_time(maxtime, \"@h\"),count,null))) as per_source_stdev_count by src_ip  \n| where num_data_samples >=4 AND count > avg_count + 3 * stdev_count AND count > per_source_avg_count + 3 * per_source_stdev_count AND _time >= relative_time(maxtime, \"@h\") \n| eval num_standard_deviations_away_from_org_average = round(abs(count - avg_count) / stdev_count,2), num_standard_deviations_away_from_per_source_average = round(abs(count - per_source_avg_count) / per_source_stdev_count,2) \n| fields - maxtime per_source* avg* stdev*"
    }, 
    "Huge Volume of DNS Requests - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our demo dataset, which comes from Firewall Logs and targets scenarios where we will have a large number of DNS connections with a small amount of volume each.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the last hour.", 
            "Now we are looking at the number of bytes sent per source ip per hour, over our time range (usually the last day).", 
            "Eventstats then allows us to calculate all manner of statistics. This is one of the more complicated stats syntaxes that you will see, but it's actually not that complicated. The big component here is leveraging stats+eval, where we can embed the flexible logic of eval inside of stats. In this case, when we are calculating our average and standard deviation, we really want to exclude the most recent values (that which we're concerned about), so that they don't sway our average (imagine you churn along at 1 kb per hour, then in the last hour it's 150 MB.. you really want your normal baseline to be 1 KB). One other note here -- we are doing two different eventstats, one on a global basis, one on a per host basis. That's so we can try to identify a host that's always at the top of the charts, while overall looking across the org, giving us a good balance across servers with static IPs and DHCP hosts that move around.", 
            "Here's where we really start doing the important work. Our lengthy eventstats gave us all these fields that we can filter on and interpret based on. (When testing this out, feel free to remove this line and those that follow, so you can see the raw fields coming out of eventstats.) Now we need to filter for hosts that are substantially above the norm.", 
            "From our last line, we have focused in to just hosts that are behaving abnormally. Here, we are using eval to add another field to the results -- not one focused on detection logic, but to try to add context and summarize some of the maths for an analyst to see why we are surfacing this host.", 
            "Finally, we clear up some of the nonsense fields we don't care that much about, again to make things clearer for the analyst."
        ], 
        "label": "Huge Volume of DNS Requests - Demo", 
        "prereqs": [
            {
                "field": "dns_data_anon.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "|  `Load_Sample_Log_Data(\"DNS Logs\")`\n| bucket _time span=1h \n| stats count by src_ip _time \n| eventstats max(_time) as maxtime avg(count) as avg_count stdev(count) as stdev_count | eventstats count as num_data_samples avg(eval(if(_time < relative_time(maxtime, \"@h\"),count,null))) as per_source_avg_count stdev(eval(if(_time < relative_time(maxtime, \"@h\"),count,null))) as per_source_stdev_count by src_ip  \n| where num_data_samples >=4 AND count > avg_count + 3 * stdev_count AND count > per_source_avg_count + 3 * per_source_stdev_count AND _time >= relative_time(maxtime, \"@h\") \n| eval num_standard_deviations_away_from_org_average = round(abs(count - avg_count) / stdev_count,2), num_standard_deviations_away_from_per_source_average = round(abs(count - per_source_avg_count) / per_source_stdev_count,2) \n| fields - maxtime per_source* avg* stdev*"
    }, 
    "Huge Volume of DNS Requests - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our basic dataset, which comes from Firewall Logs and targets scenarios where we will have a large number of DNS connections with a small amount of volume each.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the last hour.", 
            "Now we are looking at the number of bytes sent per source ip per hour, over our time range (usually the last day).", 
            "Eventstats then allows us to calculate all manner of statistics. This is one of the more complicated stats syntaxes that you will see, but it's actually not that complicated. The big component here is leveraging stats+eval, where we can embed the flexible logic of eval inside of stats. In this case, when we are calculating our average and standard deviation, we really want to exclude the most recent values (that which we're concerned about), so that they don't sway our average (imagine you churn along at 1 kb per hour, then in the last hour it's 150 MB.. you really want your normal baseline to be 1 KB). One other note here -- we are doing two different eventstats, one on a global basis, one on a per host basis. That's so we can try to identify a host that's always at the top of the charts, while overall looking across the org, giving us a good balance across servers with static IPs and DHCP hosts that move around.", 
            "Here's where we really start doing the important work. Our lengthy eventstats gave us all these fields that we can filter on and interpret based on. (When testing this out, feel free to remove this line and those that follow, so you can see the raw fields coming out of eventstats.) Now we need to filter for hosts that are substantially above the norm.", 
            "From our last line, we have focused in to just hosts that are behaving abnormally. Here, we are using eval to add another field to the results -- not one focused on detection logic, but to try to add context and summarize some of the maths for an analyst to see why we are surfacing this host.", 
            "Finally, we clear up some of the nonsense fields we don't care that much about, again to make things clearer for the analyst."
        ], 
        "label": "Huge Volume of DNS Requests - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Firewall data", 
                "resolution": "This search requires Firewall or Netflow data to run. By default, we're checking for Common Information Model compliant data, and then also manually specifying the standard sourcetypes for Check Point, Palo Alto Networks, and Cisco ASAs. You should specify your particular index and sourcetype in the actual search to improve performance (or better yet, accelerate with the common information model!)", 
                "test": "(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)| head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a src_ip and bytes_out field", 
                "resolution": "This search is also looking for firewall logs, but with the added filter of making sure that a src_ip defined and bytes_out>0.", 
                "test": "((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)) src_ip=* bytes_out>0| head 100 | stats count "
            }
        ], 
        "value": "((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa) ) dest_port=53 \n| bucket _time span=1h \n| stats count by src_ip _time \n| eventstats max(_time) as maxtime avg(count) as avg_count stdev(count) as stdev_count | eventstats count as num_data_samples avg(eval(if(_time < relative_time(maxtime, \"@h\"),count,null))) as per_source_avg_count stdev(eval(if(_time < relative_time(maxtime, \"@h\"),count,null))) as per_source_stdev_count by src_ip  \n| where num_data_samples >=4 AND count > avg_count + 3 * stdev_count AND count > per_source_avg_count + 3 * per_source_stdev_count AND _time >= relative_time(maxtime, \"@h\") \n| eval num_standard_deviations_away_from_org_average = round(abs(count - avg_count) / stdev_count,2), num_standard_deviations_away_from_per_source_average = round(abs(count - per_source_avg_count) / per_source_stdev_count,2) \n| fields - maxtime per_source* avg* stdev*"
    }, 
    "Huge Volume of DNS Traffic - Accelerated": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our accelerated dataset, which comes from Firewall Logs and targets scenarios where we will have a small number of DNS connections with a large amount of volume. tstats here is giving us the number of bytes sent per source_ip, filtered for dest_port 53", 
            "Eventstats then allows us to calculate all manner of statistics. This is one of the more complicated stats syntaxes that you will see, but it's actually not that complicated. The big component here is leveraging stats+eval, where we can embed the flexible logic of eval inside of stats. In this case, when we are calculating our average and standard deviation, we really want to exclude the most recent values (that which we're concerned about), so that they don't sway our average (imagine you churn along at 1 kb per hour, then in the last hour it's 150 MB.. you really want your normal baseline to be 1 KB). One other note here -- we are doing two different eventstats, one on a global basis, one on a per host basis. That's so we can try to identify a host that's always at the top of the charts, while overall looking across the org, giving us a good balance across servers with static IPs and DHCP hosts that move around.", 
            "Here's where we really start doing the important work. Our lengthy eventstats gave us all these fields that we can filter on and interpret based on. (When testing this out, feel free to remove this line and those that follow, so you can see the raw fields coming out of eventstats.) Now we need to filter for hosts that are substantially above the norm.", 
            "From our last line, we have focused in to just hosts that are behaving abnormally. Here, we are using eval to add another field to the results -- not one focused on detection logic, but to try to add context and summarize some of the maths for an analyst to see why we are surfacing this host.", 
            "Finally, we clear up some of the nonsense fields we don't care that much about, again to make things clearer for the analyst."
        ], 
        "label": "Huge Volume of DNS Traffic - Accelerated", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a Network Traffic data model", 
                "resolution": "This search requires Firewall or Netflow data to run. We are searching here for the common information model network traffic data model.", 
                "test": "| tstats count from datamodel=Network_Traffic where earliest=-1h "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an accelerated Network Traffic data model", 
                "resolution": "In addition to searching for the common information model network traffic data model, we are telling Splunk to only visit accelerated data models.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Network_Traffic where earliest=-1h "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Network Traffic data model must have a src_ip and bytes_out", 
                "resolution": "In addition to searching for the accelerated common information model network traffic data model, we are telling Splunk to verify that there is a src_ip, and that bytes_out is greater than zero bytes in this data set.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t sum(All_Traffic.bytes_out) as bytes dc(All_Traffic.src_ip) as src from datamodel=Network_Traffic where earliest=-1h | eval count = bytes * src"
            }
        ], 
        "value": "| tstats summariesonly=t allow_old_summaries=t sum(All_Traffic.bytes_out) as count from datamodel=Network_Traffic where All_Traffic.dest_port=53 by All_Traffic.src_ip _time span=1d \n| eventstats max(_time) as maxtime avg(bytes_out) as avg_bytes_out stdev(bytes_out) as stdev_bytes_out | eventstats count as num_data_samples avg(eval(if(_time < relative_time(maxtime, \"@h\"),bytes_out,null))) as per_source_avg_bytes_out stdev(eval(if(_time < relative_time(maxtime, \"@h\"),bytes_out,null))) as per_source_stdev_bytes_out by src_ip  \n| where num_data_samples >=4 AND bytes_out > avg_bytes_out + 3 * stdev_bytes_out AND bytes_out > per_source_avg_bytes_out + 3 * per_source_stdev_bytes_out AND _time >= relative_time(maxtime, \"@h\") \n| eval num_standard_deviations_away_from_org_average = round(abs(bytes_out - avg_bytes_out) / stdev_bytes_out,2), num_standard_deviations_away_from_per_source_average = round(abs(bytes_out - per_source_avg_bytes_out) / per_source_stdev_bytes_out,2) \n| fields - maxtime per_source* avg* stdev*"
    }, 
    "Huge Volume of DNS Traffic - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our demo dataset, which comes from Firewall Logs and targets scenarios where we will have a small number of DNS connections with a large amount of volume.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the last hour.", 
            "Now we are looking at the number of bytes sent per source ip per hour, over our time range (usually the last day).", 
            "Eventstats then allows us to calculate all manner of statistics. This is one of the more complicated stats syntaxes that you will see, but it's actually not that complicated. The big component here is leveraging stats+eval, where we can embed the flexible logic of eval inside of stats. In this case, when we are calculating our average and standard deviation, we really want to exclude the most recent values (that which we're concerned about), so that they don't sway our average (imagine you churn along at 1 kb per hour, then in the last hour it's 150 MB.. you really want your normal baseline to be 1 KB). One other note here -- we are doing two different eventstats, one on a global basis, one on a per host basis. That's so we can try to identify a host that's always at the top of the charts, while overall looking across the org, giving us a good balance across servers with static IPs and DHCP hosts that move around.", 
            "Here's where we really start doing the important work. Our lengthy eventstats gave us all these fields that we can filter on and interpret based on. (When testing this out, feel free to remove this line and those that follow, so you can see the raw fields coming out of eventstats.) Now we need to filter for hosts that are substantially above the norm.", 
            "From our last line, we have focused in to just hosts that are behaving abnormally. Here, we are using eval to add another field to the results -- not one focused on detection logic, but to try to add context and summarize some of the maths for an analyst to see why we are surfacing this host.", 
            "Finally, we clear up some of the nonsense fields we don't care that much about, again to make things clearer for the analyst."
        ], 
        "label": "Huge Volume of DNS Traffic - Demo", 
        "prereqs": [
            {
                "field": "dns_data_anon.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "|  `Load_Sample_Log_Data(\"DNS Logs\")`\n| bucket _time span=1h \n| stats sum(bytes*) as bytes* by src_ip _time \n| eventstats max(_time) as maxtime avg(bytes_out) as avg_bytes_out stdev(bytes_out) as stdev_bytes_out | eventstats count as num_data_samples avg(eval(if(_time < relative_time(maxtime, \"@h\"),bytes_out,null))) as per_source_avg_bytes_out stdev(eval(if(_time < relative_time(maxtime, \"@h\"),bytes_out,null))) as per_source_stdev_bytes_out by src_ip \n | where num_data_samples >=4 AND bytes_out > avg_bytes_out + 3 * stdev_bytes_out AND bytes_out > per_source_avg_bytes_out + 3 * per_source_stdev_bytes_out AND _time >= relative_time(maxtime, \"@h\") \n| eval num_standard_deviations_away_from_org_average = round(abs(bytes_out - avg_bytes_out) / stdev_bytes_out,2), num_standard_deviations_away_from_per_source_average = round(abs(bytes_out - per_source_avg_bytes_out) / per_source_stdev_bytes_out,2) \n| fields - maxtime per_source* avg* stdev*"
    }, 
    "Huge Volume of DNS Traffic - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our basic dataset, which comes from Firewall Logs and targets scenarios where we will have a small number of DNS connections with a large amount of volume.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the last hour.", 
            "Now we are looking at the number of bytes sent per source ip per hour, over our time range (usually the last day).", 
            "Eventstats then allows us to calculate all manner of statistics. This is one of the more complicated stats syntaxes that you will see, but it's actually not that complicated. The big component here is leveraging stats+eval, where we can embed the flexible logic of eval inside of stats. In this case, when we are calculating our average and standard deviation, we really want to exclude the most recent values (that which we're concerned about), so that they don't sway our average (imagine you churn along at 1 kb per hour, then in the last hour it's 150 MB.. you really want your normal baseline to be 1 KB). One other note here -- we are doing two different eventstats, one on a global basis, one on a per host basis. That's so we can try to identify a host that's always at the top of the charts, while overall looking across the org, giving us a good balance across servers with static IPs and DHCP hosts that move around.", 
            "Here's where we really start doing the important work. Our lengthy eventstats gave us all these fields that we can filter on and interpret based on. (When testing this out, feel free to remove this line and those that follow, so you can see the raw fields coming out of eventstats.) Now we need to filter for hosts that are substantially above the norm.", 
            "From our last line, we have focused in to just hosts that are behaving abnormally. Here, we are using eval to add another field to the results -- not one focused on detection logic, but to try to add context and summarize some of the maths for an analyst to see why we are surfacing this host.", 
            "Finally, we clear up some of the nonsense fields we don't care that much about, again to make things clearer for the analyst."
        ], 
        "label": "Huge Volume of DNS Traffic - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Firewall data", 
                "resolution": "This search requires Firewall or Netflow data to run. By default, we're checking for Common Information Model compliant data, and then also manually specifying the standard sourcetypes for Check Point, Palo Alto Networks, and Cisco ASAs. You should specify your particular index and sourcetype in the actual search to improve performance (or better yet, accelerate with the common information model!)", 
                "test": "(network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)| head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a src_ip and bytes_out field", 
                "resolution": "This search is also looking for firewall logs, but with the added filter of making sure that a src_ip defined and bytes_out>0.", 
                "test": "((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)) src_ip=* bytes_out>0| head 100 | stats count "
            }
        ], 
        "value": "(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)  \n| bucket _time span=1h \n| stats sum(bytes*) as bytes* by src_ip _time \n| eventstats max(_time) as maxtime avg(bytes_out) as avg_bytes_out stdev(bytes_out) as stdev_bytes_out | eventstats count as num_data_samples avg(eval(if(_time < relative_time(maxtime, \"@h\"),bytes_out,null))) as per_source_avg_bytes_out stdev(eval(if(_time < relative_time(maxtime, \"@h\"),bytes_out,null))) as per_source_stdev_bytes_out by src_ip  \n| where num_data_samples >=4 AND bytes_out > avg_bytes_out + 3 * stdev_bytes_out AND bytes_out > per_source_avg_bytes_out + 3 * per_source_stdev_bytes_out AND _time >= relative_time(maxtime, \"@h\") \n| eval num_standard_deviations_away_from_org_average = round(abs(bytes_out - avg_bytes_out) / stdev_bytes_out,2), num_standard_deviations_away_from_per_source_average = round(abs(bytes_out - per_source_avg_bytes_out) / per_source_stdev_bytes_out,2) \n| fields - maxtime per_source* avg* stdev*"
    }, 
    "Land Speed GDPR - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic demo dataset. In this case, AWS CloudTrail logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we pull the last src_ip for the same user using streamstats (sorted based on the user).", 
            "Next we look up the AccountId in the GDPR categorization lookup. Because we only care about GDPR Accounts for this example, we filter for only the hosts that are in scope for GDPR. (You could also categorize based on any other field in AWS.)", 
            "Here we filter for logins that are GDPR in scope, and in a short enough time range that it would be difficult to travel to distant parts of the globe.", 
            "Here we resolve the Last src_ip to a physical location, and stick that in a field so that we can conveniently use it.", 
            "Now we resolve the *current* src_ip", 
            "Now we calculate the distance using an approximation for the curvature of the earth. Easy, right? I do not understand it, I copy-pasted from https://answers.splunk.com/answers/317935/calculating-distances-between-points-with-geoip-us.html#answer-568451", 
            "Here we pull the date of the event, to make this easier to run over longer time windows.", 
            "Finally we use stats to collect all of the values into one line, per user, per day, and per set of locations. We're using some specific AWS data fields here -- if you're using a log source like VPN, then you might choose other fields."
        ], 
        "label": "Land Speed GDPR - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(AWS CloudTrail)`\n| sort 0 user, _time | streamstats window=1 current=f values(_time) as last_time values(src_ip) as last_src_ip by user \n| lookup gdpr_aws_category accountId \n| where isnotnull(category) AND last_src_ip != src_ip AND _time - last_time < 8*60*60 \n| iplocation last_src_ip | rename lat as last_lat lon as last_lon | eval location = City . \"|\" . Country . \"|\" . Region \n| iplocation src_ip \n| eval rlat1 = pi()*last_lat/180, rlat2=pi()*lat/180, rlat = pi()*(lat-last_lat)/180, rlon= pi()*(lon-last_lon)/180 | eval a = sin(rlat/2) * sin(rlat/2) + cos(rlat1) * cos(rlat2) * sin(rlon/2) * sin(rlon/2) | eval c = 2 * atan2(sqrt(a), sqrt(1-a)) | eval distance = 6371 * c, time_difference_hours = round((_time - last_time) / 3600,2), speed=round(distance/ ( time_difference_hours),2) | fields - rlat* a c \n| eval day=strftime(_time, \"%m/%d/%Y\")\n| stats values(accountId) values(awsRegion) values(eventName) values(distance) values(eval(mvappend(last_Country, Country))) as Country values(eval(mvappend(last_City, City))) as City values(eval(mvappend(last_Region, Region))) as Region  values(lat) values(lon)  values(userAgent) max(speed) as max_speed_kph min(time_difference_hours) as min_time_difference_hours by day user distance"
    }, 
    "Land Speed GDPR - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic dataset. In this case, AWS CloudTrail logs.", 
            "Next we pull the last src_ip for the same user using streamstats (sorted based on the user).", 
            "Next we look up the AccountId in the GDPR categorization lookup. Because we only care about GDPR Accounts for this example, we filter for only the hosts that are in scope for GDPR. (You could also categorize based on any other field in AWS.)", 
            "Here we filter for logins that are GDPR in scope, and in a short enough time range that it would be difficult to travel to distant parts of the globe.", 
            "Here we resolve the Last src_ip to a physical location, and stick that in a field so that we can conveniently use it.", 
            "Now we resolve the *current* src_ip", 
            "Now we calculate the distance using an approximation for the curvature of the earth. Easy, right? I do not understand it, I copy-pasted from https://answers.splunk.com/answers/317935/calculating-distances-between-points-with-geoip-us.html#answer-568451", 
            "Here we pull the date of the event, to make this easier to run over longer time windows.", 
            "Finally we use stats to collect all of the values into one line, per user, per day, and per set of locations. We're using some specific AWS data fields here -- if you're using a log source like VPN, then you might choose other fields."
        ], 
        "label": "Land Speed GDPR - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have AWS CloudTrail data (though could be applied to other data sources)", 
                "resolution": "In order to run this search, you must have AWS CloudTrail data onboard. Visit the <a href=\"/app/Splunk_Security_Essentials/data_source?technology=AWS%20CloudTrail\">data onboarding guide for AWS CloudTrail in this app</a>, or browse to <a href=\"https://splunkbase.splunk.com/app/1876/\">apps.splunk.com</a> for more information.", 
                "test": "| tstats count where earliest=-2h latest=now index=* sourcetype=aws:cloudtrail"
            }
        ], 
        "value": "index=* sourcetype=aws:cloudtrail user=* \n| sort 0 user, _time | streamstats window=1 current=f values(_time) as last_time values(src_ip) as last_src_ip by user \n| lookup gdpr_aws_category accountId \n| where isnotnull(category) AND last_src_ip != src_ip AND _time - last_time < 8*60*60 \n| iplocation last_src_ip | rename lat as last_lat lon as last_lon | eval location = City . \"|\" . Country . \"|\" . Region\n| iplocation src_ip \n| eval rlat1 = pi()*last_lat/180, rlat2=pi()*lat/180, rlat = pi()*(lat-last_lat)/180, rlon= pi()*(lon-last_lon)/180 | eval a = sin(rlat/2) * sin(rlat/2) + cos(rlat1) * cos(rlat2) * sin(rlon/2) * sin(rlon/2) | eval c = 2 * atan2(sqrt(a), sqrt(1-a)) | eval distance = 6371 * c, time_difference_hours = round((_time - last_time) / 3600,2), speed=round(distance/ ( time_difference_hours),2) | fields - rlat* a c \n| eval day=strftime(_time, \"%m/%d/%Y\")\n| stats values(accountId) values(awsRegion) values(eventName) values(distance) values(eval(mvappend(last_Country, Country))) as Country values(eval(mvappend(last_City, City))) as City values(eval(mvappend(last_Region, Region))) as Region  values(lat) values(lon)  values(userAgent) max(speed) as max_speed_kph min(time_difference_hours) as min_time_difference_hours by day user distance"
    }, 
    "Land Speed Privileged - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic demo dataset. In this case, AWS CloudTrail logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we pull the last src_ip for the same user using streamstats (sorted based on the user).", 
            "Next we look up the user in our Privileged User lookup. This will add a number of fields, including the risk_score field we'll use in the next command. ", 
            "Here we filter for privileged users (where the risk score is greater than 0), and for events in a short enough time range that it would be difficult to travel to distant parts of the globe.", 
            "Here we resolve the Last src_ip to a physical location, and stick that in a field so that we can conveniently use it.", 
            "Now we resolve the *current* src_ip", 
            "Now we calculate the distance using an approximation for the curvature of the earth. Easy, right? I do not understand it, I copy-pasted from https://answers.splunk.com/answers/317935/calculating-distances-between-points-with-geoip-us.html#answer-568451", 
            "Here we pull the date of the event, to make this easier to run over longer time windows.", 
            "Finally we use stats to collect all of the values into one line, per user, per day, and per set of locations. We're using some specific AWS data fields here -- if you're using a log source like VPN, then you might choose other fields."
        ], 
        "label": "Land Speed Privileged - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(AWS CloudTrail)`\n| sort 0 user, _time | streamstats window=1 current=f values(_time) as last_time values(src_ip) as last_src_ip by user \n| lookup PrivilegedRiskScores user \n| where risk_score>0 AND last_src_ip != src_ip AND _time - last_time < 8*60*60 \n| iplocation last_src_ip | rename lat as last_lat lon as last_lon | eval location = City . \"|\" . Country . \"|\" . Region \n| iplocation src_ip \n| eval rlat1 = pi()*last_lat/180, rlat2=pi()*lat/180, rlat = pi()*(lat-last_lat)/180, rlon= pi()*(lon-last_lon)/180 | eval a = sin(rlat/2) * sin(rlat/2) + cos(rlat1) * cos(rlat2) * sin(rlon/2) * sin(rlon/2) | eval c = 2 * atan2(sqrt(a), sqrt(1-a)) | eval distance = 6371 * c, time_difference_hours = round((_time - last_time) / 3600,2), speed=round(distance/ ( time_difference_hours),2) | fields - rlat* a c \n| eval day=strftime(_time, \"%m/%d/%Y\")\n| stats values(accountId) values(awsRegion) values(eventName) values(distance) values(eval(mvappend(last_Country, Country))) as Country values(eval(mvappend(last_City, City))) as City values(eval(mvappend(last_Region, Region))) as Region  values(lat) values(lon)  values(userAgent) max(speed) as max_speed_kph min(time_difference_hours) as min_time_difference_hours by day user distance"
    }, 
    "Land Speed Privileged - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic dataset. In this case, AWS CloudTrail logs.", 
            "Next we pull the last src_ip for the same user using streamstats (sorted based on the user).", 
            "Next we look up the user in our Privileged User lookup. This will add a number of fields, including the risk_score field we'll use in the next command. ", 
            "Here we filter for privileged users (where the risk score is greater than 0), and for events in a short enough time range that it would be difficult to travel to distant parts of the globe.", 
            "Here we resolve the Last src_ip to a physical location, and stick that in a field so that we can conveniently use it.", 
            "Now we resolve the *current* src_ip", 
            "Now we calculate the distance using an approximation for the curvature of the earth. Easy, right? I do not understand it, I copy-pasted from https://answers.splunk.com/answers/317935/calculating-distances-between-points-with-geoip-us.html#answer-568451", 
            "Here we pull the date of the event, to make this easier to run over longer time windows.", 
            "Finally we use stats to collect all of the values into one line, per user, per day, and per set of locations. We're using some specific AWS data fields here -- if you're using a log source like VPN, then you might choose other fields."
        ], 
        "label": "Land Speed Privileged - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have AWS CloudTrail data (though could be applied to other data sources)", 
                "resolution": "In order to run this search, you must have AWS CloudTrail data onboard. Visit the <a href=\"/app/Splunk_Security_Essentials/data_source?technology=AWS%20CloudTrail\">data onboarding guide for AWS CloudTrail in this app</a>, or browse to <a href=\"https://splunkbase.splunk.com/app/1876/\">apps.splunk.com</a> for more information.", 
                "test": "| tstats count where earliest=-2h latest=now index=* sourcetype=aws:cloudtrail"
            }
        ], 
        "value": "index=* sourcetype=aws:cloudtrail user=* \n| sort 0 user, _time | streamstats window=1 current=f values(_time) as last_time values(src_ip) as last_src_ip by user \n| lookup PrivilegedRiskScores user \n| where risk_score>0 AND last_src_ip != src_ip AND _time - last_time < 8*60*60 \n| iplocation last_src_ip | rename lat as last_lat lon as last_lon | eval location = City . \"|\" . Country . \"|\" . Region\n| iplocation src_ip \n| eval rlat1 = pi()*last_lat/180, rlat2=pi()*lat/180, rlat = pi()*(lat-last_lat)/180, rlon= pi()*(lon-last_lon)/180 | eval a = sin(rlat/2) * sin(rlat/2) + cos(rlat1) * cos(rlat2) * sin(rlon/2) * sin(rlon/2) | eval c = 2 * atan2(sqrt(a), sqrt(1-a)) | eval distance = 6371 * c, time_difference_hours = round((_time - last_time) / 3600,2), speed=round(distance/ ( time_difference_hours),2) | fields - rlat* a c \n| eval day=strftime(_time, \"%m/%d/%Y\")\n| stats values(accountId) values(awsRegion) values(eventName) values(distance) values(eval(mvappend(last_Country, Country))) as Country values(eval(mvappend(last_City, City))) as City values(eval(mvappend(last_Region, Region))) as Region  values(lat) values(lon)  values(userAgent) max(speed) as max_speed_kph min(time_difference_hours) as min_time_difference_hours by day user distance"
    }, 
    "Large Web Upload - Accelerated": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "This uses tstats to quickly search an accelerated Web Proxy data model for any requests that are larger than 35 MB, and provides a useful table with the results."
        ], 
        "label": "Large Web Upload - Accelerated", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Proxy data in the Web data model", 
                "resolution": "Proxy data can come in many forms, including from Palo Alto Networks and other NGFWs, dedicated proxies like BlueCoat, or network monitoring tools like Splunk Stream or bro. You must also have the Common Infomration Model app, and have the appropriate TAs installed so that your data is mapped. Follow our in-app data onboarding guides for examples of how to do this (or leverage the non-accelerated version).", 
                "test": "| tstats count from datamodel=Web where earliest=-1h"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an accelerated Web data model", 
                "resolution": "The Web data model must have the acceleration checkbox hit, and must have made decent progress. ", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Web where earliest=-1h"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a bytes_out field", 
                "resolution": "Your proxy must record how much data was sent outbound", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Web where earliest=-1h Web.bytes_out>1"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a user field", 
                "resolution": "Your proxy must record how much data was sent outbound", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Web where earliest=-1h Web.user=*"
            }
        ], 
        "value": "| tstats summariesonly=t allow_old_summaries=t count values(Web.bytes) values(Web.bytes_in) values(Web.bytes_out) values(Web.user) values(Web.app) values(Web.src) values(Web.dest) from datamodel=Web where nodename=Web.proxy Web.bytes_out>35000000 by Web.url _time span=1s"
    }, 
    "Large Web Upload - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic demo dataset, proxy logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Then we just filter for any events that are larger than about 35 MB."
        ], 
        "label": "Large Web Upload - Demo", 
        "prereqs": [
            {
                "field": "bots-webproxy-data.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Web Proxy Logs)` \n| where bytes_out>35000000"
    }, 
    "Large Web Upload - Live": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "DEFINETHIS", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "", 
        "description": [
            "First we bring in our basic dataset, proxy logs, over the last 10 minutes. ", 
            "Then we just filter for any events that are larger than about 35 MB.", 
            "Finally we put things in a nice table so that it's easy to read."
        ], 
        "label": "Large Web Upload - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Proxy data", 
                "resolution": "Proxy data can come in many forms, including from Palo Alto Networks and other NGFWs, dedicated proxies like BlueCoat, or network monitoring tools like Splunk Stream or bro.", 
                "test": "| metasearch earliest=-2h latest=now index=* sourcetype=pan:threat OR (sourcetype=opsec URL Filtering) OR sourcetype=bluecoat:proxysg* OR sourcetype=websense* | head 100 | stats count "
            }
        ], 
        "value": "index=* sourcetype=pan:threat OR (tag=web tag=proxy) earliest=-10m \n| where bytes_out>35000000 \n| table _time src_ip user bytes* app uri "
    }, 
    "Log Clearing With wevtutil - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our basic demo data", 
            "Next we look for any instances of wevtutil being launched (EventCode 1 indicates a process launch), and filter to make sure our suspicious fields are in the CommandLine string.", 
            "Then we put the data into a table because that's the easiest thing to use."
        ], 
        "label": "Log Clearing With wevtutil - Demo", 
        "prereqs": [
            {
                "field": "UC_wevtutil.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| inputlookup UC_wevtutil \n| search EventCode=1 Image=*wevtutil* CommandLine=*cl* (CommandLine=*System* OR CommandLine=*Security* OR CommandLine=*Setup* OR CommandLine=*Application*) \n| table _time host Image CommandLine"
    }, 
    "Log Clearing With wevtutil - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our Sysmon EDR data. We look for any instances of wevtutil being launched (EventCode 1 indicates a process launch), and filter to make sure our suspicious fields are in the CommandLine string.", 
            "Then we put the data into a table because that's the easiest thing to use."
        ], 
        "label": "Log Clearing With wevtutil - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Microsoft Sysmon logs", 
                "resolution": "Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=\"https://splunkbase.splunk.com/app/1914/\">Splunk App</a>. Check out the <a href=\"http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf\">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!", 
                "test": "| metasearch index=* sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\"  earliest=-1h latest=now | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have process start events (EventCode=1)", 
                "resolution": "Check your sysmon configuration file to ensure you are not filtering out EventCode 1 events.", 
                "test": "sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\" EventCode=1 index=* | head 100 | stats count"
            }
        ], 
        "value": "index=* sourcetype=XmlWinEventLog:Microsoft-Windows-Sysmon/Operational EventCode=1 Image=*wevtutil* CommandLine=*cl* (CommandLine=*System* OR CommandLine=*Security* OR CommandLine=*Setup* OR CommandLine=*Application*) \n| table _time host Image CommandLine"
    }, 
    "Login With Local Credentials - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 10, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic demo dataset. This dataset includes interactive logins from Windows Security logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data. If you're paying close attention, you'll notice that the dataset here is called Legacy Pass the Hash. The reason? The legacy (and not really relvant) pass the hash detection actually resembles this detection, where we are looking for unexpected Account_Domains.", 
            "Next we filter out the domains that we are expecting to see. Controversially, we are also ignoring accounts that end in a dollar sign, which will typically occur from server accounts. This is a problematic assumption, as there's nothing to keep attackers from using dollar sign usernames for their own purposes -- as you mature this detection, try to move away from this limitation.", 
            "Next we use eval to filter out the Account_Domain of \"-\" as Windows will typically include blank domains in one of the log fields, and we don't want to distract analysts. We aren't putting this in the prior search command because we don't want to exclude those events, we just want to strip that value from the field."
        ], 
        "label": "Login With Local Credentials - Demo", 
        "prereqs": [
            {
                "field": "Example_Legacy_Pass_The_Hash.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "|  `Load_Sample_Log_Data(\"Example Pass The Hash (Legacy)\")` \n| search NOT (Account_Domain=\"NT Authority\" OR Account_Domain=\"MYDOMAIN\" OR Account_Domain=\"SUBSIDIARY*\" OR Account_Name=\"*$\") \n| eval Account_Domain=mvfilter(Account_Domain!=\"-\")"
    }, 
    "Login With Local Credentials - Live": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 10, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic dataset. This dataset includes successful interactive logins (logon type 2, 10, 11) from Windows Security logs where we filter out the domains that we are expecting to see. Controversially, we are also ignoring accounts that end in a dollar sign, which will typically occur from server accounts. This is a problematic assumption, as there's nothing to keep attackers from using dollar sign usernames for their own purposes -- as you mature this detection, try to move away from this limitation.", 
            "Next we use eval to filter out the Account_Domain of \"-\" as Windows will typically include blank domains in one of the log fields, and we don't want to distract analysts. We aren't putting this in the prior search command because we don't want to exclude those events, we just want to strip that value from the field."
        ], 
        "label": "Login With Local Credentials - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security data", 
                "resolution": "This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.", 
                "test": "| metasearch earliest=-2h latest=now source=\"WinEventLog:Security\" index=* | head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Logon Success Data", 
                "resolution": "You should log logon events. There are many event IDs that we look for in the underlying logs, but they should all fall into the Audit Successful (or Failed) Logon events in your Windows Audit Policy. (<a href=\"https://technet.microsoft.com/en-us/library/cc431373.aspx\">docs</a>)", 
                "test": "source=\"WinEventLog:Security\" tag=authentication action=success index=* | head 100 | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have the Account_Domain field defined", 
                "resolution": "You should have a field called \"Account_Domain\" defined in your Windows Security logs. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!", 
                "test": "source=\"WinEventLog:Security\" tag=authentication action=success earliest=-2h index=*  | head 100 | stats dc(Account_Domain) as count"
            }
        ], 
        "value": "index=* source=win*security tag=authentication action=success Logon_Type=2 OR Logon_Type=10 OR Logon_Type=11 Logon Type NOT (Account_Domain=\"NT Authority\" OR Account_Domain=\"MYDOMAIN\" OR Account_Domain=\"SUBSIDIARY*\" OR Account_Domain=\"YourDomainsHere\" OR Account_Name=\"*$\")\n| eval  Account_Domain=mvfilter(Account_Domain!=\"-\")"
    }, 
    "Login to New System GDPR - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 10, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic demo dataset. This dataset includes interactive logins from Windows Security logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.", 
            "Next we look up the user in the GDPR categorization lookup.", 
            "Because a user or host can belong to many different categories, we use mvexpand to split them into a multi-value field.", 
            "Finally we look for users who don't have a matching GDPR category, or who aren't authorized for any GDPR information at all."
        ], 
        "label": "Login to New System GDPR - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Interactive Logins)`  \n| lookup gdpr_system_category.csv host as dest OUTPUT category as dest_category | search dest_category=* \n| lookup gdpr_user_category user OUTPUT category as user_category \n| makemv delim=\"|\" dest_category | makemv delim=\"|\" user_category \n| where isnull(user_category) OR user_category != dest_category"
    }, 
    "Login to New System GDPR - Live": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 10, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic dataset. This dataset includes logins from Windows Security logs.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same hour.", 
            "Now we summarize the number of logins by user, or destination.", 
            "Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.", 
            "Next we look up the user in the GDPR categorization lookup.", 
            "Because a user or host can belong to many different categories, we use mvexpand to split them into a multi-value field.", 
            "Finally we look for users who don't have a matching GDPR category, or who aren't authorized for any GDPR information at all."
        ], 
        "label": "Login to New System GDPR - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security data", 
                "resolution": "This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.", 
                "test": "| metasearch earliest=-2h latest=now source=\"WinEventLog:Security\" index=* | head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Logon Success Data", 
                "resolution": "You should log logon events. There are many event IDs that we look for in the underlying logs, but they should all fall into the Audit Successful (or Failed) Logon events in your Windows Audit Policy. (<a href=\"https://technet.microsoft.com/en-us/library/cc431373.aspx\">docs</a>)", 
                "test": "source=\"WinEventLog:Security\" (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) index=* | head 100 | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have the user field defined", 
                "resolution": "You should have a field called \"user\" defined in your Windows Security logs. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!", 
                "test": "source=\"WinEventLog:Security\" earliest=-2h index=* (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) | head 100 | stats dc(user) as count"
            }
        ], 
        "value": "index=* source=win*security user=* dest=* action=success \n| bucket _time span=1d \n| stats count by user, dest \n| lookup gdpr_system_category.csv host as dest OUTPUT category as dest_category | search dest_category=* \n| lookup gdpr_user_category user OUTPUT category as user_category\n| makemv delim=\"|\" dest_category | makemv delim=\"|\" user_category \n| where isnull(user_category) OR user_category != dest_category"
    }, 
    "Monitor TOR traffic - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our NGFW demo data", 
            "We filter for where the firewall detects the presence of tor, and where we know the source_ip involved.", 
            "Finally we put everything in a table so that it's easy to use."
        ], 
        "label": "Monitor TOR traffic - Demo", 
        "prereqs": [
            {
                "field": "UC_tor_traffic", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| inputlookup UC_tor_traffic \n| search app=tor src_ip=* \n| table _time src_ip src_port dest_ip dest_port bytes app"
    }, 
    "Monitor TOR traffic - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our NGFW data (we include many different options here -- you should specify the index and sourcetype for the device that exists in your environment. Then we filter for where the firewall detects the presence of tor, and where we know the source_ip involved.", 
            "Finally we put everything in a table so that it's easy to use."
        ], 
        "label": "Monitor TOR traffic - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have network traffic.", 
                "resolution": "Ingest network traffic logs, consider using Splunk Stream.", 
                "test": "index=* ((tag=network tag=communicate) OR (sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco:asa OR sourcetype=stream*)) earliest=-1h latest=now | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Make sure the app field is populated.", 
                "resolution": "Use a NGFW or another tool to identify the application layer protocol.", 
                "test": "index=* ((tag=network tag=communicate) OR (sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco:asa OR sourcetype=stream*)) earliest=-1h latest=now | stats dc(app) as count"
            }
        ], 
        "value": "index=* ((tag=network tag=communicate) OR (sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco:asa OR sourcetype=stream*)) app=tor src_ip=* \n| table _time src_ip src_port dest_ip dest_port bytes app"
    }, 
    "Multiple Infections on Host - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Computer_Name", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic demo dataset, Symantec Endpoint Protection Risks. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "While there are several approaches to grouping events, and stats is the fastest, we're using transaction because it's the easiest. This will let us group all the events based on the Computer_Name.", 
            "Finally we can filter for if there are at least three events and they spanned at least a few minutes."
        ], 
        "label": "Multiple Infections on Host - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Symantec Endpoint Protection Risks)` \n| transaction maxpause=1h Computer_Name \n| where eventcount >=3 AND duration>240"
    }, 
    "Multiple Infections on Host - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Computer_Name", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic dataset, Symantec Endpoint Protection Risks, over the last 24 hours.", 
            "While there are several approaches to grouping events, and stats is the fastest, we're using transaction because it's the easiest. This will let us group all the events based on the Computer_Name.", 
            "Finally we can filter for if there are at least three events and they spanned at least a few minutes."
        ], 
        "label": "Multiple Infections on Host - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Symantec AV data", 
                "resolution": "For simplicity, this search is written specifically for Symantec AV data, but it can be easily modified for other sources. ", 
                "test": "| metasearch earliest=-24h latest=now index=* sourcetype=symantec:ep:* | head 100 | stats count "
            }
        ], 
        "value": "index=* sourcetype=symantec:* earliest=-24h \n| transaction maxpause=1h Computer_Name \n| where eventcount >=3 AND duration>240"
    }, 
    "New Connection GDPR - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic dataset. In this case, Firewall Data. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we use stats to caluate the earliest and the latest time we've seen this calculation.", 
            "Eventstats allows us to look across the entire dataset and determine what the largest \"latest\" time is. This step is only necessary when you don't have fresh data, so it really only applies to the demo data.", 
            "Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.", 
            "Finally, we filter for events where the earliest time seen is within a day of the latest time seen (aka, this is the first time we've seen this)."
        ], 
        "label": "New Connection GDPR - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Sample Firewall Data)`  \n| stats count min(_time) as earliest max(_time) as maxtime  by src_ip, dest_ip \n| eventstats max(maxtime) as maxtime \n| lookup gdpr_system_category host as dest_ip | search category=*\n| where earliest>relative_time(maxtime, \"-1d@d\")"
    }, 
    "New Connection GDPR - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "src_ip", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic dataset. In this case, Firewall Data.", 
            "Next we use stats to caluate the earliest and the latest time we've seen this calculation.", 
            "Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.", 
            "Finally, we filter for events where the earliest time is within the last day (aka, this is the first time we've seen this)."
        ], 
        "label": "New Connection GDPR - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Firewall data", 
                "resolution": "This search requires Firewall or Netflow data to run. By default, we're checking for Common Information Model compliant data, and then also manually specifying the standard sourcetypes for Check Point, Palo Alto Networks, and Cisco ASAs. You should specify your particular index and sourcetype in the actual search to improve performance (or better yet, accelerate with the common information model!)", 
                "test": "index=* ((tag=traffic tag=communicate) OR sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco*)| head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a dest_ip and src_ip field", 
                "resolution": "This search is also looking for firewall logs, but with the added filter of making sure that a dest_ip and src_ip defined.", 
                "test": "index=* ((tag=traffic tag=communicate) OR sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco*) dest_ip=* src_ip=*| head 100 | stats count "
            }
        ], 
        "value": "index=* ((tag=traffic tag=communicate) OR sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco*) src_ip=* dest_ip=* \n| stats count min(_time) as earliest max(_time) as maxtime  by src_ip, dest_ip \n| lookup gdpr_system_category host as dest_ip | search category=*\n| where earliest>relative_time(now(), \"-1d@d\")"
    }, 
    "New Local Admin - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Account_Name", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we pull in our demo dataset.", 
            "This line won't exist in production, it is just so that we can format the demo data (coming from a CSV file) correctly.", 
            "Next we filter to make sure we're looking for just account creation events or account changes with group membership events.", 
            "Transaction will now group everything together so that we can see multiple events occurring to the same username.", 
            "We can now filter for users where both event IDs occurred.", 
            "Finally we can display everything in a nice table for the user to consume."
        ], 
        "label": "New Local Admin - Demo", 
        "prereqs": [
            {
                "field": "Local_Short_Lived_Account.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "|`Load_Sample_Log_Data(\"Local Short-Lived Account\")` \n| rex mode=sed field=Security_ID \"s/\n/;/g\" | makemv Security_ID delim=\";\"| makemv Account_Name delim=\";\" \n| search EventCode=4720 OR (EventCode=4732 Administrators) \n| transaction Security_ID maxspan=180m \n| search EventCode=4720 EventCode=4732\n| table _time EventCode Security_ID Group_Name Account_Name Message "
    }, 
    "New Local Admin - Live": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Account_Name", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we pull in our dataset, of Windows Security Logs with account creation events or account changes with group membership events.", 
            "Transaction will now group everything together so that we can see multiple events occurring to the same username.", 
            "Now we can filter to just transactions with both event IDs", 
            "Finally we can display everything in a nice table for the user to consume."
        ], 
        "label": "New Local Admin - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security Logs", 
                "resolution": "Begin ingesting Windows Security Logs", 
                "test": "| metasearch index=* earliest=-2h latest=now source=\"winEventLog:Security\" | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Local Account Management Logs (Event ID 4720)", 
                "resolution": "Turn on Account Management Audit Logs in your Local Windows Security Policy (<a href=\"http://www.thewindowsclub.com/track-user-activity-windows\">docs</a>)", 
                "test": "| metasearch earliest=-30d latest=now index=* source=\"winEventLog:Security\" TERM(eventcode=4720) | head | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Local Group Management Logs (Event ID 4732)", 
                "resolution": "Turn on Group Management Audit Logs in your Local Windows Security Policy (<a href=\"http://whatevernetworks.com/?p=21\">docs</a>)", 
                "test": "| metasearch earliest=-30d latest=now index=* source=\"winEventLog:Security\" TERM(eventcode=4732) | head | stats count "
            }
        ], 
        "value": "index=* source=\"winEventLog:Security\" EventCode=4720 OR (EventCode=4732 Administrators) \n| transaction Security_ID maxspan=180m \n| search EventCode=4720 (EventCode=4732 Administrators) \n | table _time EventCode Account_Name Target_Account_Name Message"
    }, 
    "Old Passwords In Use - CSV": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "sAMAccountName", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic demo dataset, the output of the ldapsearch search command against an AD environment. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Active Directory gives us timestamps in a couple of formats, none of which allow us to do comparisons. To solve this, we use the convert search command to convert those other time formats into Unix epoch time (a nice and clean number) that we can do math on.", 
            "Now we look for any accounts where the password is more than 120 days old, and the login is in the last 30 days. Why 120 days? If someone's password expires while they're on vacation, we don't want to alert, so this gives us a month of buffer. We also are filtering for only accounts that are actively used, so that we don't trigger on old disabled accounts (though that's also a worthwhile detection!). ", 
            "We've got our suspect values, our last step is to format the data to be more meaningful to an analyst by converting those numeric timestamps into human-readable ones."
        ], 
        "label": "Old Passwords In Use - CSV", 
        "prereqs": [
            {
                "field": "UC_active_directory_search.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| inputlookup UC_active_directory_search.csv \n| convert timeformat=\"%Y-%m-%dT%H:%M:%S.%6QZ\" mktime(pwdLastSet) mktime(lastLogonTimestamp) | convert timeformat=\"%Y%m%d%H%M%S.0Z\"  mktime(whenCreated) \n| where pwdLastSet < relative_time(now(), \"-120d\") AND lastLogonTimestamp > relative_time(now(), \"-30d\") \n| convert ctime(lastLogonTimestamp) ctime(whenCreated) ctime(pwdLastSet)"
    }, 
    "Old Passwords In Use - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "sAMAccountName", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic demo dataset, the output of the ldapsearch search command against an AD environment. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Active Directory gives us timestamps in a couple of formats, none of which allow us to do comparisons. To solve this, we use the convert search command to convert those other time formats into Unix epoch time (a nice and clean number) that we can do math on.", 
            "This step is only required for demo data -- eval has a now() function that will tell us the current epoch time, but because we're using demo data that gets more out of date with every day that passes, we have to calculate that manually. You won't see this in the live queries.", 
            "Now we look for any accounts where the password is more than 120 days old, and the login is in the last 30 days. Why 120 days? If someone's password expires while they're on vacation, we don't want to alert, so this gives us a month of buffer. We also are filtering for only accounts that are actively used, so that we don't trigger on old disabled accounts (though that's also a worthwhile detection!). ", 
            "We've got our suspect values, our last step is to format the data to be more meaningful to an analyst by converting those numeric timestamps into human-readable ones. We also clear out the unnecessary maxtime field."
        ], 
        "label": "Old Passwords In Use - Demo", 
        "prereqs": [
            {
                "field": "UC_active_directory_search.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(LDAPSearch Output)` \n| convert timeformat=\"%Y-%m-%dT%H:%M:%S.%6QZ\" mktime(pwdLastSet) mktime(lastLogonTimestamp) | convert timeformat=\"%Y%m%d%H%M%S.0Z\"  mktime(whenCreated) \n| eventstats max(lastLogonTimestamp) as maxtime\n| where pwdLastSet < relative_time(maxtime, \"-120d\") AND lastLogonTimestamp > relative_time(maxtime, \"-30d\") \n| convert ctime(lastLogonTimestamp) ctime(whenCreated) ctime(pwdLastSet) | fields - maxtime"
    }, 
    "Old Passwords In Use - LDAPSearch": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "sAMAccountName", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we run our LDAP query. This requires that you've properly configured the app, and often that you've got some basic understanding of how your Active Directory is configured. When in doubt, consult with an AD admin about where your user and service accounts are located.", 
            "Active Directory gives us timestamps in a couple of formats, none of which allow us to do comparisons. To solve this, we use the convert search command to convert those other time formats into Unix epoch time (a nice and clean number) that we can do math on.", 
            "Now we look for any accounts where the password is more than 120 days old, and the login is in the last 30 days. Why 120 days? If someone's password expires while they're on vacation, we don't want to alert, so this gives us a month of buffer. We also are filtering for only accounts that are actively used, so that we don't trigger on old disabled accounts (though that's also a worthwhile detection!). ", 
            "We've got our suspect values, our last step is to format the data to be more meaningful to an analyst by converting those numeric timestamps into human-readable ones."
        ], 
        "label": "Old Passwords In Use - LDAPSearch", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Splunk Supporting Add-on for Active Directory Installed", 
                "resolution": "The Splunk Supporting Add-on for Active Directory app allows us to query AD environments via LDAP to get everything we need. Check it out -- <a href=\"https://splunkbase.splunk.com/app/1151/\" target=\"_blank\">app link</a>, <a href=\"http://docs.splunk.com/Documentation/SA-LdapSearch/2.1.6/User/AbouttheSplunkSupportingAdd-onforActiveDirectory\" target=\"_blank\">docs link</a>.", 
                "test": "| rest /services/apps/local | search disabled=0 title=\"SA-ldapsearch\" | stats count"
            }
        ], 
        "value": "| ldapsearch search=\"(&(objectclass=user)(!(objectClass=computer)))\" attrs=\"sAMAccountName,pwdLastSet,lastLogonTimestamp,whenCreated,badPwdCount,logonCount\" | fields - _raw host _time \n| convert timeformat=\"%Y-%m-%dT%H:%M:%S.%6QZ\" mktime(pwdLastSet) mktime(lastLogonTimestamp) | convert timeformat=\"%Y%m%d%H%M%S.0Z\"  mktime(whenCreated) \n| where pwdLastSet < relative_time(now(), \"-120d\") AND lastLogonTimestamp > relative_time(now(), \"-30d\") \n| convert ctime(lastLogonTimestamp) ctime(whenCreated) ctime(pwdLastSet)"
    }, 
    "Outdated Malware Definitions - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Host_Name", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic demo dataset, Symantec Endpoint Operational Logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we use a relatively complicated stats command to track the time of the last update, and the time of the last error.", 
            "Next we filter for the events where the time of the last update was more than three days ago, or where the last error was more recent than the last update.", 
            "Finally, we format the timestamps in a human readable way."
        ], 
        "label": "Outdated Malware Definitions - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Symantec Endpoint Protection Operations)` \n|stats max(eval(if(like(Event_Description, \"%LiveUpdate session ran successfully%\") , _time, null))) as LatestUpdate max(_time) as LatestMessage max(eval(if(tag=\"error\", _time, null))) as LatestError by Host_Name  \n| where LatestUpdate < relative_time(LatestMessage, \"-3d\") OR LatestError > LatestUpdate \n| convert ctime(LatestUpdate) ctime(LatestMessage) ctime(LatestError)"
    }, 
    "Outdated Malware Definitions - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Host_Name", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic dataset, Symantec Endpoint Operational Logs.", 
            "Next we use a relatively complicated stats command to track the time of the last update, and the time of the last error.", 
            "Next we filter for the events where the time of the last update was more than three days ago, or where the last error was more recent than the last update.", 
            "Finally, we format the timestamps in a human readable way."
        ], 
        "label": "Outdated Malware Definitions - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Symantec AV data", 
                "resolution": "For simplicity, this search is written specifically for Symantec AV data, but it can be easily modified for other sources. ", 
                "test": "| metasearch earliest=-24h latest=now index=* sourcetype=symantec:ep:* | head 100 | stats count "
            }
        ], 
        "value": " index=* sourcetype=symantec:* \n|stats max(eval(if(like(Event_Description, \"%LiveUpdate session ran successfully%\") , _time, null))) as LatestUpdate max(_time) as LatestMessage max(eval(if(tag=\"error\", _time, null))) as LatestError by Host_Name \n| where LatestUpdate < relative_time(LatestMessage, \"-3d\") OR LatestError > LatestUpdate \n| convert ctime(LatestUpdate) ctime(LatestMessage) ctime(LatestError)"
    }, 
    "Outdated Malware Definitions GDPR - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Host_Name", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic demo dataset, Symantec Endpoint Operational Logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we use a relatively complicated stats command to track the time of the last update, and the time of the last error.", 
            "Next we filter for the events where the time of the last update was more than three days ago, or where the last error was more recent than the last update.", 
            "Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.", 
            "Finally, we format the timestamps in a human readable way."
        ], 
        "label": "Outdated Malware Definitions GDPR - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Symantec Endpoint Protection Operations)` \n|stats max(eval(if(like(Event_Description, \"%LiveUpdate session ran successfully%\") , _time, null))) as LatestUpdate max(_time) as LatestMessage max(eval(if(tag=\"error\", _time, null))) as LatestError by Host_Name  \n| where LatestUpdate < relative_time(LatestMessage, \"-3d\") OR LatestError > LatestUpdate \n| lookup gdpr_system_category host as Host_Name | search category=*\n| convert ctime(LatestUpdate) ctime(LatestMessage) ctime(LatestError)"
    }, 
    "Outdated Malware Definitions GDPR - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Host_Name", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic dataset, Symantec Endpoint Operational Logs.", 
            "Next we use a relatively complicated stats command to track the time of the last update, and the time of the last error.", 
            "Next we filter for the events where the time of the last update was more than three days ago, or where the last error was more recent than the last update.", 
            "Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.", 
            "Finally, we format the timestamps in a human readable way."
        ], 
        "label": "Outdated Malware Definitions GDPR - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Symantec AV data", 
                "resolution": "For simplicity, this search is written specifically for Symantec AV data. Some other AV vendors will report similar information.", 
                "test": "| metasearch earliest=-24h latest=now index=* sourcetype=symantec:ep:* | head 100 | stats count "
            }
        ], 
        "value": " index=* sourcetype=symantec:* \n|stats max(eval(if(like(Event_Description, \"%LiveUpdate session ran successfully%\") , _time, null))) as LatestUpdate max(_time) as LatestMessage max(eval(if(tag=\"error\", _time, null))) as LatestError by Host_Name \n| where LatestUpdate < relative_time(LatestMessage, \"-3d\") OR LatestError > LatestUpdate \n| lookup gdpr_system_category Host_Name | search category=* \n| convert ctime(LatestUpdate) ctime(LatestMessage) ctime(LatestError)"
    }, 
    "Privileged Actions by Unprivileged Users - Demo": {
        "actions_UBASeverity": 2, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 10, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic demo dataset. In this case, a list of anonymized Windows events with the EventCode and the tags (which come from the technology add-ons). We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "This line is only needed for demo data, to convert a semi-colon separated list into a multi-value field. In the live search, it's already multi-value.", 
            "Next we use a lookup with the risk scores of user accounts. In our Pulling List of Privileged User use case, we calculate what users have admin rights.", 
            "Finally, we filter for where the account is not an admin account, and the event is tagged as being privileged."
        ], 
        "label": "Privileged Actions by Unprivileged Users - Demo", 
        "prereqs": [
            {
                "field": "anonymized_windows_security_events_with_tags.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup with Risk Events", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }, 
            {
                "field": "PrivilegedRiskScores.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Lookup with Privileged Risk Scores", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Windows Events with Tags)` \n| makemv tag delim=\";\" \n| lookup PrivilegedRiskScores user OUTPUT isAdminAccount \n| search isAdminAccount=0 tag=privileged"
    }, 
    "Privileged Actions by Unprivileged Users - Live": {
        "actions_UBASeverity": 2, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 10, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our dataset of Windows Security events, filtered to just those that are tagged as being privileged actions.", 
            "Next we use a lookup with the risk scores of user accounts. In our Pulling List of Privileged User use case, we calculate what users have admin rights.", 
            "Finally, we filter for where the account is not an admin account."
        ], 
        "label": "Privileged Actions by Unprivileged Users - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a risk index", 
                "resolution": "This search presumes the presence of Splunk Enteprise Security to provide the Risk Framework. Reach out to your Splunk team to find out more about Splunk ES, or adapt this search for your own list of risk events.", 
                "test": "| metasearch index=risk | head 100"
            }
        ], 
        "value": "index=* source=win*security tag=privileged\n| lookup PrivilegedRiskScores user OUTPUT isAdminAccount \n| search isAdminAccount=0 "
    }, 
    "Processes With High Entropy Names in Users Directory - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Endpoint", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our demo dataset. We are filtering here to just process launches in the Users directory, so that we are focusing in on what unprivileged users can run (and not getting noise from things like software updates).", 
            "Next we use the Shannon Entropy algorithm provided by the free app URL Toolbox to calculate a very basic randomness score for this string.", 
            "Shannon Entropy gives a numeric score, you will usually want to filter on values above of 3.5 or 4.", 
            "Finally we use stats to put everything in a convenient table.", 
            "And of course we use rename to provide field names that will make sense to analysts."
        ], 
        "label": "Processes With High Entropy Names in Users Directory - Demo", 
        "prereqs": [
            {
                "field": "STE_Win4688.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have URL Toolbox Installed (provides Shannon entropy checking)", 
                "resolution": "The URL Toolbox app, written by Cedric Le Roux, not only provides effective URL Parsing but also Levenshtein similarity checking (e.g., typo detection) and Shannon entropy detection. Download <a href=\"https://splunkbase.splunk.com/app/2734/\">here</a>.", 
                "test": "| rest /services/apps/local | search disabled=0 label=\"URL Toolbox\" | stats count"
            }
        ], 
        "value": "| `Load_Sample_Log_Data(\"Windows 4688 Data\")` | search New_Process_Name=*\\Users\\*\n| lookup ut_shannon_lookup word as New_Process_Name \n| where ut_shannon > 4.5 \n| stats  values(ut_shannon)  as \"Shannon Entropy Score\" by New_Process_Name,host \n| rename  New_Process_Name as Process,host as Endpoint "
    }, 
    "Processes With High Entropy Names in Users Directory - Live": {
        "actions_UBASeverity": 7, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Endpoint", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our basic dataset, which consists of Process Launch Logs (in this case coming from Windows Security Event ID 4688, but could come from any). We are filtering here to just process launches in the Users directory, so that we are focusing in on what unprivileged users can run (and not getting noise from things like software updates).", 
            "Next we use the Shannon Entropy algorithm provided by the free app URL Toolbox to calculate a very basic randomness score for this string.", 
            "Shannon Entropy gives a numeric score, you will usually want to filter on values above of 4 or 4.5.", 
            "Finally we use stats to put everything in a convenient table.", 
            "And of course we use rename to provide field names that will make sense to analysts."
        ], 
        "label": "Processes With High Entropy Names in Users Directory - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security Logs", 
                "resolution": "Begin ingesting Windows Security Logs", 
                "test": "| metasearch index=* earliest=-2h latest=now source=\"winEventLog:Security\" | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Process Launch Logs (Event ID 4688)", 
                "resolution": "Turn on Process Tracking in your Windows Audit logs (<a href=\"https://technet.microsoft.com/en-us/library/cc976411.aspx\">docs</a>)", 
                "test": "earliest=-2h latest=now index=* source=\"winEventLog:Security\" EventCode=4688 | head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have URL Toolbox Installed (provides Shannon Entropy Checking)", 
                "resolution": "Splunk's URL Toolbox app, written by Cedric Le Roux, not only provides effective URL Parsing but also Levenshtein similarity checking (e.g., typo detection) and Shannon Entropy Detection. Download <a href=\"https://splunkbase.splunk.com/app/2734/\">here</a>.", 
                "test": "| rest /services/apps/local | search disabled=0 label=\"URL Toolbox\" | stats count"
            }
        ], 
        "value": "index=* source=\"winEventLog:Security\" EventCode=4688 New_Process_Name=C:\\\\Users*\n| lookup ut_shannon_lookup word as New_Process_Name \n| where ut_shannon > 4.5 \n| stats  values(ut_shannon)  as \"Shannon Entropy Score\" by New_Process_Name,host \n| rename  New_Process_Name as Process,host as Endpoint | sort  -\"Shannon Entropy Score\" "
    }, 
    "Processes With Lookalike Filenames - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "hosts", 
        "actions_riskObjectScore": 80, 
        "actions_riskObjectType": "system", 
        "description": [
            "This is one of the longest searches in Splunk Security Essentials, but we'll break it down for you. We start easy: first we pull in our demo dataset.", 
            "Earlier versions of sysmon didn't extract a filename by default, so we are adding that in here.", 
            "Ultimately when we're analyzing filenames, we are going to want to have a stats by filename, but with the context to understand the broader incident. So here we're adding the hosts that ran that filename, and the Image (file path) that it ran from.", 
            "We are about to start the process of comparing multiple standard Windows processes against each process that runs, and recording the scores in a field called ut_levenshtein. To make that work, we first have to initialize the field, so that we can continually add to it.", 
            "In this line, we are comparing the filename we see to svchost.exe using the Levenshtein algorithm that comes packaged in the free URL Toolbox app. Levenshtein will compare two strings and count the number of edits it would take to make them match, e.g., svchost.exe vs ssvchost.exe would be a difference of 1 because you'd need to add one character. It is known as an edit distance algorithm. At the end, we are using eval's mvappend command to add the score to the field levenshtein_scores, which by the end will have four different values.", 
            "This one is a neat SPL trick, though it only works in this one scenario. When you have an eval (and not an eval inside of a | foreach, or anything like that), you specify another variable in curly braces and it will insert the value of that variable. So here, ut_levenshtein contains a numeric score.. let's suppose it is a 2 for argument sake. What we're going to end up doing is assigning the value of comparisonterm (svchost.exe) to a field called score2. Note that this only works when it's on the left hand side in an eval statement (e.g., you can't create pointers, for those coming from C/C++ land), and it doesn't work inside of things like foreach. But still, it can allow you to do some awesome things you probably didn't expect.", 
            "Now we're going to repeat lines 4-6 for the term iexplore.exe, again adding the score to levenshtein_scores.", 
            "Now we're going to repeat lines 4-6 for the term ipconfig.exe, again adding the score to levenshtein_scores.", 
            "Now we're going to repeat lines 4-6 for the term explorer.exe, again adding the score to levenshtein_scores.", 
            "Before we filter, we're going to grab the total number of hosts in our environment, so we can filter out files that are seen by all of them (unlikely to be malware).", 
            "Now we filter out noise. Generally with Levenshtein, we look for scores that are greater then 0 (i.e., not an exact match), but less than 3.", 
            "Great, we now just have suspicious process launches, so it's now time to start making this usable for an analyst. To start with, let's grab that lowest levenshtein_scores value so we can tell them that.", 
            "Now we need to pull out the matching suspicious filename. This uses foreach, which basically iterates over anything starting with score (remember that SPL trick from line 6?), and if the score is less than 3, it will add it to the suspect_files field.", 
            "Next we calculate what percentage of the environment shows this filename.", 
            "Finally! Finally, we create a nice table"
        ], 
        "label": "Processes With Lookalike Filenames - Demo", 
        "prereqs": [
            {
                "field": "generic_sysmon_service_launch_logs.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have URL Toolbox Installed (provides Levenshtein lookalike detection)", 
                "resolution": "The URL Toolbox app, written by Cedric Le Roux, not only provides effective URL Parsing but also Levenshtein similarity checking (e.g., typo detection) and Shannon entropy detection (random characters). Download <a href=\"https://splunkbase.splunk.com/app/2734/\">here</a>.", 
                "test": "| rest /services/apps/local | search disabled=0 label=\"URL Toolbox\" | stats count"
            }
        ], 
        "value": "| `Load_Sample_Log_Data(\"Generic Sysmon Process Launches\")` \n| rex field=Image \"(?<filename>[^\\\\\\\\/]*$)\" \n| stats values(host) as hosts dc(host) as num_hosts values(Image) as Images by filename \n| eval levenshtein_scores=null \n| eval comparisonterm=\"svchost.exe\"  | lookup ut_levenshtein_lookup word1 as filename, word2 as comparisonterm | eval levenshtein_scores=mvappend(levenshtein_scores, ut_levenshtein)\n| eval score{ut_levenshtein} = comparisonterm \n| eval comparisonterm=\"iexplore.exe\" | lookup ut_levenshtein_lookup word1 as filename, word2 as comparisonterm | eval levenshtein_scores=mvappend(levenshtein_scores, ut_levenshtein) | eval score{ut_levenshtein} = comparisonterm \n| eval comparisonterm=\"ipconfig.exe\" | lookup ut_levenshtein_lookup word1 as filename, word2 as comparisonterm | eval levenshtein_scores=mvappend(levenshtein_scores, ut_levenshtein) | eval score{ut_levenshtein} = comparisonterm \n| eval comparisonterm=\"explorer.exe\" | lookup ut_levenshtein_lookup word1 as filename, word2 as comparisonterm | eval levenshtein_scores=mvappend(levenshtein_scores, ut_levenshtein) | eval score{ut_levenshtein} = comparisonterm \n| eventstats max(num_hosts) as max_num_hosts \n| where isnull(mvfilter(levenshtein_scores=\"0\")) AND min(levenshtein_scores) <3 \n| eval lowest_levenshtein_score=min(levenshtein_scores) \n| eval suspect_files = null | foreach score* [eval temp = \"<<FIELD>>\" | rex field=temp \"(?<num>\\d*)$\" | eval suspect_files=if(num<3,mvappend('<<FIELD>>', suspect_files),suspect_files) | fields - temp \"<<FIELD>>\"] \n| eval percentage_of_hosts_affected = round(100*num_hosts/max_num_hosts,2)\n| table filename lowest_levenshtein_score suspect_files Images hosts num_hosts percentage_of_hosts_affected"
    }, 
    "Processes With Lookalike Filenames - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "hosts", 
        "actions_riskObjectScore": 80, 
        "actions_riskObjectType": "system", 
        "description": [
            "This is one of the longest searches in Splunk Security Essentials, but we'll break it down for you. We start easy: first we pull in our dataset of process launch events (here from Windows, but could come from any EDR data source).", 
            "Earlier versions of sysmon didn't extract a filename by default, so we are adding that in here.", 
            "Ultimately when we're analyzing filenames, we are going to want to have a stats by filename, but with the context to understand the broader incident. So here we're adding the hosts that ran that filename, and the Image (file path) that it ran from.", 
            "We are about to start the process of comparing multiple standard Windows processes against each process that runs, and recording the scores in a field called ut_levenshtein. To make that work, we first have to initialize the field, so that we can continually add to it.", 
            "In this line, we are comparing the filename we see to svchost.exe using the Levenshtein algorithm that comes packaged in the free URL Toolbox app. Levenshtein will compare two strings and count the number of edits it would take to make them match, e.g., svchost.exe vs ssvchost.exe would be a difference of 1 because you'd need to add one character. It is known as an edit distance algorithm. At the end, we are using eval's mvappend command to add the score to the field levenshtein_scores, which by the end will have four different values.", 
            "This one is a neat SPL trick, though it only works in this one scenario. When you have an eval (and not an eval inside of a | foreach, or anything like that), you specify another variable in curly braces and it will insert the value of that variable. So here, ut_levenshtein contains a numeric score.. let's suppose it is a 2 for argument sake. What we're going to end up doing is assigning the value of comparisonterm (svchost.exe) to a field called score2. Note that this only works when it's on the left hand side in an eval statement (e.g., you can't create pointers, for those coming from C/C++ land), and it doesn't work inside of things like foreach. But still, it can allow you to do some awesome things you probably didn't expect.", 
            "Now we're going to repeat lines 4-6 for the term iexplore.exe, again adding the score to levenshtein_scores.", 
            "Now we're going to repeat lines 4-6 for the term ipconfig.exe, again adding the score to levenshtein_scores.", 
            "Now we're going to repeat lines 4-6 for the term explorer.exe, again adding the score to levenshtein_scores.", 
            "Before we filter, we're going to grab the total number of hosts in our environment, so we can filter out files that are seen by all of them (unlikely to be malware).", 
            "Now we filter out noise. Generally with Levenshtein, we look for scores that are greater then 0 (i.e., not an exact match), but less than 3.", 
            "Great, we now just have suspicious process launches, so it's now time to start making this usable for an analyst. To start with, let's grab that lowest levenshtein_scores value so we can tell them that.", 
            "Now we need to pull out the matching suspicious filename. This uses foreach, which basically iterates over anything starting with score (remember that SPL trick from line 6?), and if the score is less than 3, it will add it to the suspect_files field.", 
            "Next we calculate what percentage of the environment shows this filename.", 
            "Finally! Finally, we create a nice table"
        ], 
        "label": "Processes With Lookalike Filenames - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security Logs", 
                "resolution": "Begin ingesting Windows Security Logs", 
                "test": "| metasearch index=* earliest=-2h latest=now source=\"winEventLog:Security\" | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Process Launch Logs (Event ID 4688)", 
                "resolution": "Turn on Process Tracking in your Windows Audit logs (<a href=\"https://technet.microsoft.com/en-us/library/cc976411.aspx\">docs</a>)", 
                "test": "earliest=-2h latest=now index=* source=\"winEventLog:Security\" EventCode=4688 | head 100 | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have URL Toolbox Installed (provides Levenshtein lookalike detection)", 
                "resolution": "The URL Toolbox app, written by Cedric Le Roux, not only provides effective URL Parsing but also Levenshtein similarity checking (e.g., typo detection) and Shannon entropy detection (random characters). Download <a href=\"https://splunkbase.splunk.com/app/2734/\">here</a>.", 
                "test": "| rest /services/apps/local | search disabled=0 label=\"URL Toolbox\" | stats count"
            }
        ], 
        "value": "index=* source=\"winEventLog:Security\" EventCode=4688  \n| rex field=Image \"(?<filename>[^\\\\\\\\/]*$)\" \n| stats values(host) as hosts dc(host) as num_hosts values(Image) as Images by filename \n| eval levenshtein_scores=null \n| eval comparisonterm=\"svchost.exe\"  | lookup ut_levenshtein_lookup word1 as filename, word2 as comparisonterm | eval levenshtein_scores=mvappend(levenshtein_scores, ut_levenshtein) \n| eval score{ut_levenshtein} = comparisonterm \n| eval comparisonterm=\"iexplore.exe\" | lookup ut_levenshtein_lookup word1 as filename, word2 as comparisonterm | eval levenshtein_scores=mvappend(levenshtein_scores, ut_levenshtein) | eval score{ut_levenshtein} = comparisonterm \n| eval comparisonterm=\"ipconfig.exe\" | lookup ut_levenshtein_lookup word1 as filename, word2 as comparisonterm | eval levenshtein_scores=mvappend(levenshtein_scores, ut_levenshtein) | eval score{ut_levenshtein} = comparisonterm \n| eval comparisonterm=\"explorer.exe\" | lookup ut_levenshtein_lookup word1 as filename, word2 as comparisonterm | eval levenshtein_scores=mvappend(levenshtein_scores, ut_levenshtein) | eval score{ut_levenshtein} = comparisonterm \n| eventstats max(num_hosts) as max_num_hosts \n| where isnull(mvfilter(levenshtein_scores=\"0\")) AND min(levenshtein_scores) <3 \n| eval lowest_levenshtein_score=min(levenshtein_scores) \n| eval suspect_files = null | foreach score* [eval temp = \"<<FIELD>>\" | rex field=temp \"(?<num>\\d*)$\" | eval suspect_files=if(num<3,mvappend('<<FIELD>>', suspect_files),suspect_files) | fields - temp \"<<FIELD>>\"] \n| eval percentage_of_hosts_affected = round(100*num_hosts/max_num_hosts,2)\n| table filename lowest_levenshtein_score suspect_files Images hosts num_hosts percentage_of_hosts_affected"
    }, 
    "Pull List of Privileged Users - Calculate Risk Scores from LDAP": {
        "actions_UBASeverity": 0, 
        "actions_createNotable": 0, 
        "actions_createRisk": 0, 
        "description": [
            "First we bring in our dataset from LDAPSearch. As noted in the How To Implement, there are many different approaches to ingesting this data -- ldapsearch is generally the simplest to get started with.", 
            "First we initialize our risk score with information about the title of the employee. We're using the case function in eval because it allows us to try out several different titles and assign them different scores, but not risk doubling up. For example, if someone is an Executive Vice President, it would match on the second clause and assign a score or 40, but then stop before moving on to *Vice President*.", 
            "Next, we look for users who are members of privileged groups. You could use case here as well, but I opt'd to instead be additive by using different if statements (such that if someone was a member of Enterprise Admins and Domain Admins, they'd end up with a much higher score. In some organizations with many different domains, you might also wish to calculate how many different domains a user has domain admin rights in.", 
            "Next, we use table to include just the fields that we really want (excluding _raw and generally excluding memberOf as it can be very large).", 
            "Finally we output the results into a lookup that we can use in future searches."
        ], 
        "label": "Pull List of Privileged Users - Calculate Risk Scores from LDAP", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Splunk Supporting Add-on for Active Directory Installed", 
                "resolution": "The Splunk Supporting Add-on for Active Directory app allows us to query AD environments via LDAP to get everything we need. Check it out -- <a href=\"https://splunkbase.splunk.com/app/1151/\" target=\"_blank\">app link</a>, <a href=\"http://docs.splunk.com/Documentation/SA-LdapSearch/2.1.6/User/AbouttheSplunkSupportingAdd-onforActiveDirectory\" target=\"_blank\">docs link</a>.", 
                "test": "| rest /services/apps/local | search disabled=0 title=\"SA-ldapsearch\" | stats count"
            }
        ], 
        "value": "| ldapsearch search=\"(&(objectclass=user)(!(objectclass=computer)))\"  attrs=\"displayName,sAMAccountName,mail,department,company,title,memberOf\" \n| eval risk_score=case(searchmatch(\"title=Chief* OR title=CEO OR title=CIO OR title=CRO OR title=CISO OR title=CFO OR title=CMO\"), 50, searchmatch(\"title=EVP* OR title=\\\"*Executive Vice President*\\\"\"), 40, searchmatch(\"title=SVP* OR title=\\\"*Senior Vice President*\\\"\"), 30, searchmatch(\"title=VP* OR title=\\\"*Vice President*\\\"\"), 20, 1=1, 0) \n| eval risk_score=if(searchmatch(\"memberOf=\\\"CN=*Domain Admins*\\\"\"), risk_score + 30, risk_score), risk_score=if(searchmatch(\"memberOf=\\\"CN=*Enterprise Admins*\\\"\"), risk_score + 40, risk_score), risk_score=if(searchmatch(\"memberOf=\\\"CN=*Admins*\\\"\"), risk_score + 10, risk_score)\n | table risk_score description title displayName sAMAccountName mail department company \n| rename sAMAccountName as user \n| outputlookup privileged_users_high_level_in_company.csv"
    }, 
    "Pull List of Privileged Users - Connect Multiple Accounts to Employee": {
        "actions_UBASeverity": 0, 
        "actions_createNotable": 0, 
        "actions_createRisk": 0, 
        "description": [
            "First we bring in our dataset from LDAPSearch. As noted in the How To Implement, there are many different approaches to ingesting this data -- ldapsearch is generally the simplest to get started with.", 
            "Next we determine what users are privileged. There are faster ways to do this, but we use eval's searchmatch just for the sake of simplicity. This will assign a value of 1 for any users where we see the specified titles or groups.", 
            "Now we use eventstats to distribute the title and the privilege level across all the accounts from the same employee. The theory here is that if we have an employee named Chuck who has an admin account and a normal account, then we see some suspicious actions on his normal account, we want to know that this account belongs to a privileged employee even though this account itself isn't. Often you will also want to distribute other fields such as phone numbers, managers, etc.", 
            "Finally we can filter for just the accounts belonging to privileged employees."
        ], 
        "label": "Pull List of Privileged Users - Connect Multiple Accounts to Employee", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Splunk Supporting Add-on for Active Directory Installed", 
                "resolution": "The Splunk Supporting Add-on for Active Directory app allows us to query AD environments via LDAP to get everything we need. Check it out -- <a href=\"https://splunkbase.splunk.com/app/1151/\" target=\"_blank\">app link</a>, <a href=\"http://docs.splunk.com/Documentation/SA-LdapSearch/2.1.6/User/AbouttheSplunkSupportingAdd-onforActiveDirectory\" target=\"_blank\">docs link</a>.", 
                "test": "| rest /services/apps/local | search disabled=0 title=\"SA-ldapsearch\" | stats count"
            }
        ], 
        "value": "| ldapsearch search=\"(&(objectclass=user)(!(objectClass=computer)))\" \n| eval isPrivileged=if(searchmatch(\"(title=VP* OR title=SVP* OR title=EVP* OR title=\\\"*Vice President*\\\" OR title=Chief* OR title=CEO OR title=CIO OR title=CRO OR title=CISO OR title=CFO OR title=CMO) OR (memberOf=*Admin*)\"), 1, 0) \n| eventstats max(isPrivileged) as isPrivileged values(title) as title by employeeid \n| search isPrivileged=1"
    }, 
    "Pull List of Privileged Users - Connect Multiple Accounts to Employee - Demo": {
        "actions_UBASeverity": 0, 
        "actions_createNotable": 0, 
        "actions_createRisk": 0, 
        "description": [
            "First we bring in our basic demo dataset. In this case, anonymized LDAPSearch Output. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we determine what users are privileged. There are faster ways to do this, but we use eval's searchmatch just for the sake of simplicity. This will assign a value of 1 for any users where we see the specified titles or groups.", 
            "Now we use eventstats to distribute the title and the privilege level across all the accounts from the same employee. The theory here is that if we have an employee named Chuck who has an admin account and a normal account, then we see some suspicious actions on his normal account, we want to know that this account belongs to a privileged employee even though this account itself isn't. Often you will also want to distribute other fields such as phone numbers, managers, etc.", 
            "Finally we can filter for just the accounts belonging to privileged employees.", 
            "Finally, put just the fields we want in a table."
        ], 
        "label": "Pull List of Privileged Users - Connect Multiple Accounts to Employee - Demo", 
        "prereqs": [
            {
                "field": "UC_raw_data_for_privilege_calculations.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(LDAP Data for Privilege Analysis)`  \n| eval isPrivileged=if(searchmatch(\"(title=VP* OR title=SVP* OR title=EVP* OR title=\\\"*Vice President*\\\" OR title=Chief* OR title=CEO OR title=CIO OR title=CRO OR title=CISO OR title=CFO OR title=CMO) OR (memberOf=*Admin*)\"), 1, 0) \n| eventstats max(isPrivileged) as isPrivileged values(title) as title by employeeid \n| search isPrivileged=1\n| table employeeid displayName sAMAccountName title mail department company"
    }, 
    "Pull List of Privileged Users - Demo": {
        "actions_UBASeverity": 0, 
        "actions_createNotable": 0, 
        "actions_createRisk": 0, 
        "description": [
            "First we bring in our basic demo dataset. In this case, anonymized LDAPSearch Output. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we filter to look for interesting titles, or interesting groups", 
            "Finally, put just the fields we want in a table."
        ], 
        "label": "Pull List of Privileged Users - Demo", 
        "prereqs": [
            {
                "field": "UC_raw_data_for_privilege_calculations.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(LDAP Data for Privilege Analysis)` \n| search (title=VP* OR title=SVP* OR title=EVP* OR title=\"*Vice President*\" OR title=Chief* OR title=CEO OR title=CIO OR title=CRO OR title=CISO OR title=CFO OR title=CMO) OR (memberOf=*Admin*)\n| table employeeid displayName sAMAccountName title mail department company "
    }, 
    "Pull List of Privileged Users - SysAdmins from LDAP": {
        "actions_UBASeverity": 0, 
        "actions_createNotable": 0, 
        "actions_createRisk": 0, 
        "description": [
            "Okay, this is pretty tricky. Because ldapsearch (at least in my testing) doesn't allow us to look for users who are members of particular groups (which would be so much simpler, much like the VIPs from LDAP example), we have to first pull the list of privileged groups and then look for the information of every user in those groups. We will start by pulling the list of privileged groups. As noted in the How To Implement, there are many different approaches to ingesting this data -- ldapsearch is generally the simplest to get started with.", 
            "Next we filter for security groups, as we don't care about mailing lists. Any tuning you need to do to this query will likely occur at this point.", 
            "Now we just want the list of group members -- there are many ways to do this (dedup, etc.) but I personally prefer | stats count by member.", 
            "Because we're going to be turning this into a new LDAP Search in a moment, we have to convert this to the LDAP search syntax", 
            "Smaller organizations don't need to worry as much about this particular section. While the LDAP RFC doesn't list the max size of a query, AD limits queries to 10 MB each. We are setting a limit slightly lower (10,000,000 bytes) and breaking into different LDAP queries. This way, we'll never generate a query too big for LDAP to handle. You could tune this as much as you want, as well.", 
            "With this stats query, we will transform a long list of distinguishedNames into an actual LDAP query that can be run. First we group them together into a big multi-value field, then we use eval to combine them all into one field.", 
            "Now that we have everything into a single field, we can use the map search command. map will run the same query as many times as you ask it to, which is really useful for two reasons. For one, ldapsearch is a generating command and must be the first command in a search -- without map, we would have to output to a lookup, then start an entirely new search to use ldapsearch (what a hassle!). With map, we are effectively telling Splunk \"Okay, I've coaxed my data into the right form, let's start a brand new search from here\" and any earlier fields and results (not used by the $tokens$ in map) will be thrown away. The second reason, is that if we have a small to medium organization we will have just a single query (less than 10 MB in size). A big organization might have several queries, but we don't have to deal with any complexity to make that work.", 
            "Finally, put just the fields we want in a table.", 
            "And we can outputlookup to a CSV file that we can use in other searches. That wasn't so hard, was it?"
        ], 
        "label": "Pull List of Privileged Users - SysAdmins from LDAP", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Splunk Supporting Add-on for Active Directory Installed", 
                "resolution": "The Splunk Supporting Add-on for Active Directory app allows us to query AD environments via LDAP to get everything we need. Check it out -- <a href=\"https://splunkbase.splunk.com/app/1151/\" target=\"_blank\">app link</a>, <a href=\"http://docs.splunk.com/Documentation/SA-LdapSearch/2.1.6/User/AbouttheSplunkSupportingAdd-onforActiveDirectory\" target=\"_blank\">docs link</a>.", 
                "test": "| rest /services/apps/local | search disabled=0 title=\"SA-ldapsearch\" | stats count"
            }
        ], 
        "value": "| ldapsearch search=\"(&(objectclass=group)(|(cn=*Admins*)(cn=*Administrators*)))\" attrs=\"cn,distinguishedName,groupType,member,memberOf\" \n| search groupType=SECURITY_ENABLED \n| stats count by member \n| eval member=\"(distinguishedName=\" . member . \")\" \n| eval length = len(member) | streamstats sum(length) as length \n| eval queryNum = round((length + 5000000) / 10000000, 0)\n| stats values(member) as search by queryNum| eval search=\"(|\" . mvjoin(search, \"\") . \")\" \n| map maxsearch=100 search=\"| ldapsearch search=\\\"$search$\\\" attrs=\\\"sAMAccountName,title,displayName,sAMAccountName,mail,department,company,description\\\"\" \n| table description title displayName sAMAccountName mail department company \n| outputlookup users_who_are_members_of_admin_groups.csv"
    }, 
    "Pull List of Privileged Users - Users Taking Privileged Acts": {
        "actions_UBASeverity": 0, 
        "actions_createNotable": 0, 
        "actions_createRisk": 0, 
        "description": [
            "First we bring in our privileged authentication data from the accelerated data model. (If you're wondering, yes you could do this without acceleration, but this is a relatively easy data model to accelerate and it's way faster).", 
            "Next we rename Authentication.user to just user because shorter is better than longer.", 
            "Next we append an existing lookup. The idea here is that we want to cache this data so that we don't have to run over long periods of time every time we run this search -- acceleration makes it fast, but we still like efficiency.", 
            "Now we have two sources of data in the same stream of results -- the first are the times that we've observed in the last day of users taking accelerated actions. The second is the results of the lookup that has our historical information. We now use | stats to combine those two data sources and get the \"true\" earliest and latest time for each user. Notably, you could filter for accounts older than 3 months here if you wanted to (or keep that stuff around for other searches!).", 
            "Finally we output the results to a lookup."
        ], 
        "label": "Pull List of Privileged Users - Users Taking Privileged Acts", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an accelerated Authentication data model", 
                "resolution": "This search requires an accelerated authentication data model to run. If it is not present, consider ingesting Windows Security data via the Splunk Universal Forwarder, and then accelerating it with the Common Information App from <a href=\"http://apps.splunk.com/\">apps.splunk.com</a>.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count  from datamodel=Authentication where earliest=-2h "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a node for Privileged Authentication in your accelerated Authentication data model.", 
                "resolution": "This node is defined by default in the common information model, and is defined by authentication events (tag=authentication) that are privileged (tag=privileged). If it is not present, consider ingesting Windows Security data via the Splunk Universal Forwarder, and then accelerating it with the Common Information App from <a href=\"http://apps.splunk.com/\">apps.splunk.com</a>.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Authentication nodename=\"Authentication.Privileged_Authentication\" where earliest=-2h"
            }
        ], 
        "value": "| tstats summariesonly=t allow_old_summaries=t min(_time) as earliest max(_time) as latest from datamodel=Authentication where nodename=\"Authentication.Privileged_Authentication\" earliest=-1d@d latest=@d by Authentication.user \n| rename Authentication.user as user\n| inputlookup append=t users_taking_privileged_actions.csv \n| stats min(earliest) as earliest max(latest) as latest by user \n| outputlookup users_taking_privileged_actions.csv"
    }, 
    "Pull List of Privileged Users - VIP Employees from LDAP": {
        "actions_UBASeverity": 0, 
        "actions_createNotable": 0, 
        "actions_createRisk": 0, 
        "description": [
            "First we bring in our dataset from LDAPSearch. As noted in the How To Implement, there are many different approaches to ingesting this data -- ldapsearch is generally the simplest to get started with. For performance reasons, rather than bringing ALL the data into Splunk and then filtering in Splunk, we're using the LDAP search syntax to filter at the data source.", 
            "Next we use the table command to restrict to just the fields we actually want to look at.", 
            "Now we rename sAMAccountName to user because user is the standard field in the Common Information Model (and let's be honest, getting the punctuation right in sAMAccountName is probably the source of bugs in at least 20% of the queries that use that field).", 
            "Finally we output the results into a lookup that we can use in future searches."
        ], 
        "label": "Pull List of Privileged Users - VIP Employees from LDAP", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Splunk Supporting Add-on for Active Directory Installed", 
                "resolution": "The Splunk Supporting Add-on for Active Directory app allows us to query AD environments via LDAP to get everything we need. Check it out -- <a href=\"https://splunkbase.splunk.com/app/1151/\" target=\"_blank\">app link</a>, <a href=\"http://docs.splunk.com/Documentation/SA-LdapSearch/2.1.6/User/AbouttheSplunkSupportingAdd-onforActiveDirectory\" target=\"_blank\">docs link</a>.", 
                "test": "| rest /services/apps/local | search disabled=0 title=\"SA-ldapsearch\" | stats count"
            }
        ], 
        "value": "| ldapsearch search=\"(&(objectclass=user)(!(objectclass=computer))(|(title=VP*)(title=SVP*)(title=EVP*)(title=*Vice President*)(title=Chief*)(title=CEO)(title=CIO)(title=CRO)(title=CISO)(title=CFO)(title=CMO)))\"\n| table description title displayName sAMAccountName mail department company \n| rename sAMAccountName as user \n| outputlookup privileged_users_high_level_in_company.csv"
    }, 
    "Ransomware Extensions - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our Sysmon demo data", 
            "From sysmon data, we care primarily about file writes (code 11) or timestamp changes (code 2), so we filter for that", 
            "Next we use the rex command to extract file extensions using a moderately complex regular expression.", 
            "Now that we have our file extensions, we want to look them up. Splunk has a capability of looking up data in a CSV file through the lookup command. This will take the file extension we just extracted, \"look it up\" in the csv file, and then add any new fields.", 
            "The field from the lookup is \"Name\" so we can now search for any true Name field.", 
            "And finally we can pull out all the filenames and put them into a usable format via the stats command."
        ], 
        "label": "Ransomware Extensions - Demo", 
        "prereqs": [
            {
                "field": "UC_ransomware_extentions.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": " | inputlookup UC_ransomware_extentions \n| search EventCode=2 OR EventCode=11 \n| rex field=TargetFilename \"(?<file_extension>\\.[^\\.]+)$\" \n| lookup ransomware_extensions_lookup Extensions AS extension \n| search Name!=false  \n| stats values(TargetFilename) AS \"Files Written\" by _time, host, Image, Name, extension"
    }, 
    "Ransomware Extensions - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our Sysmon data. We care primarily about file writes (code 11) or timestamp changes (code 2), so we filter for that", 
            "Next we use the rex command to extract file extensions using a moderately complex regular expression.", 
            "Now that we have our file extensions, we want to look them up. Splunk has a capability of looking up data in a CSV file through the lookup command. This will take the file extension we just extracted, \"look it up\" in the csv file, and then add any new fields.", 
            "The field from the lookup is \"Name\" so we can now search for any true Name field.", 
            "And finally we can pull out all the filenames and put them into a usable format via the stats command."
        ], 
        "label": "Ransomware Extensions - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Microsoft Sysmon logs", 
                "resolution": "Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=\"https://splunkbase.splunk.com/app/1914/\">Splunk App</a>. Check out the <a href=\"http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf\">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!", 
                "test": "| metasearch index=* sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\"  earliest=-1h latest=now | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have file events (EventCode=2 or EventCode=11)", 
                "resolution": "Check your sysmon configuration file to ensure you are not filtering out EventCode 2 and EventCode 11 events.", 
                "test": "sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\" (EventCode=2 OR EventCode=11) index=* | head 100 | stats count"
            }
        ], 
        "value": "index=* sourcetype=XmlWinEventLog:Microsoft-Windows-Sysmon/Operational EventCode=2 OR EventCode=11 \n| rex field=TargetFilename \"^\\S+(?<extension>\\.\\S+)$\" \n| lookup ransomware_extensions_lookup Extensions AS extension \n| search Name!=false  \n| stats values(TargetFilename) AS \"Files Written\" by _time, host, Image, Name, extension"
    }, 
    "Ransomware Notes - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our Sysmon demo data", 
            "From sysmon data, we care primarily about file writes (code 11) or timestamp changes (code 2), so we filter for that", 
            "Splunk has a capability of looking up data in a CSV file through the lookup command. This will take the filename, \"look it up\" in the csv file, and then add any new fields.", 
            "The field from the lookup is \"status\" so we can now search for any true Name field.", 
            "And finally we can pull out all the filenames and put them into a usable format via the stats command."
        ], 
        "label": "Ransomware Notes - Demo", 
        "prereqs": [
            {
                "field": "UC_ransomware_notes.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| inputlookup UC_ransomware_notes \n| search EventCode=2 OR EventCode=11 \n| lookup ransomware_notes_lookup ransomware_notes as TargetFilename \n| search status=True \n| stats values(TargetFilename) AS \"RansomNotes Detected\" by _time, host, Image"
    }, 
    "Ransomware Notes - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our Sysmon data. From sysmon data, we care primarily about file writes (code 11) or timestamp changes (code 2), so we filter for that", 
            "From line one we have our process launch logs, now we need to filter that down to just the potential attack tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- it will return a single column with the name \"TargetFilename\" that include a number of our search strings. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the day.", 
            "And finally we can pull out all the filenames and put them into a usable format via the stats command."
        ], 
        "label": "Ransomware Notes - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Microsoft Sysmon logs", 
                "resolution": "Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=\"https://splunkbase.splunk.com/app/1914/\">Splunk App</a>. Check out the <a href=\"http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf\">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!", 
                "test": "| metasearch index=* sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\"  earliest=-1h latest=now | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have file events (EventCode=2 or EventCode=11)", 
                "resolution": "Check your sysmon configuration file to ensure you are not filtering out EventCode 2 and EventCode 11 events.", 
                "test": "sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\" (EventCode=2 OR EventCode=11) index=* | head 100 | stats count"
            }
        ], 
        "value": "index=* sourcetype=XmlWinEventLog:Microsoft-Windows-Sysmon/Operational EventCode=2 EventCode=11 \n[ | inputlookup ransomware_notes_lookup | rename ransomware_notes as TargetFilename | eval TargetFilename=\"*\" . TargetFilename | table TargetFilename]\n| bucket _time span=1d \n| stats values(TargetFilename) as filesWritten by _time host Image"
    }, 
    "Recurring Infection on Host - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Computer_Name", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic demo dataset, Symantec Endpoint Protection Risks. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we use stats to calculate the distance between the earliest and latest time of an infection with the range() function. This will give us a number in seconds.", 
            "Then we filter for a range of greater than 30 minutes (so at least 30 minutes between infections).", 
            "Finally we do some formatting to provide usable numbers, so that no one has to calculate seconds to days."
        ], 
        "label": "Recurring Infection on Host - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Symantec Endpoint Protection Risks)` \n| stats count range(_time) as TimeRange by Risk_Name, Computer_Name \n| where TimeRange>1800 \n| eval TimeRange_In_Hours = round(TimeRange/3600,2), TimeRange_In_Days = round(TimeRange/3600/24,2)"
    }, 
    "Recurring Infection on Host - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "Computer_Name", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic dataset, Symantec Endpoint Protection Risks, over the last thirty days.", 
            "Next we use stats to calculate the distance between the earliest and latest time of an infection with the range() function. This will give us a number in seconds.", 
            "Then we filter for a range of greater than 30 minutes (so at least 30 minutes between infections).", 
            "Finally we do some formatting to provide usable numbers, so that no one has to calculate seconds to days."
        ], 
        "label": "Recurring Infection on Host - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Symantec AV data", 
                "resolution": "For simplicity, this search is written specifically for Symantec AV data, but it can be easily modified for other sources. ", 
                "test": "| metasearch earliest=-24h latest=now index=* sourcetype=symantec:ep:* | head 100 | stats count "
            }
        ], 
        "value": "index=* sourcetype=symantec:* earliest=-30d \n| stats count range(_time) as TimeRange by Risk_Name, Computer_Name \n| where TimeRange>1800 \n| eval TimeRange_In_Hours = round(TimeRange/3600,2), TimeRange_In_Days = round(TimeRange/3600/24,2)"
    }, 
    "Risky Events from Privileged Users - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "risk_object", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic demo dataset. In this case, a list of events from the risk index of a demo Splunk Enterprise Security environment. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we use a lookup with the risk scores of user accounts.", 
            "Finally we can filter for just the events occurring to privileged employees."
        ], 
        "label": "Risky Events from Privileged Users - Demo", 
        "prereqs": [
            {
                "field": "UC_generic_risk_events.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup with Risk Events", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }, 
            {
                "field": "PrivilegedRiskScores.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Lookup with Privileged Risk Scores", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Risk Events)` \n| lookup PrivilegedRiskScores user as risk_object \n| search risk_score>0"
    }, 
    "Risky Events from Privileged Users - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "risk_object", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our dataset of events from the risk index of Splunk Enterprise Security.", 
            "Next we use a lookup with the risk scores of user accounts.", 
            "Finally we can filter for just the events occurring to privileged employees."
        ], 
        "label": "Risky Events from Privileged Users - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a risk index", 
                "resolution": "This search presumes the presence of Splunk Enteprise Security to provide the Risk Framework. Reach out to your Splunk team to find out more about Splunk ES, or adapt this search for your own list of risk events.", 
                "test": "| metasearch index=risk | head 100"
            }, 
            {
                "field": "PrivilegedRiskScores.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Lookup with Privileged Risk Scores", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "index=risk risk_object_type=user \n| lookup PrivilegedRiskScores user as risk_object \n| search risk_score>0"
    }, 
    "Series of Discovery Filenames - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our demo dataset. This could be any EDR data source that provides process launch information.", 
            "From line one we have our process launch logs, now we need to filter that down to just the potential discovery tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called \"search\" that has a bunch of file names with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.", 
            "From Line 1-2, we have a list of suspicious process launches. Now we want to see if many of those fire around the same time. Transaction is great for that -- it lets us group together events that all have the same value for a field, in this case the same host. maxpause=5m lets us continue grouping together any events that have no more than 5 minutes between each one.", 
            "From line 1-3, we have grouping of suspicious process launches, now we're going to look and see how many different unique programs were launched using mvcount, which gives us the # of events for a multi-value field.", 
            "Finally we clean up a few fields that transaction adds, so that we get a nice clean display."
        ], 
        "label": "Series of Discovery Filenames - Demo", 
        "prereqs": [
            {
                "field": "generic_sysmon_service_launch_logs.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(\"Generic Sysmon Process Launches\")` \n |search [|inputlookup tools.csv | search discovery_or_attack=discovery | eval filename=\"Image=\\\"*\\\\\\\\\" . filename . \"\\\"\" | stats values(filename) as search | eval search=mvjoin(search, \" OR \")] \n| transaction host maxpause=5m\n | where mvcount(Image)>=6\n| fields - _raw closed_txn field_match_sum linecount"
    }, 
    "Series of Discovery Filenames - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our demo dataset. This could be any EDR data source that provides process launch information.", 
            "From line one we have our process launch logs, now we need to filter that down to just the potential discovery tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called \"search\" that has a bunch of file names with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.", 
            "From Line 1-2, we have a list of suspicious process launches. Now we want to see if many of those fire around the same time. Transaction is great for that -- it lets us group together events that all have the same value for a field, in this case the same host. maxpause=5m lets us continue grouping together any events that have no more than 5 minutes between each one.", 
            "From line 1-3, we have grouping of suspicious process launches, now we're going to look and see how many different unique programs were launched using mvcount, which gives us the # of events for a multi-value field.", 
            "Finally we clean up a few fields that transaction adds, so that we get a nice clean display."
        ], 
        "label": "Series of Discovery Filenames - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security Logs", 
                "resolution": "Begin ingesting Windows Security Logs", 
                "test": "| metasearch index=* earliest=-2h latest=now source=\"winEventLog:Security\" | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Process Launch Logs (Event ID 4688)", 
                "resolution": "Turn on Process Tracking in your Windows Audit logs (<a href=\"https://technet.microsoft.com/en-us/library/cc976411.aspx\">docs</a>)", 
                "test": "earliest=-2h latest=now index=* source=\"winEventLog:Security\" EventCode=4688 | head 100 | stats count "
            }
        ], 
        "value": "index=* source=\"winEventLog:Security\" EventCode=4688 \n[|inputlookup tools.csv | search discovery_or_attack=discovery | stats values(filename) as search | eval search=mvjoin(search, \" OR \")] \n| transaction host maxpause=5m \n| where mvcount(Image)>=6\n| fields - _raw closed_txn field_match_sum linecount"
    }, 
    "Series of Discovery Hashes - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our demo dataset. This could be any EDR data source that provides file hash information.", 
            "From line one we have our process launch logs, now we need to filter that down to just the potential discovery tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called \"search\" that has a bunch of file hashes with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.", 
            "From Line 1-2, we have a list of suspicious process launches. Now we want to see if many of those fire around the same time. Transaction is great for that -- it lets us group together events that all have the same value for a field, in this case the same host. maxpause=5m lets us continue grouping together any events that have no more than 5 minutes between each one.", 
            "From line 1-3, we have grouping of suspicious process launches, now we're going to look and see how many different unique programs were launched using mvcount, which gives us the # of events for a multi-value field.", 
            "Finally we clean up a few fields that transaction adds, so that we get a nice clean display."
        ], 
        "label": "Series of Discovery Hashes - Demo", 
        "prereqs": [
            {
                "field": "generic_sysmon_service_launch_logs.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(\"Generic Sysmon Process Launches\")` \n |search [|inputlookup tools.csv | search discovery_or_attack=discovery | eval hash=\"sha1=\" . hash | stats values(hash) as search | eval search=mvjoin(search, \" OR \")] \n| transaction host maxpause=5m \n| where mvcount(Image)>=6\n | fields - _raw closed_txn field_match_sum linecount"
    }, 
    "Series of Discovery Hashes - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our basic dataset, which consists of XML format Sysmon logs from the endpoints (ingested via the Sysmon TA). This could be any EDR data source that provides file hash information. Because we're looking for process launches, we then filter for EventCode=1 (the Sysmon Process Launch code).", 
            "From line one we have our process launch logs, now we need to filter that down to just the potential discovery tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called \"search\" that has a bunch of file hashes with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.", 
            "From Line 1-2, we have a list of suspicious process launches. Now we want to see if many of those fire around the same time. Transaction is great for that -- it lets us group together events that all have the same value for a field, in this case the same host. maxpause=5m lets us continue grouping together any events that have no more than 5 minutes between each one.", 
            "From line 1-3, we have grouping of suspicious process launches, now we're going to look and see how many different unique programs were launched using mvcount, which gives us the # of events for a multi-value field.", 
            "Finally we clean up a few fields that transaction adds, so that we get a nice clean display."
        ], 
        "label": "Series of Discovery Hashes - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Microsoft Sysmon logs", 
                "resolution": "Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=\"https://splunkbase.splunk.com/app/1914/\">Splunk App</a>. Check out the <a href=\"http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf\">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!", 
                "test": "| metasearch index=* sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\"  earliest=-1h latest=now  | stats count "
            }
        ], 
        "value": "index=* sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\" EventCode=1  \n[|inputlookup tools.csv | search discovery_or_attack=discovery | stats values(hash) as search | eval search=mvjoin(search, \" OR \")] \n| transaction host maxpause=5m \n| where mvcount(Image)>=6\n | fields - _raw closed_txn field_match_sum linecount"
    }, 
    "Series of Hacker Filenames - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic dataset. This could be any EDR data source that provides process launch logs, including Windows 4688 logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "From line one we have our process launch logs, now we need to filter that down to just the potential attack tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called \"search\" that has a bunch of file names with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.", 
            "From Line 1-2, we have a list of suspicious process launches. Now we want to see if many of those fire around the same time. Transaction is great for that -- it lets us group together events that all have the same value for a field, in this case the same host. maxpause=5m lets us continue grouping together any events that have no more than 5 minutes between each one.", 
            "From line 1-3, we have grouping of suspicious process launches, but also transaction has added a few new fields, such as duration and eventcount. Eventcount lets us see how many process launches are in each transaction (each grouping of suspicious process launches), so we can filter for when there are at least 4 launch events together.", 
            "Finally, we filter out a few fields that we don't need here."
        ], 
        "label": "Series of Hacker Filenames - Demo", 
        "prereqs": [
            {
                "field": "generic_sysmon_service_launch_logs.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(\"Generic Sysmon Process Launches\")`  \n|search [|inputlookup tools.csv | search discovery_or_attack=attack | eval filename=\"Image=\\\"*\\\\\\\\\" . filename . \"\\\"\" | stats values(filename) as search | eval search=mvjoin(search, \" OR \")] \n| transaction host maxpause=5m \n| where eventcount>=4 \n| fields - _raw closed_txn field_match_sum linecount"
    }, 
    "Series of Hacker Filenames - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic dataset, which in this case is Windows Security EventID 4688 Process Launch Logs, but could come from any EDR data source. We use index=* here (though you should replace with the index you use for Windows Security events, like index=oswinsec) and specify the standard sourcetype of Windows Security events pulled from the Splunk TA for Windows. Finally, we look for EventCode 4688, which are the Windows Process Launch Logs.", 
            "From line one we have our process launch logs, now we need to filter that down to just the potential attack tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called \"search\" that has a bunch of file names with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.", 
            "From Line 1-2, we have a list of suspicious process launches. Now we want to see if many of those fire around the same time. Transaction is great for that -- it lets us group together events that all have the same value for a field, in this case the same host. maxpause=5m lets us continue grouping together any events that have no more than 5 minutes between each one.", 
            "From line 1-3, we have grouping of suspicious process launches, but also transaction has added a few new fields, such as duration and eventcount. Eventcount lets us see how many process launches are in each transaction (each grouping of suspicious process launches), so we can filter for when there are at least 4 launch events together.", 
            "Finally we clear up a few fields that Transaction adds."
        ], 
        "label": "Series of Hacker Filenames - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security Logs", 
                "resolution": "Begin ingesting Windows Security Logs", 
                "test": "| metasearch index=* earliest=-2h latest=now source=\"winEventLog:Security\" | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Process Launch Logs (Event ID 4688)", 
                "resolution": "Turn on Process Tracking in your Windows Audit logs (<a href=\"https://technet.microsoft.com/en-us/library/cc976411.aspx\">docs</a>)", 
                "test": "earliest=-2h latest=now index=* source=\"winEventLog:Security\" EventCode=4688 | head 100 | stats count "
            }
        ], 
        "value": "index=* source=\"winEventLog:Security\" EventCode=4688 \n[| inputlookup tools.csv WHERE discovery_or_attack=attack | stats values(filename) as search | format] \n| transaction host maxpause=5m \n| where eventcount>=4\n| fields - _raw closed_txn field_match_sum linecount"
    }, 
    "Series of Hacker Hashes - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our demo dataset. This could be any EDR data source that provides file hash information.", 
            "From line one we have our process launch logs, now we need to filter that down to just the potential attack tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called \"search\" that has a bunch of file hashes with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.", 
            "From Line 1-2, we have a list of suspicious process launches. Now we want to see if many of those fire around the same time. Transaction is great for that -- it lets us group together events that all have the same value for a field, in this case the same host. maxpause=5m lets us continue grouping together any events that have no more than 5 minutes between each one.", 
            "From line 1-3, we have grouping of suspicious process launches, but also transaction has added a few new fields, such as duration and eventcount. Eventcount lets us see how many process launches are in each transaction (each grouping of suspicious process launches), so we can filter for when there are at least 4 launch events together.", 
            "Finally we clear up a few fields that Transaction adds."
        ], 
        "label": "Series of Hacker Hashes - Demo", 
        "prereqs": [
            {
                "field": "generic_sysmon_service_launch_logs.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(\"Generic Sysmon Process Launches\")` \n |search [|inputlookup tools.csv | search discovery_or_attack=attack | eval hash=\"sha1=\" . hash | stats values(hash) as search | eval search=mvjoin(search, \" OR \")]\n | transaction host maxpause=5m\n | where eventcount>=4 \n| fields - _raw closed_txn field_match_sum linecount"
    }, 
    "Series of Hacker Hashes - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our basic dataset, which consists of XML format Sysmon logs from the endpoints (ingested via the Sysmon TA). This could be any EDR data source that provides file hash information. Because we're looking for process launches, we then filter for EventCode=1 (the Sysmon Process Launch code).", 
            "From line one we have our process launch logs, now we need to filter that down to just the potential attack tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called \"search\" that has a bunch of file hashes with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.", 
            "From Line 1-2, we have a list of suspicious process launches. Now we want to see if many of those fire around the same time. Transaction is great for that -- it lets us group together events that all have the same value for a field, in this case the same host. maxpause=5m lets us continue grouping together any events that have no more than 5 minutes between each one.", 
            "From line 1-3, we have grouping of suspicious process launches, but also transaction has added a few new fields, such as duration and eventcount. Eventcount lets us see how many process launches are in each transaction (each grouping of suspicious process launches), so we can filter for when there are at least 4 launch events together.", 
            "Finally we clear up a few fields that Transaction adds."
        ], 
        "label": "Series of Hacker Hashes - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Microsoft Sysmon logs", 
                "resolution": "Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=\"https://splunkbase.splunk.com/app/1914/\">Splunk App</a>. Check out the <a href=\"http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf\">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!", 
                "test": "| metasearch index=* sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\"  earliest=-1h latest=now  | stats count "
            }
        ], 
        "value": "index=* sourcetype=\"xmlwineventlog:microsoft-windows-sysmon/operational\" EventCode=1 \n [|inputlookup tools.csv | search discovery_or_attack=attack | stats values(hash) as search | eval search=mvjoin(search, \" OR \")] \n| transaction host maxpause=5m \n| where eventcount>=4 \n| fields - _raw closed_txn field_match_sum linecount"
    }, 
    "Short Lived Accounts - Accelerated": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "dest", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "Here, tstats gives us a super-fast count of account creation events or account deletion events.", 
            "Next we rename the fields make it easier to work with them.", 
            "Transaction will now group everything together so that we can see multiple events occurring to the same username.", 
            "Now we can filter for transactions with both events."
        ], 
        "label": "Short Lived Accounts - Accelerated", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an accelerated Change Analysis data model", 
                "resolution": "Begin ingesting Windows Security Logs", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Change_Analysis where earliest=-2d"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Change Analysis data model must have Local Account Management Logs (Event ID 4720 and 4726)", 
                "resolution": "Turn on Group Management Audit Logs in your Local Windows Security Policy (<a href=\"http://whatevernetworks.com/?p=21\">docs</a>)", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Change_Analysis where earliest=-30d (All_Changes.result_id=4720 OR All_Changes.result_id=4726) "
            }
        ], 
        "value": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Change_Analysis where All_Changes.result_id=4720 OR All_Changes.result_id=4726 by All_Changes.result_id All_Changes.user All_Changes.dest _time \n| rename All_Changes.* as *  \n| transaction user maxspan=180m  \n| search result_id=4720 result_id=4726"
    }, 
    "Short Lived Accounts - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "dest", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our demo dataset.", 
            "This line won't exist in production, it is just so that we can format the demo data (coming from a CSV file) correctly.", 
            "Next we filter to make sure we're looking for just account creation events or account deletions.", 
            "Transaction will now group everything together so that we can see multiple events occurring to the same username.", 
            "We can now filter for users where both event IDs occurred.", 
            "Finally we can display everything in a nice table for the user to consume."
        ], 
        "label": "Short Lived Accounts - Demo", 
        "prereqs": [
            {
                "field": "Local_Short_Lived_Account.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "|`Load_Sample_Log_Data(\"Local Short-Lived Account\")`  \n| rex mode=sed field=Account_Name \"s/\n/;/g\" | makemv Account_Name delim=\";\" \n | search  EventCode=4726 OR EventCode=4720 \n| transaction Target_Account_Name maxspan=180m \n| search EventCode=4720 EventCode=4726\n| table _time EventCode Account_Name Target_Account_Name Message"
    }, 
    "Short Lived Accounts - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "dest", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we pull in our dataset, of Windows Security Logs with account creation events or account deletions.", 
            "Windows Security Logs by default will have two fields for the Account_Name -- the acting username, and the target username. We want the latter (this isn't a canonical always-guaranteed command, but seems to work correctly in this scenario).", 
            "Transaction will now group everything together so that we can see multiple events occurring to the same username.", 
            "Finally we can display everything in a nice table for the user to consume."
        ], 
        "label": "Short Lived Accounts - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security Logs", 
                "resolution": "Begin ingesting Windows Security Logs", 
                "test": "| metasearch index=* earliest=-2h latest=now source=\"winEventLog:Security\" | stats count "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Local Account Management Logs (Event ID 4720 and 4726)", 
                "resolution": "Turn on Group Management Audit Logs in your Local Windows Security Policy (<a href=\"http://whatevernetworks.com/?p=21\">docs</a>)", 
                "test": "| metasearch earliest=-30d latest=now index=* source=\"winEventLog:Security\" TERM(eventcode=4726) OR TERM(eventcode=4720) | head | stats count "
            }
        ], 
        "value": "index=* source=\"winEventLog:Security\"   EventCode=4726 OR EventCode=4720 \n| eval Account_Name=mvindex(Account_Name,1) \n| transaction Account_Name maxspan=180m startswith=\"EventCode=4726\" endswith=\"EventCode=4720\"\n| table _time EventCode Account_Name Message"
    }, 
    "Splunk Role Check GDPR - Demo": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic dataset. This is the output of an exceedingly complicated macro that pulls from Splunk's authorization information. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we enrich the index field with the GDPR category.", 
            "Because this is a GDPR search, we search for just the GDPR in scope indexes.", 
            "Next we enrich the user field with the User's GDPR status.", 
            "Because a user or index can belong to many different categories, we use mvexpand to split them into a multi-value field.", 
            "Finally we look for users who don't have a matching GDPR category, or who aren't authorized for any GDPR information at all."
        ], 
        "label": "Splunk Role Check GDPR - Demo", 
        "prereqs": [
            {
                "field": "anon_interactive_logons.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Splunk Index Provisioning)`  \n| lookup gdpr_splunk_index_category index as accessible_indexes OUTPUT category as index_category \n| search index_category=* \n| lookup gdpr_user_category user OUTPUT category as user_category\n | makemv delim=\"|\" index_category | makemv delim=\"|\" user_category \n| where isnull(user_category) OR user_category != index_category"
    }, 
    "Splunk Role Check GDPR - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our dataset. This is the output of an exceedingly complicated macro that pulls from Splunk's authorization information, which was grabbed for Data Governance.", 
            "Next we enrich the index field with the GDPR category.", 
            "Because this is a GDPR search, we search for just the GDPR in scope indexes.", 
            "Next we enrich the user field with the User's GDPR status.", 
            "Because a user or index can belong to many different categories, we use mvexpand to split them into a multi-value field.", 
            "Finally we look for users who don't have a matching GDPR category, or who aren't authorized for any GDPR information at all."
        ], 
        "label": "Splunk Role Check GDPR - Live", 
        "value": "| `User_to_Index_Provisioning_From_Data_Governance_App`\n| lookup gdpr_splunk_index_category index as accessible_indexes OUTPUT category as index_category \n| search index_category=* \n| lookup gdpr_user_category user OUTPUT category as user_category\n| makemv delim=\"|\" index_category | makemv delim=\"|\" user_category \n| where isnull(user_category) OR user_category != index_category "
    }, 
    "Stale Account Used - Accelerated": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our dataset of success authentication events. You could also add additional fields (e.g. the hosts), but make sure to remove those fields later before outputting to the lookup.", 
            "We rename Authentication.user to user just to make the search easier to read and easier to write.", 
            "Now we go off the grid. We're using an undocumented search command called multireport here, which effectively runs the end of your search multiple times, and appends each result. It's tricky, but powerful!", 
            "multireport Search One: multireport requires that you use a | stats command (or similar reporting command). We don't need to do any transformation of the data, so we are just doing a stats values(*) as * by user to meet the requirement.", 
            "multireport Search One: Next we pull the historical baseline in, via lookup. We rename the baseline fields as prior_* so that we can distinguish between what we just saw, and what we've seen before.", 
            "multireport Search One: Now we filter for just users where the latest login our baseline (effectively, the last login we saw *before* this search) was more than 90 days ago.", 
            "multireport Search One: Because we want to present usable results to the SOC, we provide an explanation of what we saw.", 
            "multireport Search One: And we convert epoch times into friendly timestamps. These results will now show up to the user (or be made a notable event, etc.), as this multireport search now ends.", 
            "multireport Search Two: We're now in the second search. This search functionally sits right after the stats command on line 2 (as if multireport search one didn't exist). The point of this portion is to update our baseline, so we don't have to maintain a separate search. We start that by appending our baseline to our recent results.", 
            "multireport Search Two: Now we can re-calculate the true count, earliest, and latest by user (including both our baseline and recent results).", 
            "multireport Search Two: outputlookup will update the baseline on disk with the most recent results, for the next time that we use this search.", 
            "multireport Search Two: We don't actually want any results from this portion to be returned to the user, so we check the value of a non-existent field -- effectively clearing out any results, and leaving just any results from multireport search one."
        ], 
        "label": "Stale Account Used - Accelerated", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an accelerated Authentication data model", 
                "resolution": "This search requires an accelerated authentication data model to run. If it is not present, consider ingesting Windows Security data via the Splunk Universal Forwarder, and then accelerating it with the Common Information App from <a href=\"http://apps.splunk.com/\">apps.splunk.com</a>.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count  from datamodel=Authentication where earliest=-2h "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a node for Successful Authentication in your accelerated Authentication data model.", 
                "resolution": "This node is defined by default in the common information model, and is defined by authentication events (tag=authentication) that are successful (action=success). If it is not present, consider ingesting Windows Security data via the Splunk Universal Forwarder, and then accelerating it with the Common Information App from <a href=\"http://apps.splunk.com/\">apps.splunk.com</a>.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Authentication nodename=\"Authentication.Successful_Authentication\" where earliest=-2h"
            }, 
            {
                "field": "account_status_tracker.csv", 
                "greaterorequalto": 1, 
                "name": "Must have an Account Status Tracker lookup", 
                "resolution": "You can create a new lookup by running | makeresults | eval user=\"test\", earliest=0, latest=now() , count=0| fields - _time | outputlookup account_status_tracker.csv. Then go into lookup configuration to create a new configuration for account_status_tracker mapped to account_status_tracker.csv", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| tstats summariesonly=t allow_old_summaries=t count min(_time) as earliest max(_time) as latest from datamodel=Authentication where nodename=\"Authentication.Successful_Authentication\" earliest=-1d@d latest=@d by Authentication.user \n| rename Authentication.user as user\n| multireport \n    [| stats values(*) as * by user \n    | lookup account_status_tracker user OUTPUT count as prior_count earliest as prior_earliest latest as prior_latest \n    | where prior_latest < relative_time(now(), \"-90d\") \n    | eval explanation=\"The last login from this user was \" . (round( (earliest-prior_latest) / 3600/24, 2) ) . \" days ago.\" \n    | convert ctime(earliest) ctime(latest) ctime(prior_earliest) ctime(prior_latest) ] \n    [| inputlookup append=t account_status_tracker \n    | stats min(earliest) as earliest max(latest) as latest sum(count) as count by user \n    | outputlookup account_status_tracker \n    | where this_only_exists_to_update_the_lookup='so we will make sure there are no results'] "
    }, 
    "Stale Account Used - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic demo dataset. In this case, a list of anonymized Windows events. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Next we calculate the earliest and latest time in our time range -- this is mostly for convenience of the search itself. You could also add additional fields (e.g. the hosts), but make sure to remove those fields later before outputting to the lookup.", 
            "Next we pull the historical baseline in, via lookup. We rename the baseline fields as prior_* so that we can distinguish between what we just saw, and what we've seen before.", 
            "Now we filter for just users where the latest login our baseline (effectively, the last login we saw *before* this search) was more than 60 days earlier.", 
            "Because we want to present usable results to the SOC, we provide an explanation of what we saw.", 
            "And we convert epoch times into friendly timestamps. These results will now show up to the user (or be made a notable event, etc.). You might note that in the demo version, we never actually update the lookup with the updated data. That's because we want you to be able to run this search multiple times -- check out the live or accelerated version of this search for a fully fleshed out version of this search."
        ], 
        "label": "Stale Account Used - Demo", 
        "prereqs": [
            {
                "field": "anonymized_windows_security_events_with_tags.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security Events", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }, 
            {
                "field": "account_status_tracker.csv", 
                "greaterorequalto": 1, 
                "name": "Must have an Account Status Tracker lookup", 
                "resolution": "You can create a new lookup by running | makeresults | eval user=\"test\", earliest=0, latest=now() , count=0| fields - _time | outputlookup account_status_tracker.csv. Then go into lookup configuration to create a new configuration for account_status_tracker mapped to account_status_tracker.csv", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Windows Logon Activity)` \n| stats count min(_time) as earliest max(_time) as latest by user \n| lookup account_status_tracker user OUTPUT count as prior_count earliest as prior_earliest latest as prior_latest \n| where prior_latest < relative_time(latest, \"-60d\") \n| eval explanation=\"The last login from this user was \" . (round( (earliest-prior_latest) / 3600/24, 2) ) . \" days ago.\" \n| convert ctime(earliest) ctime(latest) ctime(prior_earliest) ctime(prior_latest)\n"
    }, 
    "Stale Account Used - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our dataset of success Windows Security logins. We could use the same search for other datasets as well.", 
            "Next we calculate the earliest and latest time in our time range -- this is mostly for convenience of the search itself. You could also add additional fields (e.g. the hosts), but make sure to remove those fields later before outputting to the lookup.", 
            "Now we go off the grid. We're using an undocumented search command called multireport here, which effectively runs the end of your search multiple times, and appends each result. It's tricky, but powerful!", 
            "multireport Search One: multireport requires that you use a | stats command (or similar reporting command). We don't need to do any transformation of the data, so we are just doing a stats values(*) as * by user to meet the requirement.", 
            "multireport Search One: Next we pull the historical baseline in, via lookup. We rename the baseline fields as prior_* so that we can distinguish between what we just saw, and what we've seen before.", 
            "multireport Search One: Now we filter for just users where the latest login our baseline (effectively, the last login we saw *before* this search) was more than 90 days ago.", 
            "multireport Search One: Because we want to present usable results to the SOC, we provide an explanation of what we saw.", 
            "multireport Search One: And we convert epoch times into friendly timestamps. These results will now show up to the user (or be made a notable event, etc.), as this multireport search now ends.", 
            "multireport Search Two: We're now in the second search. This search functionally sits right after the stats command on line 2 (as if multireport search one didn't exist). The point of this portion is to update our baseline, so we don't have to maintain a separate search. We start that by appending our baseline to our recent results.", 
            "multireport Search Two: Now we can re-calculate the true count, earliest, and latest by user (including both our baseline and recent results).", 
            "multireport Search Two: outputlookup will update the baseline on disk with the most recent results, for the next time that we use this search.", 
            "multireport Search Two: We don't actually want any results from this portion to be returned to the user, so we check the value of a non-existent field -- effectively clearing out any results, and leaving just any results from multireport search one."
        ], 
        "label": "Stale Account Used - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security Logs", 
                "resolution": "Begin ingesting Windows Security Logs", 
                "test": "| metasearch index=* earliest=-2h latest=now source=\"winEventLog:Security\" | stats count "
            }, 
            {
                "field": "account_status_tracker.csv", 
                "greaterorequalto": 1, 
                "name": "Must have an Account Status Tracker lookup", 
                "resolution": "You can create a new lookup by running | makeresults | eval user=\"test\", earliest=0, latest=now() , count=0| fields - _time | outputlookup account_status_tracker.csv. Then go into lookup configuration to create a new configuration for account_status_tracker mapped to account_status_tracker.csv", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "index=* source=win*security action=success \n| stats count min(_time) as earliest max(_time) as latest by user \n| multireport \n    [| stats values(*) as * by user \n    | lookup account_status_tracker user OUTPUT count as prior_count earliest as prior_earliest latest as prior_latest \n    | where prior_latest < relative_time(now(), \"-90d\") \n    | eval explanation=\"The last login from this user was \" . (round( (earliest-prior_latest) / 3600/24, 2) ) . \" days ago.\" \n    | convert ctime(earliest) ctime(latest) ctime(prior_earliest) ctime(prior_latest) ] \n    [| inputlookup append=t account_status_tracker \n    | stats min(earliest) as earliest max(latest) as latest sum(count) as count by user \n    | outputlookup account_status_tracker \n    | where this_only_exists_to_update_the_lookup='so we will make sure there are no results'] "
    }, 
    "Successful Backups - Demo": {
        "actions_UBASeverity": 0, 
        "actions_createNotable": 0, 
        "actions_createRisk": 0, 
        "description": [
            "First we load our basic demo data", 
            "Next we filter for the specific message that NetBackup sends for successful backups", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the day.", 
            "Finally we can look at the hosts that are successfully backed up over time, thanks to stats."
        ], 
        "label": "Successful Backups - Demo", 
        "prereqs": [
            {
                "field": "UC_successful_backups.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| inputlookup UC_successful_backups \n|search MESSAGE=\"Disk/Partition backup completed successfully.\" \n| bucket _time span=1d \n| stats values(COMPUTERNAME) as \"Systems Backed Up\" by _time, MESSAGE"
    }, 
    "Successful Backups - Live": {
        "actions_UBASeverity": 0, 
        "actions_createNotable": 0, 
        "actions_createRisk": 0, 
        "description": [
            "First we load our NetBackup data and filter for the specific message that NetBackup sends for successful backups", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the day.", 
            "Finally we can look at the hosts that are successfully backed up over time, thanks to stats."
        ], 
        "label": "Successful Backups - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Netbackup Logs", 
                "resolution": "Consider ingesting your logs from your Netbackup systems.", 
                "test": "| metasearch index=* sourcetype=netbackup_logs earliest=-1d latest=now | stats count"
            }
        ], 
        "value": "index=* sourcetype=\"netbackup_logs\" \"Disk/Partition backup completed successfully.\" \n| bucket _time span=1d \n| stats values(COMPUTERNAME) as \"Systems Backed Up\" by _time, MESSAGE"
    }, 
    "Suspected Day Trading Activity - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our demo dataset of anonymized proxy events.", 
            "Next, we filter to look at just financial-services data, which is the PAN categorization for used for trading applications.", 
            "Next, we look at the number of connections per user per hour, so that we can filter for where there are is a decent amount of activity. We also bring along the max(duration) so that we can later show how long the longest session was.", 
            "Now we count how many hours had active trading activity over our time window (the last week). We also provide a few data points for illustration.", 
            "Finally, we look for users that have concerted amounts of trading activity at least 20 hours out of the last 7 days.. which seems pretty hard core."
        ], 
        "label": "Suspected Day Trading Activity - Demo", 
        "prereqs": [
            {
                "field": "od_splunklive_fw_data.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Sample Firewall Data)` \n| search category=financial-services  \n| bucket _time span=1h | stats count max(duration) as maxduration by user _time  | where count > 30  \n| stats count as num_hours_active sum(count) as total_connections median(count) as median_connections_per_hour max(maxduration) as max_duration_in_seconds by user   \n| where num_hours_active>20"
    }, 
    "Suspected Day Trading Activity - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our dataset of proxy events, filtered for trading categories and where there is a duration of at least one minute for the connection (we don't want to accidentally include one-and-done ad serving).", 
            "Next, we look at the number of connections per user per hour, so that we can filter for where there are is a decent amount of activity. We also bring along the max(duration) so that we can later show how long the longest session was.", 
            "Now we count how many hours had active trading activity over our time window (the last week). We also provide a few data points for illustration.", 
            "Finally, we look for users that have concerted amounts of trading activity at least 20 hours out of the last 7 days.. which seems pretty hard core."
        ], 
        "label": "Suspected Day Trading Activity - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Proxy data", 
                "resolution": "This search requires NGFW or Web Proxy data to run. By default, we're checking for Common Information Model compliant data, and then also manually specifying the standard sourcetypes for Check Point, Palo Alto Networks, and Blue Coat. You should specify your particular index and sourcetype in the actual search to improve performance (or better yet, accelerate with the common information model!)", 
                "test": "(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa) category=*| head 100 | stats count "
            }
        ], 
        "value": "index=pan_logs category=financial-services earliest=-7d@d user!=unknown duration>60  \n|bucket _time span=1h | stats count max(duration) as maxduration by user _time | where count > 30 \n| stats count as num_hours_active sum(count) as total_connections median(count) as median_connections_per_hour max(maxduration) as max_duration_in_seconds by user  \n| where num_hours_active>20"
    }, 
    "Suspicious Windows Registry activity - Demo": {
        "actions_UBASeverity": 2, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 10, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our basic demo data", 
            "Then we filter for some of the most common AutoRuns keys that are seen in the wild", 
            "Finally we put everything in a table that is easy for analysts to read"
        ], 
        "label": "Suspicious Windows Registry activity - Demo", 
        "prereqs": [
            {
                "field": "UC_autorun_reg_keys.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| inputlookup UC_autorun_reg_keys \n| search process_image=\"*AppData\\\\*\" key_path=\"*currentversion\\\\run*\" \n| table _time, host, process_image, key_path"
    }, 
    "Suspicious Windows Registry activity - Live": {
        "actions_UBASeverity": 2, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 10, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our basic Windows Registry data and filter for some of the most common AutoRuns keys that are seen in the wild", 
            "Finally we put everything in a table that is easy for analysts to read"
        ], 
        "label": "Suspicious Windows Registry activity - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have registry data from Windows TA", 
                "resolution": "This data is provided by the Windows TA. Consider using the TA for a better experience.", 
                "test": "| metasearch index=* source=winRegistry earliest=-1h latest=now | stats count"
            }
        ], 
        "value": "index=* source=winRegistry process_image=\"*AppData\\\\*\" key_path=\"*currentversion\\\\run*\" \n| table _time, host, process_image, key_path"
    }, 
    "Systems with Timestamps Far Into the Future - Live": {
        "actions_UBASeverity": 7, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we use tstats to grab events that were indexed in the last half hour, with timestamps ranging from the last half hour to the distant future.", 
            "Next we look for the time ranges from a single host. This is a quick and dirty baseline, because if we're constantly running this search and an attacker changes the system time from now to a year from now, there will be some period when there is a massive range of timestamps coming from that host.", 
            "Finally we filter for those large ranges."
        ], 
        "label": "Systems with Timestamps Far Into the Future - Live", 
        "value": "| tstats count where index=* _index_earliest=-30m earliest=-30m latest=+16y groupby host _time span=10m \n| stats range(_time) as time_range by host \n| where time_range>3600"
    }, 
    "Unauthorized Login Attempt - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we bring in our basic demo dataset. In this case, a list of anonymized Windows logon events. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "We search for where there is a Failure Reason, and specifically look for code 0xC000015B which indicates that 'The user has not been granted the requested logon type (aka logon right) at this machine'"
        ], 
        "label": "Unauthorized Login Attempt - Demo", 
        "prereqs": [
            {
                "field": "anon_system_logon_with_failure_codes.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security Events", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Windows Logons with Failure Codes)` | search Failure_Reason=* Status=0xC000015B"
    }, 
    "Unauthorized Login Attempt - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "We're bringing in our Windows security logs, looking specifically for the status code 0xC000015B which indicates that the user hasn't been granted the requested logon type (aka, logon right). You may note that we use the bare words failure, reason, logon, and type. These words are found in the raw event, which will help Splunk focus just to in-scope fields. Probably the 0xC000015B is enough to suffice here, but why not be complete."
        ], 
        "label": "Unauthorized Login Attempt - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security Logs", 
                "resolution": "Begin ingesting Windows Security Logs", 
                "test": "| metasearch index=* earliest=-2h latest=now source=\"winEventLog:Security\" | stats count "
            }
        ], 
        "value": "index=* source=win*security user=* EventCode=* action=failure Logon_Type=* Failure Reason Logon Type Status=0xC000015B"
    }, 
    "Unauthorized Web Browsing - Demo": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our demo dataset of anonymized proxy events.", 
            "Next, we filter to look at just blocked connections.", 
            "Finally, we filter for users where we've seen printing on multiple days."
        ], 
        "label": "Unauthorized Web Browsing - Demo", 
        "prereqs": [
            {
                "field": "od_splunklive_fw_data.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Sample Firewall Data)` \n| search action=blocked \n| stats count values(category) as categories by user"
    }, 
    "Unauthorized Web Browsing - Live": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our dataset of proxy events, filtered for just blocked proxy activity where the user is not unknown, and the app is recognized.", 
            "Next, we count the number of connections and the categories and apps per user.", 
            "Finally, filter for where the count is >= 30, or where there are at least 30 events in the searched for time window."
        ], 
        "label": "Unauthorized Web Browsing - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Proxy data", 
                "resolution": "This search requires NGFW or Web Proxy data to run. By default, we're checking for Common Information Model compliant data, and then also manually specifying the standard sourcetypes for Check Point, Palo Alto Networks, and Blue Coat. You should specify your particular index and sourcetype in the actual search to improve performance (or better yet, accelerate with the common information model!)", 
                "test": "(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa) category=*| head 100 | stats count "
            }
        ], 
        "value": "index=pan_logs action=blocked user!=unknown app!=not-applicable\n  | stats count values(app) as apps values(category) as categories by user \n| where count>=30"
    }, 
    "Unsuccessful Backups - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "COMPUTERNAME", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our basic demo data", 
            "Next we filter for the specific message that NetBackup sends for failed backups", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the day.", 
            "Finally we can look at the hosts that failed to back up over time, thanks to stats."
        ], 
        "label": "Unsuccessful Backups - Demo", 
        "prereqs": [
            {
                "field": "UC_unsuccessful_backups.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| inputlookup UC_unsuccessful_backups \n| search MESSAGE=\"An error occurred, failed to backup.\" \n| bucket _time span=1d\n| stats values(COMPUTERNAME) as \"Systems Backed Up\" by _time, MESSAGE"
    }, 
    "Unsuccessful Backups - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "COMPUTERNAME", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our NetBackup data and filter for the specific message that NetBackup sends for failed backups", 
            "Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the day.", 
            "Finally we can look at the hosts that failed to back up over time, thanks to stats."
        ], 
        "label": "Unsuccessful Backups - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Netbackup Logs", 
                "resolution": "Consider ingesting your logs from your Netbackup systems.", 
                "test": "| metasearch index=* sourcetype=netbackup_logs earliest=-1d latest=now | stats count"
            }
        ], 
        "value": "index=* sourcetype=\"netbackup_logs\" \"An error occurred, failed to backup.\" \n| bucket _time span=1d \n| stats values(COMPUTERNAME) as \"Back Up Failures\" by _time, MESSAGE "
    }, 
    "User Login Unauthorized Geo - Accelerated": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our dataset of success authentication events.", 
            "We rename Authentication.user to user and Authentication.dest to dest just to make the search easier to read and easier to write.", 
            "Now it's time for enrichment. First we pull a lookup associating hosts with the countries to which they belong. Most organizations who desire this detection will pull this data from a CMDB which lists physical server location (as most countries that have the greatest concern also have requirements that data stay local). However, you could also look at this based on the country of origin of the people whose data is on the server, as is done in the similar GDPR example.", 
            "Now we need to see what users are authorized to log in here. This lookup doesn't need to list all associations, just those for countries that are problematic. Because it's probable that many users will be permitted to log into multiple hosts, we are doing an mvexpand to break the value into multiple values based on a pipe (|) between them. You could also replace this with a lookup based on LDAP OU or group membership to determine what users are permitted to log on in Germany.", 
            "We've got the enrichment done, now we can look for users logging into hosts where they're not permitted. We are also constricting the list of countries that are in-scope for this detection. Most people who ask for this detection are looking specifically at Germany or Japan, but you can include any countries you desire here.", 
            "We're about to do a stats, and we have a multi-value field here. This means we could end up with many rows accidentally. If a user was permitted to log into 15 different countries, a stats by country would result in 15 rows -- very confusing! Much easier to combine the authorized countries back into a comma-separated list so that the analyst has all of the information consolidated for them.", 
            "Because it's very easy to generate a large number of login events, we are counting the number of logon events (and unique server names) by user, authorized geo, and server geo."
        ], 
        "label": "User Login Unauthorized Geo - Accelerated", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have an accelerated Authentication data model", 
                "resolution": "This search requires an accelerated authentication data model to run. If it is not present, consider ingesting Windows Security data via the Splunk Universal Forwarder, and then accelerating it with the Common Information App from <a href=\"http://apps.splunk.com/\">apps.splunk.com</a>.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count  from datamodel=Authentication where earliest=-2h "
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have a node for Successful Authentication in your accelerated Authentication data model.", 
                "resolution": "This node is defined by default in the common information model, and is defined by authentication events (tag=authentication) that are successful (action=success). If it is not present, consider ingesting Windows Security data via the Splunk Universal Forwarder, and then accelerating it with the Common Information App from <a href=\"http://apps.splunk.com/\">apps.splunk.com</a>.", 
                "test": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Authentication nodename=\"Authentication.Successful_Authentication\" where earliest=-2h"
            }, 
            {
                "field": "sse_host_to_country.csv", 
                "greaterorequalto": 1, 
                "name": "Must have an User to Authorized Countries lookup", 
                "resolution": "Build a lookup with the list of authorized countries for each user. In this case we use the lookup named \"sse_host_to_country.csv\" for app convenience, but you can adjust the final search as needed.", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }, 
            {
                "field": "gdpr_user_category.csv", 
                "greaterorequalto": 1, 
                "name": "Must have a Server to Country lookup", 
                "resolution": "Build a lookup with the country that each server is located in. In this case we use the lookup named \"gdpr_user_category.csv\" for app convenience (we already built one for GDPR..), but you can adjust the final search as needed.", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| tstats summariesonly=t allow_old_summaries=t count from datamodel=Authentication where nodename=\"Authentication.Successful_Authentication\" by Authentication.user, Authentication.dest \n| rename Authentication.* as *\n| lookup sse_host_to_country dest OUTPUT country as server_country \n| lookup gdpr_user_category user OUTPUT countries as authorized_countries | makemv authorized_countries delim=\"|\" \n| where authorized_countries!=server_country AND server_country IN (\"Germany\",\"Japan\") \n| eval authorized_countries=mvjoin(authorized_countries, \", \") \n| stats dc(anonymized_ComputerName) as num_servers count as num_logins by user authorized_countries server_country "
    }, 
    "User Login Unauthorized Geo - Demo": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our basic demo dataset. In this case, a list of anonymized Windows events. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.", 
            "Now it's time for enrichment. First we pull a lookup associating hosts with the countries to which they belong. Most organizations who desire this detection will pull this data from a CMDB which lists physical server location (as most countries that have the greatest concern also have requirements that data stay local). However, you could also look at this based on the country of origin of the people whose data is on the server, as is done in the similar GDPR example.", 
            "Now we need to see what users are authorized to log in here. This lookup doesn't need to list all associations, just those for countries that are problematic. Because it's probable that many users will be permitted to log into multiple hosts, we are doing an mvexpand to break the value into multiple values based on a pipe (|) between them. You could also replace this with a lookup based on LDAP OU or group membership to determine what users are permitted to log on in Germany.", 
            "We've got the enrichment done, now we can look for users logging into hosts where they're not permitted. We are also constricting the list of countries that are in-scope for this detection. Most people who ask for this detection are looking specifically at Germany or Japan, but you can include any countries you desire here.", 
            "We're about to do a stats, and we have a multi-value field here. This means we could end up with many rows accidentally. If a user was permitted to log into 15 different countries, a stats by country would result in 15 rows -- very confusing! Much easier to combine the authorized countries back into a comma-separated list so that the analyst has all of the information consolidated for them.", 
            "Because it's very easy to generate a large number of login events, we are counting the number of logon events (and unique server names) by user, authorized geo, and server geo."
        ], 
        "label": "User Login Unauthorized Geo - Demo", 
        "prereqs": [
            {
                "field": "Sampled_AnonymizedLogonActivity.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security Events", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "\n| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }, 
            {
                "field": "sse_host_to_country.csv", 
                "greaterorequalto": 1, 
                "name": "Must have an User to Authorized Countries lookup", 
                "resolution": "Build a lookup with the list of authorized countries for each user. In this case we use the lookup named \"sse_host_to_country.csv\" for app convenience, but you can adjust the final search as needed.", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }, 
            {
                "field": "gdpr_user_category.csv", 
                "greaterorequalto": 1, 
                "name": "Must have a Server to Country lookup", 
                "resolution": "Build a lookup with the country that each server is located in. In this case we use the lookup named \"gdpr_user_category.csv\" for app convenience (we already built one for GDPR..), but you can adjust the final search as needed.", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "| `Load_Sample_Log_Data(Windows Logon Activity)` \n| lookup sse_host_to_country dest as anonymized_ComputerName OUTPUT country as server_country \n| lookup gdpr_user_category user OUTPUT countries as authorized_countries | makemv authorized_countries delim=\"|\" \n| where authorized_countries!=server_country AND server_country IN (\"Germany\",\"Japan\") \n| eval authorized_countries=mvjoin(authorized_countries, \", \") \n| stats dc(anonymized_ComputerName) as num_servers count as num_logins by user authorized_countries server_country"
    }, 
    "User Login Unauthorized Geo - Live": {
        "actions_UBASeverity": 3, 
        "actions_createRisk": 1, 
        "actions_riskObject": "user", 
        "actions_riskObjectScore": 30, 
        "actions_riskObjectType": "user", 
        "description": [
            "First we bring in our dataset, in this case of successful Windows security events.", 
            "Now it's time for enrichment. First we pull a lookup associating hosts with the countries to which they belong. Most organizations who desire this detection will pull this data from a CMDB which lists physical server location (as most countries that have the greatest concern also have requirements that data stay local). However, you could also look at this based on the country of origin of the people whose data is on the server, as is done in the similar GDPR example.", 
            "Now we need to see what users are authorized to log in here. This lookup doesn't need to list all associations, just those for countries that are problematic. Because it's probable that many users will be permitted to log into multiple hosts, we are doing an mvexpand to break the value into multiple values based on a pipe (|) between them. You could also replace this with a lookup based on LDAP OU or group membership to determine what users are permitted to log on in Germany.", 
            "We've got the enrichment done, now we can look for users logging into hosts where they're not permitted. We are also constricting the list of countries that are in-scope for this detection. Most people who ask for this detection are looking specifically at Germany or Japan, but you can include any countries you desire here.", 
            "We're about to do a stats, and we have a multi-value field here. This means we could end up with many rows accidentally. If a user was permitted to log into 15 different countries, a stats by country would result in 15 rows -- very confusing! Much easier to combine the authorized countries back into a comma-separated list so that the analyst has all of the information consolidated for them.", 
            "Because it's very easy to generate a large number of login events, we are counting the number of logon events (and unique server names) by user, authorized geo, and server geo."
        ], 
        "label": "User Login Unauthorized Geo - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security Logs", 
                "resolution": "Begin ingesting Windows Security Logs", 
                "test": "| metasearch index=* earliest=-2h latest=now source=\"winEventLog:Security\" | stats count "
            }, 
            {
                "field": "sse_host_to_country.csv", 
                "greaterorequalto": 1, 
                "name": "Must have an User to Authorized Countries lookup", 
                "resolution": "Build a lookup with the list of authorized countries for each user. In this case we use the lookup named \"sse_host_to_country.csv\" for app convenience, but you can adjust the final search as needed.", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }, 
            {
                "field": "gdpr_user_category.csv", 
                "greaterorequalto": 1, 
                "name": "Must have a Server to Country lookup", 
                "resolution": "Build a lookup with the country that each server is located in. In this case we use the lookup named \"gdpr_user_category.csv\" for app convenience (we already built one for GDPR..), but you can adjust the final search as needed.", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files \n| eval blah=1, row=\"row\"\n| xyseries row title blah "
            }
        ], 
        "value": "index=* source=win*security action=success \n| lookup sse_host_to_country dest OUTPUT country as server_country \n| lookup gdpr_user_category user OUTPUT countries as authorized_countries | makemv authorized_countries delim=\"|\" \n| where authorized_countries!=server_country AND server_country IN (\"Germany\",\"Japan\") \n| eval authorized_countries=mvjoin(authorized_countries, \", \") \n| stats dc(anonymized_ComputerName) as num_servers count as num_logins by user authorized_countries server_country"
    }, 
    "Vulnerabilities Exploited by Ransomwares - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "hostname", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our Vuln Scanning demo data", 
            "Then we filter the specific CVEs that we care about (in this case, for WannaCry related exploits", 
            "Vuln data is refreshed over time, so we will want that context. Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to that day.", 
            "Finally, we can use stats to put this data in a usable format, showing the CVEs per host, per status, per day."
        ], 
        "label": "Vulnerabilities Exploited by Ransomwares - Demo", 
        "prereqs": [
            {
                "field": "UC_ransomware_vulnerabilities.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| inputlookup UC_ransomware_vulnerabilities \n| search (cve = cve-2017-0143 OR cve = cve-2017-0144 OR cve = cve-2017-0145 OR cve = cve-2017-0146 OR cve = cve-2017-0147 OR cve = cve-2017-0148 OR cve = cve-2014-6332 OR cve = cve-2012-0158 OR cve = cve-2014-4114 OR cve = cve-2014-1761 OR cve = cve-2013-3906 OR cve = cve-2015-1641) \n| bucket _time span=1d \n| stats values(cve) as CVEs by _time, signature, netbios-name, hostname"
    }, 
    "Vulnerabilities Exploited by Ransomwares - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "hostname", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our Vuln Scanning data and filter the specific CVEs that we care about (in this case, for WannaCry related exploits", 
            "Vuln data is refreshed over time, so we will want that context. Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to that day.", 
            "Finally, we can use stats to put this data in a usable format, showing the CVEs per host, per status, per day."
        ], 
        "label": "Vulnerabilities Exploited by Ransomwares - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have nessus data", 
                "resolution": "Ingest your nessus data, use the <a href=\"https://splunkbase.splunk.com/app/1710/\">Splunk Add-on for Tenable</a> for best results/", 
                "test": "index=* (sourcetype=nessus:scan OR tag=vulnerability) | head 100 | stats count"
            }
        ], 
        "value": "index=* (sourcetype=nessus:scan OR tag=vulnerability) (cve = cve-2017-0143 OR cve = cve-2017-0144 OR cve = cve-2017-0145 OR cve = cve-2017-0146 OR cve = cve-2017-0147 OR cve = cve-2017-0148 OR cve = cve-2014-6332 OR cve = cve-2012-0158 OR cve = cve-2014-4114 OR cve = cve-2014-1761 OR cve = cve-2013-3906 OR cve = cve-2015-1641) \n| bucket _time span=1d \n| stats values(cve) as CVEs by _time, signature, netbios-name, hostname"
    }, 
    "Windows Event Log Clearing Events - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our basic demo data", 
            "Next we filter for the Event Codes that indicate the Windows event log is being cleared. You can see there are a few possibilities. ", 
            "Finally, because we respect analysts, we put it in a nice easy-to-consume table."
        ], 
        "label": "Windows Event Log Clearing Events - Demo", 
        "prereqs": [
            {
                "field": "UC_windows_event_log.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": " | inputlookup UC_windows_event_log \n| search ((source=wineventlog:security OR XmlWinEventlog:Security) AND (EventCode=1102 OR EventCode=1100)) OR ((source=wineventlog:system OR XmlWinEventlog:System) AND EventCode=104) \n| table _time EventCode Message sourcetype host"
    }, 
    "Windows Event Log Clearing Events - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "host", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our Windows Event Log data and filter for the Event Codes that indicate the Windows event log is being cleared. You can see there are a few possibilities. ", 
            "Then, because we respect analysts, we put it in a nice easy-to-consume table."
        ], 
        "label": "Windows Event Log Clearing Events - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Security Log data", 
                "resolution": "This search requires Windows Security Log data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.", 
                "test": "| metasearch earliest=-2h latest=now source=\"win*security\" index=* | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows System Log data", 
                "resolution": "This search requires Windows System Log data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.", 
                "test": "| metasearch earliest=-2h latest=now source=\"*in*system\" index=* | stats count"
            }, 
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have audit log events (EventCode=1100 or EventCode=1102 or EventCode=104)", 
                "resolution": "Verify that you are generating and receiving logs with these EventCodes. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!", 
                "test": "(source=\"win*system\" OR source=\"win*security\") (EventCode=1100 OR EventCode=1102 OR EventCode=104) index=* | head 100 | stats count"
            }
        ], 
        "value": "index=* ((source=wineventlog:security OR XmlWinEventlog:Security) AND (EventCode=1102 OR EventCode=1100)) OR ((source=wineventlog:system OR XmlWinEventlog:System) AND EventCode=104) \n| stats count by _time EventCode sourcetype host"
    }, 
    "Windows Successful Updates Install - Demo": {
        "actions_UBASeverity": 0, 
        "actions_createNotable": 0, 
        "actions_createRisk": 0, 
        "description": [
            "First we load our basic demo data", 
            "Next we filter our search for just Windows Update messages that are related to specific KB we know we need to focus on.", 
            "Now we have a large number of events and we will want to roll them up into something more usable. stats is great at that!", 
            "And finally we can filter for exactly the events we care about."
        ], 
        "label": "Windows Successful Updates Install - Demo", 
        "prereqs": [
            {
                "field": "UC_successful_windows_updates.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| inputlookup UC_successful_windows_updates \n| search signature_id=KB4012598 OR signature_id=KB4012212 OR signature_id=KB4012215 OR signature_id=KB4012213 OR signature_id=KB4012216 OR signature_id=KB4012214 OR signature_id=KB4012217 OR signature_id= KB4012606 OR signature_id=KB4013198 OR signature_id= KB4013429 \n| stats last(status) as lastStatus by _time, dest, signature, signature_id \n| search lastStatus=installed"
    }, 
    "Windows Successful Updates Install - Live": {
        "actions_UBASeverity": 0, 
        "actions_createNotable": 0, 
        "actions_createRisk": 0, 
        "description": [
            "First we load our Windows Update data, filtered for just Windows Update messages that are related to specific KB we know we need to focus on.", 
            "Now we have a large number of events and we will want to roll them up into something more usable. stats is great at that!", 
            "And finally we can filter for exactly the events we care about."
        ], 
        "label": "Windows Successful Updates Install - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Update logs", 
                "resolution": "This data is provided by the Windows TA. Consider using the TA for a better experience.", 
                "test": "| metasearch index=* source=windowsUpdateLog earliest=-1h latest=now | stats count"
            }
        ], 
        "value": "index=* (source=\"winEventLog:System\" OR source=\"windowsUpdateLog\") (KB4012598 OR KB4012212 OR KB4012215 OR KB4012213 OR KB4012216 OR KB4012214 OR KB4012217 OR KB4012606 OR KB4013198 OR KB4013429) \n| stats latest(status) as lastStatus by _time, dest, signature, signature_id \n| search lastStatus=installed"
    }, 
    "Windows Updates Install Failure - Demo": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "dest", 
        "actions_riskObjectScore": 10, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our basic demo data", 
            "Next we filter our search for just Windows Update messages that are related to specific KB we know we need to focus on.", 
            "Now we have a large number of events and we will want to roll them up into something more usable. stats is great at that!", 
            "And finally we can filter for exactly the events we care about."
        ], 
        "label": "Windows Updates Install Failure - Demo", 
        "prereqs": [
            {
                "field": "UC_unsuccessful_windows_updates.csv", 
                "greaterorequalto": 1, 
                "name": "Must have Demo Lookup", 
                "resolution": "Verify that lookups installed with Splunk Security Essentials is present", 
                "test": "| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=\"row\"| xyseries row title blah "
            }
        ], 
        "value": "| inputlookup UC_unsuccessful_windows_updates \n| search signature_id=KB4012598 OR signature_id=KB4012212 OR signature_id=KB4012215 OR signature_id=KB4012213 OR signature_id=KB4012216 OR signature_id=KB4012214 OR signature_id=KB4012217 OR signature_id= KB4012606 OR signature_id=KB4013198 OR signature_id= KB4013429 \n| stats last(status) as lastStatus by _time, dest, signature, signature_id \n| search lastStatus=failure"
    }, 
    "Windows Updates Install Failure - Live": {
        "actions_UBASeverity": 6, 
        "actions_createNotable": 1, 
        "actions_createRisk": 1, 
        "actions_riskObject": "dest", 
        "actions_riskObjectScore": 60, 
        "actions_riskObjectType": "system", 
        "description": [
            "First we load our Windows Update data, filtered for just Windows Update messages that are related to specific KB we know we need to focus on.", 
            "Now we have a large number of events and we will want to roll them up into something more usable. stats is great at that!", 
            "And finally we can filter for exactly the events we care about."
        ], 
        "label": "Windows Updates Install Failure - Live", 
        "prereqs": [
            {
                "field": "count", 
                "greaterorequalto": 1, 
                "name": "Must have Windows Update logs", 
                "resolution": "This data is provided by the Windows TA. Consider using the TA for a better experience.", 
                "test": "| metasearch index=* source=windowsUpdateLog earliest=-1h latest=now | stats count"
            }
        ], 
        "value": "index=* source=windowsUpdateLog (KB4012598 OR KB4012212 OR KB4012215 OR KB4012213 OR KB4012216 OR KB4012214 OR KB4012217 OR KB4012606 OR KB4013198 OR KB4013429) \n| stats latest(status) as lastStatus by _time, dest, signature, signature_id \n| search lastStatus=failure"
    }
}